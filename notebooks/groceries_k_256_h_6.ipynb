{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 256\n",
    "tree_depth = 6\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.214082717895508 | KNN Loss: 6.232590198516846 | BCE Loss: 1.9814924001693726\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.18209171295166 | KNN Loss: 6.232646942138672 | BCE Loss: 1.9494444131851196\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.180830955505371 | KNN Loss: 6.232651233673096 | BCE Loss: 1.948180079460144\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.165058135986328 | KNN Loss: 6.232604503631592 | BCE Loss: 1.9324537515640259\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.141905784606934 | KNN Loss: 6.232545852661133 | BCE Loss: 1.9093595743179321\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.150469779968262 | KNN Loss: 6.232403755187988 | BCE Loss: 1.9180659055709839\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.167644500732422 | KNN Loss: 6.232351779937744 | BCE Loss: 1.9352922439575195\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.160579681396484 | KNN Loss: 6.232356071472168 | BCE Loss: 1.9282238483428955\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.123835563659668 | KNN Loss: 6.232311248779297 | BCE Loss: 1.8915246725082397\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.09347915649414 | KNN Loss: 6.232260704040527 | BCE Loss: 1.8612183332443237\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.106090545654297 | KNN Loss: 6.232259750366211 | BCE Loss: 1.8738305568695068\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.10519790649414 | KNN Loss: 6.232176303863525 | BCE Loss: 1.8730220794677734\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.099777221679688 | KNN Loss: 6.232084274291992 | BCE Loss: 1.8676934242248535\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.133140563964844 | KNN Loss: 6.231821060180664 | BCE Loss: 1.9013192653656006\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.124731063842773 | KNN Loss: 6.231690406799316 | BCE Loss: 1.8930405378341675\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.071130752563477 | KNN Loss: 6.231929779052734 | BCE Loss: 1.8392012119293213\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.105834007263184 | KNN Loss: 6.231644630432129 | BCE Loss: 1.8741893768310547\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.121956825256348 | KNN Loss: 6.231703281402588 | BCE Loss: 1.8902533054351807\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.049108505249023 | KNN Loss: 6.231678485870361 | BCE Loss: 1.8174302577972412\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.032014846801758 | KNN Loss: 6.231799125671387 | BCE Loss: 1.800215244293213\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.064201354980469 | KNN Loss: 6.231545448303223 | BCE Loss: 1.8326561450958252\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 7.998784065246582 | KNN Loss: 6.231647968292236 | BCE Loss: 1.7671359777450562\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 7.994251251220703 | KNN Loss: 6.231356620788574 | BCE Loss: 1.762894630432129\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 8.070786476135254 | KNN Loss: 6.231185436248779 | BCE Loss: 1.8396008014678955\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 8.045243263244629 | KNN Loss: 6.231196880340576 | BCE Loss: 1.8140463829040527\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 8.020301818847656 | KNN Loss: 6.231118679046631 | BCE Loss: 1.7891833782196045\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.98768424987793 | KNN Loss: 6.2311906814575195 | BCE Loss: 1.7564934492111206\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 7.9941887855529785 | KNN Loss: 6.23111629486084 | BCE Loss: 1.7630723714828491\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.9377665519714355 | KNN Loss: 6.230917930603027 | BCE Loss: 1.7068487405776978\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.981931209564209 | KNN Loss: 6.230902671813965 | BCE Loss: 1.7510285377502441\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.900906562805176 | KNN Loss: 6.230621814727783 | BCE Loss: 1.670284628868103\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.988317489624023 | KNN Loss: 6.230410575866699 | BCE Loss: 1.7579069137573242\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.887657165527344 | KNN Loss: 6.230483531951904 | BCE Loss: 1.6571733951568604\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.886631011962891 | KNN Loss: 6.230345249176025 | BCE Loss: 1.6562857627868652\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.884189605712891 | KNN Loss: 6.230061054229736 | BCE Loss: 1.6541287899017334\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.887661933898926 | KNN Loss: 6.230043888092041 | BCE Loss: 1.6576180458068848\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.859395503997803 | KNN Loss: 6.229602336883545 | BCE Loss: 1.6297931671142578\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.840106964111328 | KNN Loss: 6.229769706726074 | BCE Loss: 1.610337495803833\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.831353187561035 | KNN Loss: 6.229483604431152 | BCE Loss: 1.6018693447113037\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.7908172607421875 | KNN Loss: 6.229163646697998 | BCE Loss: 1.5616536140441895\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.824409484863281 | KNN Loss: 6.228938579559326 | BCE Loss: 1.595470905303955\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.789121627807617 | KNN Loss: 6.22918701171875 | BCE Loss: 1.5599346160888672\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.796356201171875 | KNN Loss: 6.228203296661377 | BCE Loss: 1.568152904510498\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.733280181884766 | KNN Loss: 6.228513240814209 | BCE Loss: 1.5047671794891357\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.740494251251221 | KNN Loss: 6.228175640106201 | BCE Loss: 1.5123186111450195\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.752398490905762 | KNN Loss: 6.227339744567871 | BCE Loss: 1.5250587463378906\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.724961757659912 | KNN Loss: 6.227004528045654 | BCE Loss: 1.4979572296142578\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.692065715789795 | KNN Loss: 6.226607322692871 | BCE Loss: 1.4654585123062134\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.65323543548584 | KNN Loss: 6.226807117462158 | BCE Loss: 1.4264283180236816\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.636423587799072 | KNN Loss: 6.225770473480225 | BCE Loss: 1.4106531143188477\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.630361080169678 | KNN Loss: 6.224920272827148 | BCE Loss: 1.4054409265518188\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.564391136169434 | KNN Loss: 6.2251362800598145 | BCE Loss: 1.3392549753189087\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.539538860321045 | KNN Loss: 6.2246928215026855 | BCE Loss: 1.3148459196090698\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.577909469604492 | KNN Loss: 6.224069595336914 | BCE Loss: 1.3538401126861572\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.497400283813477 | KNN Loss: 6.223607063293457 | BCE Loss: 1.2737932205200195\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.509946346282959 | KNN Loss: 6.222168445587158 | BCE Loss: 1.2877779006958008\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 7.497913360595703 | KNN Loss: 6.222589015960693 | BCE Loss: 1.2753245830535889\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 7.478194236755371 | KNN Loss: 6.220063209533691 | BCE Loss: 1.2581310272216797\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 7.4364824295043945 | KNN Loss: 6.220405101776123 | BCE Loss: 1.2160775661468506\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 7.433290958404541 | KNN Loss: 6.218837261199951 | BCE Loss: 1.2144536972045898\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 7.418358325958252 | KNN Loss: 6.2178826332092285 | BCE Loss: 1.2004755735397339\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 7.421088695526123 | KNN Loss: 6.2163310050964355 | BCE Loss: 1.2047576904296875\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 7.411500930786133 | KNN Loss: 6.215938568115234 | BCE Loss: 1.195562481880188\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 7.396144866943359 | KNN Loss: 6.214215278625488 | BCE Loss: 1.181929349899292\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 7.372439384460449 | KNN Loss: 6.21171760559082 | BCE Loss: 1.1607215404510498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 7.382832050323486 | KNN Loss: 6.2128143310546875 | BCE Loss: 1.1700177192687988\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 7.365210056304932 | KNN Loss: 6.208114147186279 | BCE Loss: 1.1570959091186523\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 7.319311141967773 | KNN Loss: 6.20708703994751 | BCE Loss: 1.1122238636016846\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 7.334786415100098 | KNN Loss: 6.203920841217041 | BCE Loss: 1.1308658123016357\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 7.328578948974609 | KNN Loss: 6.200413227081299 | BCE Loss: 1.1281657218933105\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 7.3075175285339355 | KNN Loss: 6.196969509124756 | BCE Loss: 1.1105480194091797\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 7.304122447967529 | KNN Loss: 6.196120738983154 | BCE Loss: 1.1080015897750854\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 7.274090766906738 | KNN Loss: 6.193741321563721 | BCE Loss: 1.0803496837615967\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 7.291193008422852 | KNN Loss: 6.190807342529297 | BCE Loss: 1.1003856658935547\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 7.282218933105469 | KNN Loss: 6.188279628753662 | BCE Loss: 1.0939393043518066\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 7.286120414733887 | KNN Loss: 6.180683612823486 | BCE Loss: 1.1054370403289795\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 7.282588005065918 | KNN Loss: 6.17832088470459 | BCE Loss: 1.104266881942749\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 7.291761875152588 | KNN Loss: 6.169872283935547 | BCE Loss: 1.121889591217041\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 7.269546985626221 | KNN Loss: 6.170170783996582 | BCE Loss: 1.0993762016296387\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 7.237257957458496 | KNN Loss: 6.1607255935668945 | BCE Loss: 1.0765321254730225\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 7.252580642700195 | KNN Loss: 6.164277076721191 | BCE Loss: 1.088303565979004\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 7.201642036437988 | KNN Loss: 6.150150299072266 | BCE Loss: 1.0514914989471436\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 7.220907211303711 | KNN Loss: 6.140155792236328 | BCE Loss: 1.0807515382766724\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 7.220436096191406 | KNN Loss: 6.140373229980469 | BCE Loss: 1.0800626277923584\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 7.216014862060547 | KNN Loss: 6.131558895111084 | BCE Loss: 1.0844558477401733\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 7.180375576019287 | KNN Loss: 6.128896236419678 | BCE Loss: 1.0514793395996094\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 7.181018829345703 | KNN Loss: 6.1164960861206055 | BCE Loss: 1.0645228624343872\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 7.174922466278076 | KNN Loss: 6.102327346801758 | BCE Loss: 1.072595238685608\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 7.149316310882568 | KNN Loss: 6.089675426483154 | BCE Loss: 1.059640884399414\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 7.159794807434082 | KNN Loss: 6.07797384262085 | BCE Loss: 1.081821084022522\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 7.180157661437988 | KNN Loss: 6.077416896820068 | BCE Loss: 1.1027408838272095\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 7.179282188415527 | KNN Loss: 6.072275161743164 | BCE Loss: 1.1070067882537842\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 7.132084846496582 | KNN Loss: 6.048713207244873 | BCE Loss: 1.083371877670288\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 7.125070095062256 | KNN Loss: 6.041243076324463 | BCE Loss: 1.0838271379470825\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 7.08466911315918 | KNN Loss: 6.024267196655273 | BCE Loss: 1.0604019165039062\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 7.104804039001465 | KNN Loss: 6.02042818069458 | BCE Loss: 1.0843760967254639\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 7.100409507751465 | KNN Loss: 5.992762565612793 | BCE Loss: 1.107647180557251\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 7.0792155265808105 | KNN Loss: 5.997204780578613 | BCE Loss: 1.0820107460021973\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 7.0337395668029785 | KNN Loss: 5.9800310134887695 | BCE Loss: 1.053708553314209\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 7.035468101501465 | KNN Loss: 5.9656219482421875 | BCE Loss: 1.0698459148406982\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 6.998342037200928 | KNN Loss: 5.937544822692871 | BCE Loss: 1.060797095298767\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 7.028321743011475 | KNN Loss: 5.952569007873535 | BCE Loss: 1.07575261592865\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 6.965326309204102 | KNN Loss: 5.916130065917969 | BCE Loss: 1.0491962432861328\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 6.970754623413086 | KNN Loss: 5.9088006019592285 | BCE Loss: 1.0619539022445679\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 6.984631061553955 | KNN Loss: 5.91193962097168 | BCE Loss: 1.0726914405822754\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 6.953631401062012 | KNN Loss: 5.8962626457214355 | BCE Loss: 1.057368516921997\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 6.961620330810547 | KNN Loss: 5.873046875 | BCE Loss: 1.088573694229126\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 6.981193542480469 | KNN Loss: 5.89480447769165 | BCE Loss: 1.0863890647888184\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 6.938249588012695 | KNN Loss: 5.86090612411499 | BCE Loss: 1.0773435831069946\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 6.9091877937316895 | KNN Loss: 5.863299369812012 | BCE Loss: 1.0458884239196777\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 6.919251918792725 | KNN Loss: 5.862334728240967 | BCE Loss: 1.0569171905517578\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 6.903835296630859 | KNN Loss: 5.8349289894104 | BCE Loss: 1.0689060688018799\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 6.910294532775879 | KNN Loss: 5.843179225921631 | BCE Loss: 1.0671155452728271\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 6.935032844543457 | KNN Loss: 5.865218639373779 | BCE Loss: 1.0698144435882568\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 6.912432670593262 | KNN Loss: 5.837092399597168 | BCE Loss: 1.0753405094146729\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 6.864699840545654 | KNN Loss: 5.8041534423828125 | BCE Loss: 1.0605463981628418\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 6.893640041351318 | KNN Loss: 5.820405960083008 | BCE Loss: 1.0732340812683105\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 6.861554145812988 | KNN Loss: 5.800714015960693 | BCE Loss: 1.060840129852295\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 6.918296813964844 | KNN Loss: 5.816798210144043 | BCE Loss: 1.1014988422393799\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 6.872287750244141 | KNN Loss: 5.794939041137695 | BCE Loss: 1.0773489475250244\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 6.865514755249023 | KNN Loss: 5.796772480010986 | BCE Loss: 1.0687425136566162\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 6.86391019821167 | KNN Loss: 5.798954963684082 | BCE Loss: 1.064955234527588\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 6.869673728942871 | KNN Loss: 5.7943644523620605 | BCE Loss: 1.0753092765808105\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 6.865348815917969 | KNN Loss: 5.7938432693481445 | BCE Loss: 1.0715057849884033\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 6.8398027420043945 | KNN Loss: 5.773758888244629 | BCE Loss: 1.0660438537597656\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 6.849353790283203 | KNN Loss: 5.773988246917725 | BCE Loss: 1.0753653049468994\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 6.81065034866333 | KNN Loss: 5.767730236053467 | BCE Loss: 1.0429202318191528\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 6.811240196228027 | KNN Loss: 5.767305374145508 | BCE Loss: 1.0439348220825195\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 6.827362537384033 | KNN Loss: 5.758532524108887 | BCE Loss: 1.068830132484436\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 6.826870918273926 | KNN Loss: 5.75700044631958 | BCE Loss: 1.0698707103729248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 6.817303657531738 | KNN Loss: 5.757551193237305 | BCE Loss: 1.0597525835037231\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 6.8298659324646 | KNN Loss: 5.766357898712158 | BCE Loss: 1.0635080337524414\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 6.808271408081055 | KNN Loss: 5.757097244262695 | BCE Loss: 1.0511741638183594\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 6.804651737213135 | KNN Loss: 5.737254619598389 | BCE Loss: 1.067397117614746\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 6.8376383781433105 | KNN Loss: 5.74813985824585 | BCE Loss: 1.0894984006881714\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 6.7630462646484375 | KNN Loss: 5.725612640380859 | BCE Loss: 1.037433385848999\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 6.782855987548828 | KNN Loss: 5.726426124572754 | BCE Loss: 1.0564301013946533\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 6.758255958557129 | KNN Loss: 5.711081504821777 | BCE Loss: 1.0471744537353516\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 6.8074469566345215 | KNN Loss: 5.709840297698975 | BCE Loss: 1.0976065397262573\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 6.778820991516113 | KNN Loss: 5.740299701690674 | BCE Loss: 1.0385215282440186\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 6.82491397857666 | KNN Loss: 5.754732131958008 | BCE Loss: 1.0701816082000732\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 6.751884937286377 | KNN Loss: 5.699626445770264 | BCE Loss: 1.0522584915161133\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 6.766030788421631 | KNN Loss: 5.711295127868652 | BCE Loss: 1.0547356605529785\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 6.762176036834717 | KNN Loss: 5.693283557891846 | BCE Loss: 1.068892478942871\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 6.786391735076904 | KNN Loss: 5.700481414794922 | BCE Loss: 1.0859102010726929\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 6.751077651977539 | KNN Loss: 5.691286087036133 | BCE Loss: 1.0597914457321167\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 6.742364883422852 | KNN Loss: 5.692251205444336 | BCE Loss: 1.0501134395599365\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 6.730671405792236 | KNN Loss: 5.686811923980713 | BCE Loss: 1.0438594818115234\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 6.729419708251953 | KNN Loss: 5.677875518798828 | BCE Loss: 1.051543951034546\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 6.736562728881836 | KNN Loss: 5.676079750061035 | BCE Loss: 1.0604827404022217\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 6.70888614654541 | KNN Loss: 5.666404724121094 | BCE Loss: 1.0424811840057373\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 6.750535488128662 | KNN Loss: 5.677901744842529 | BCE Loss: 1.0726337432861328\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 6.720786094665527 | KNN Loss: 5.673183917999268 | BCE Loss: 1.0476019382476807\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 6.7106499671936035 | KNN Loss: 5.672023296356201 | BCE Loss: 1.0386265516281128\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 6.740318775177002 | KNN Loss: 5.687255382537842 | BCE Loss: 1.0530632734298706\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 6.681023597717285 | KNN Loss: 5.646630764007568 | BCE Loss: 1.0343927145004272\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 6.694886207580566 | KNN Loss: 5.64055061340332 | BCE Loss: 1.0543357133865356\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 6.710011959075928 | KNN Loss: 5.667630672454834 | BCE Loss: 1.0423811674118042\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 6.698781967163086 | KNN Loss: 5.641603469848633 | BCE Loss: 1.0571784973144531\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 6.74860954284668 | KNN Loss: 5.682526111602783 | BCE Loss: 1.0660831928253174\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 6.750526428222656 | KNN Loss: 5.686486721038818 | BCE Loss: 1.064039945602417\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 6.770081043243408 | KNN Loss: 5.709356784820557 | BCE Loss: 1.0607243776321411\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 6.721567153930664 | KNN Loss: 5.680845737457275 | BCE Loss: 1.0407214164733887\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 6.717352867126465 | KNN Loss: 5.646707057952881 | BCE Loss: 1.070646047592163\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 6.682051658630371 | KNN Loss: 5.635868072509766 | BCE Loss: 1.0461838245391846\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 6.702693939208984 | KNN Loss: 5.665139675140381 | BCE Loss: 1.0375545024871826\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 6.699221134185791 | KNN Loss: 5.65440559387207 | BCE Loss: 1.0448155403137207\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 6.727046966552734 | KNN Loss: 5.663532257080078 | BCE Loss: 1.0635144710540771\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 6.685893535614014 | KNN Loss: 5.622943878173828 | BCE Loss: 1.062949776649475\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 6.698256015777588 | KNN Loss: 5.633917808532715 | BCE Loss: 1.064338207244873\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 6.7267255783081055 | KNN Loss: 5.651452541351318 | BCE Loss: 1.0752732753753662\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 6.686459064483643 | KNN Loss: 5.629597187042236 | BCE Loss: 1.0568618774414062\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 6.67077112197876 | KNN Loss: 5.618101596832275 | BCE Loss: 1.0526695251464844\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 6.740300178527832 | KNN Loss: 5.6586012840271 | BCE Loss: 1.0816986560821533\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 6.732584476470947 | KNN Loss: 5.666399002075195 | BCE Loss: 1.0661855936050415\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 6.62492561340332 | KNN Loss: 5.621947288513184 | BCE Loss: 1.0029783248901367\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 6.677785873413086 | KNN Loss: 5.624293804168701 | BCE Loss: 1.0534921884536743\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 6.718375205993652 | KNN Loss: 5.641543388366699 | BCE Loss: 1.0768320560455322\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 6.680807590484619 | KNN Loss: 5.61459493637085 | BCE Loss: 1.06621253490448\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 6.729631423950195 | KNN Loss: 5.692299842834473 | BCE Loss: 1.0373313426971436\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 6.659051418304443 | KNN Loss: 5.609259605407715 | BCE Loss: 1.049791932106018\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 6.696140289306641 | KNN Loss: 5.662506580352783 | BCE Loss: 1.0336339473724365\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 6.721554756164551 | KNN Loss: 5.66022253036499 | BCE Loss: 1.061332106590271\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 6.6795501708984375 | KNN Loss: 5.626132011413574 | BCE Loss: 1.0534179210662842\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 6.723899841308594 | KNN Loss: 5.657522678375244 | BCE Loss: 1.0663774013519287\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 6.726463317871094 | KNN Loss: 5.657681465148926 | BCE Loss: 1.068781852722168\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 6.716783046722412 | KNN Loss: 5.670083522796631 | BCE Loss: 1.0466995239257812\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 6.709136486053467 | KNN Loss: 5.650005340576172 | BCE Loss: 1.0591310262680054\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 6.716803550720215 | KNN Loss: 5.661867618560791 | BCE Loss: 1.0549359321594238\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 6.686152458190918 | KNN Loss: 5.6254191398620605 | BCE Loss: 1.0607335567474365\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 6.746030330657959 | KNN Loss: 5.678231239318848 | BCE Loss: 1.0677990913391113\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 6.718303680419922 | KNN Loss: 5.644774913787842 | BCE Loss: 1.0735286474227905\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 6.706460952758789 | KNN Loss: 5.64552116394043 | BCE Loss: 1.0609400272369385\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 6.6751909255981445 | KNN Loss: 5.622191429138184 | BCE Loss: 1.0529992580413818\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 6.760815620422363 | KNN Loss: 5.688978672027588 | BCE Loss: 1.0718367099761963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 6.665799617767334 | KNN Loss: 5.621352672576904 | BCE Loss: 1.0444470643997192\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 6.676266670227051 | KNN Loss: 5.643748760223389 | BCE Loss: 1.0325177907943726\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 6.670083045959473 | KNN Loss: 5.616166591644287 | BCE Loss: 1.0539166927337646\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 6.766299247741699 | KNN Loss: 5.700475215911865 | BCE Loss: 1.0658239126205444\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 6.715966701507568 | KNN Loss: 5.669598579406738 | BCE Loss: 1.04636812210083\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 6.698056221008301 | KNN Loss: 5.666419982910156 | BCE Loss: 1.0316359996795654\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 6.733022212982178 | KNN Loss: 5.679352760314941 | BCE Loss: 1.0536694526672363\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 6.645362377166748 | KNN Loss: 5.630128383636475 | BCE Loss: 1.0152339935302734\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 6.678497314453125 | KNN Loss: 5.650598049163818 | BCE Loss: 1.0278995037078857\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 6.682220458984375 | KNN Loss: 5.611510276794434 | BCE Loss: 1.0707101821899414\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 6.681626796722412 | KNN Loss: 5.622779846191406 | BCE Loss: 1.0588470697402954\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 6.682939052581787 | KNN Loss: 5.642361164093018 | BCE Loss: 1.0405778884887695\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 6.655270099639893 | KNN Loss: 5.611522197723389 | BCE Loss: 1.043747901916504\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 6.673163890838623 | KNN Loss: 5.618680477142334 | BCE Loss: 1.054483413696289\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 6.680071830749512 | KNN Loss: 5.608414173126221 | BCE Loss: 1.071657419204712\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 6.738224029541016 | KNN Loss: 5.667428970336914 | BCE Loss: 1.0707951784133911\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 6.660572052001953 | KNN Loss: 5.619968414306641 | BCE Loss: 1.0406038761138916\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 6.686734199523926 | KNN Loss: 5.633575916290283 | BCE Loss: 1.0531585216522217\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 6.679752349853516 | KNN Loss: 5.657271862030029 | BCE Loss: 1.0224804878234863\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 6.714372634887695 | KNN Loss: 5.648703098297119 | BCE Loss: 1.065669298171997\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 6.789684772491455 | KNN Loss: 5.737590312957764 | BCE Loss: 1.0520944595336914\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 6.6729350090026855 | KNN Loss: 5.619827747344971 | BCE Loss: 1.0531072616577148\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 6.694794654846191 | KNN Loss: 5.6326212882995605 | BCE Loss: 1.0621731281280518\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 6.740745544433594 | KNN Loss: 5.679940700531006 | BCE Loss: 1.060805082321167\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 6.722027778625488 | KNN Loss: 5.645447731018066 | BCE Loss: 1.076580286026001\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 6.657106399536133 | KNN Loss: 5.604979515075684 | BCE Loss: 1.0521268844604492\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 6.616509437561035 | KNN Loss: 5.6156325340271 | BCE Loss: 1.0008769035339355\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 6.7101006507873535 | KNN Loss: 5.6382904052734375 | BCE Loss: 1.0718101263046265\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 6.680851936340332 | KNN Loss: 5.610904693603516 | BCE Loss: 1.0699472427368164\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 6.723417282104492 | KNN Loss: 5.678601264953613 | BCE Loss: 1.0448161363601685\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 6.692925930023193 | KNN Loss: 5.627840042114258 | BCE Loss: 1.0650858879089355\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 6.682925701141357 | KNN Loss: 5.641454219818115 | BCE Loss: 1.0414714813232422\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 6.677666664123535 | KNN Loss: 5.605939865112305 | BCE Loss: 1.0717270374298096\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 6.751880645751953 | KNN Loss: 5.688145637512207 | BCE Loss: 1.063734769821167\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 6.6994242668151855 | KNN Loss: 5.643439292907715 | BCE Loss: 1.0559849739074707\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 6.6729631423950195 | KNN Loss: 5.609276294708252 | BCE Loss: 1.0636866092681885\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 6.739566326141357 | KNN Loss: 5.673235893249512 | BCE Loss: 1.0663303136825562\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 6.695838928222656 | KNN Loss: 5.634738445281982 | BCE Loss: 1.0611003637313843\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 6.637249946594238 | KNN Loss: 5.596942901611328 | BCE Loss: 1.0403072834014893\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 6.643985271453857 | KNN Loss: 5.6007080078125 | BCE Loss: 1.043277382850647\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 6.743569374084473 | KNN Loss: 5.700582504272461 | BCE Loss: 1.0429871082305908\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 6.743806838989258 | KNN Loss: 5.700889587402344 | BCE Loss: 1.042917013168335\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 6.660545349121094 | KNN Loss: 5.617494106292725 | BCE Loss: 1.0430512428283691\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 6.643959045410156 | KNN Loss: 5.60104513168335 | BCE Loss: 1.0429139137268066\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 6.646212577819824 | KNN Loss: 5.601785182952881 | BCE Loss: 1.0444276332855225\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 6.672750473022461 | KNN Loss: 5.612194538116455 | BCE Loss: 1.0605559349060059\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 6.689894676208496 | KNN Loss: 5.654047012329102 | BCE Loss: 1.0358476638793945\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 6.742237567901611 | KNN Loss: 5.712630748748779 | BCE Loss: 1.029606819152832\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 6.698897361755371 | KNN Loss: 5.655879974365234 | BCE Loss: 1.0430173873901367\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 6.703329563140869 | KNN Loss: 5.622901916503906 | BCE Loss: 1.0804277658462524\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 6.650784969329834 | KNN Loss: 5.6018900871276855 | BCE Loss: 1.0488948822021484\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 6.707928657531738 | KNN Loss: 5.6423115730285645 | BCE Loss: 1.0656169652938843\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 6.6497039794921875 | KNN Loss: 5.606942653656006 | BCE Loss: 1.0427610874176025\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 6.695801258087158 | KNN Loss: 5.6280951499938965 | BCE Loss: 1.0677061080932617\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 6.75964879989624 | KNN Loss: 5.695112705230713 | BCE Loss: 1.0645360946655273\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 6.675225257873535 | KNN Loss: 5.616596698760986 | BCE Loss: 1.058628797531128\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 6.629484176635742 | KNN Loss: 5.604483127593994 | BCE Loss: 1.025001049041748\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 6.6667375564575195 | KNN Loss: 5.620303153991699 | BCE Loss: 1.0464344024658203\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 6.707126140594482 | KNN Loss: 5.654364585876465 | BCE Loss: 1.052761435508728\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 6.702338218688965 | KNN Loss: 5.662137985229492 | BCE Loss: 1.0402002334594727\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 6.69031286239624 | KNN Loss: 5.628629684448242 | BCE Loss: 1.0616830587387085\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 6.684531211853027 | KNN Loss: 5.667604446411133 | BCE Loss: 1.0169270038604736\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 6.615481376647949 | KNN Loss: 5.5994181632995605 | BCE Loss: 1.0160634517669678\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 6.700181484222412 | KNN Loss: 5.6353440284729 | BCE Loss: 1.0648373365402222\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 6.665473461151123 | KNN Loss: 5.620742321014404 | BCE Loss: 1.0447310209274292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 6.698171615600586 | KNN Loss: 5.619533538818359 | BCE Loss: 1.0786380767822266\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 6.653359413146973 | KNN Loss: 5.622973442077637 | BCE Loss: 1.0303857326507568\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 6.652497291564941 | KNN Loss: 5.626805305480957 | BCE Loss: 1.025692105293274\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 6.656472206115723 | KNN Loss: 5.606506824493408 | BCE Loss: 1.0499656200408936\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 6.697007179260254 | KNN Loss: 5.646609783172607 | BCE Loss: 1.0503973960876465\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 6.674751281738281 | KNN Loss: 5.63383150100708 | BCE Loss: 1.0409200191497803\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 6.68321418762207 | KNN Loss: 5.612985134124756 | BCE Loss: 1.0702292919158936\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 6.666189193725586 | KNN Loss: 5.630638122558594 | BCE Loss: 1.035550832748413\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 6.698775768280029 | KNN Loss: 5.652093410491943 | BCE Loss: 1.0466824769973755\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 6.724609375 | KNN Loss: 5.688595771789551 | BCE Loss: 1.0360137224197388\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 6.683544635772705 | KNN Loss: 5.631209850311279 | BCE Loss: 1.0523347854614258\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 6.654967308044434 | KNN Loss: 5.605662822723389 | BCE Loss: 1.049304723739624\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 6.678292274475098 | KNN Loss: 5.615876197814941 | BCE Loss: 1.0624160766601562\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 6.699872970581055 | KNN Loss: 5.617654323577881 | BCE Loss: 1.0822185277938843\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 6.657077789306641 | KNN Loss: 5.618127822875977 | BCE Loss: 1.038949728012085\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 6.661803245544434 | KNN Loss: 5.6206583976745605 | BCE Loss: 1.0411449670791626\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 6.640955924987793 | KNN Loss: 5.600815296173096 | BCE Loss: 1.0401408672332764\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 6.659120559692383 | KNN Loss: 5.615511417388916 | BCE Loss: 1.0436091423034668\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 6.689162731170654 | KNN Loss: 5.620316028594971 | BCE Loss: 1.0688467025756836\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 6.614951133728027 | KNN Loss: 5.601256847381592 | BCE Loss: 1.013694405555725\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 6.662930011749268 | KNN Loss: 5.6157307624816895 | BCE Loss: 1.0471991300582886\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 6.674572467803955 | KNN Loss: 5.632866859436035 | BCE Loss: 1.04170560836792\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 6.651647567749023 | KNN Loss: 5.608532905578613 | BCE Loss: 1.0431149005889893\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 6.772434234619141 | KNN Loss: 5.711657524108887 | BCE Loss: 1.060776710510254\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 6.698826313018799 | KNN Loss: 5.6559672355651855 | BCE Loss: 1.0428589582443237\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 6.6813249588012695 | KNN Loss: 5.620754241943359 | BCE Loss: 1.0605707168579102\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 6.743003845214844 | KNN Loss: 5.72868537902832 | BCE Loss: 1.0143187046051025\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 6.640365123748779 | KNN Loss: 5.602714538574219 | BCE Loss: 1.03765070438385\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 6.703006744384766 | KNN Loss: 5.647292137145996 | BCE Loss: 1.0557146072387695\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 6.673986434936523 | KNN Loss: 5.612797737121582 | BCE Loss: 1.0611885786056519\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 6.679134368896484 | KNN Loss: 5.6268415451049805 | BCE Loss: 1.052293062210083\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 6.685500144958496 | KNN Loss: 5.6258015632629395 | BCE Loss: 1.0596983432769775\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 6.681915283203125 | KNN Loss: 5.617282867431641 | BCE Loss: 1.0646322965621948\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 6.633717060089111 | KNN Loss: 5.594477653503418 | BCE Loss: 1.0392394065856934\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 6.6794586181640625 | KNN Loss: 5.631394863128662 | BCE Loss: 1.0480637550354004\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 6.727304458618164 | KNN Loss: 5.685861110687256 | BCE Loss: 1.0414432287216187\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 6.707090377807617 | KNN Loss: 5.657175064086914 | BCE Loss: 1.0499155521392822\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 6.6672773361206055 | KNN Loss: 5.6186041831970215 | BCE Loss: 1.0486732721328735\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 6.6447601318359375 | KNN Loss: 5.595888137817383 | BCE Loss: 1.0488719940185547\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 6.6742753982543945 | KNN Loss: 5.626008033752441 | BCE Loss: 1.0482676029205322\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 6.6338300704956055 | KNN Loss: 5.596891403198242 | BCE Loss: 1.0369384288787842\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 6.685237884521484 | KNN Loss: 5.6056599617004395 | BCE Loss: 1.0795778036117554\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 6.7158002853393555 | KNN Loss: 5.659393787384033 | BCE Loss: 1.0564062595367432\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 6.656948089599609 | KNN Loss: 5.625116348266602 | BCE Loss: 1.031831979751587\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 6.656208038330078 | KNN Loss: 5.607500076293945 | BCE Loss: 1.0487077236175537\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 6.6946611404418945 | KNN Loss: 5.622942924499512 | BCE Loss: 1.071718454360962\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 6.644554615020752 | KNN Loss: 5.6074371337890625 | BCE Loss: 1.0371174812316895\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 6.723137378692627 | KNN Loss: 5.663261413574219 | BCE Loss: 1.0598758459091187\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 6.680203437805176 | KNN Loss: 5.624189376831055 | BCE Loss: 1.0560142993927002\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 6.745204925537109 | KNN Loss: 5.660043239593506 | BCE Loss: 1.0851619243621826\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 6.6511149406433105 | KNN Loss: 5.616066932678223 | BCE Loss: 1.035048007965088\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 6.663893699645996 | KNN Loss: 5.6074395179748535 | BCE Loss: 1.0564539432525635\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 6.650622367858887 | KNN Loss: 5.62278413772583 | BCE Loss: 1.0278382301330566\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 6.643685340881348 | KNN Loss: 5.604252338409424 | BCE Loss: 1.0394331216812134\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 6.653087139129639 | KNN Loss: 5.595621109008789 | BCE Loss: 1.0574660301208496\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 6.654624938964844 | KNN Loss: 5.6124982833862305 | BCE Loss: 1.0421267747879028\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 6.645280838012695 | KNN Loss: 5.592629909515381 | BCE Loss: 1.0526506900787354\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 6.740934371948242 | KNN Loss: 5.719285488128662 | BCE Loss: 1.0216491222381592\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 6.70570182800293 | KNN Loss: 5.683318614959717 | BCE Loss: 1.022383213043213\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 6.687200546264648 | KNN Loss: 5.600749492645264 | BCE Loss: 1.0864510536193848\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 6.653023719787598 | KNN Loss: 5.609556674957275 | BCE Loss: 1.0434668064117432\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 6.680617332458496 | KNN Loss: 5.648125648498535 | BCE Loss: 1.03249192237854\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 6.680891513824463 | KNN Loss: 5.609280109405518 | BCE Loss: 1.0716115236282349\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 6.727347373962402 | KNN Loss: 5.680643081665039 | BCE Loss: 1.0467045307159424\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 6.645607948303223 | KNN Loss: 5.61026668548584 | BCE Loss: 1.0353412628173828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 6.629484176635742 | KNN Loss: 5.595648765563965 | BCE Loss: 1.0338356494903564\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 6.626345634460449 | KNN Loss: 5.591723918914795 | BCE Loss: 1.0346214771270752\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 6.645418167114258 | KNN Loss: 5.607794761657715 | BCE Loss: 1.037623405456543\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 6.714426517486572 | KNN Loss: 5.657421588897705 | BCE Loss: 1.0570048093795776\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 6.633336067199707 | KNN Loss: 5.59903621673584 | BCE Loss: 1.0342998504638672\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 6.661944389343262 | KNN Loss: 5.621927738189697 | BCE Loss: 1.0400164127349854\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 6.698716163635254 | KNN Loss: 5.634062767028809 | BCE Loss: 1.0646536350250244\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 6.7162041664123535 | KNN Loss: 5.650175094604492 | BCE Loss: 1.0660290718078613\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 6.709595680236816 | KNN Loss: 5.65225076675415 | BCE Loss: 1.057344913482666\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 6.6786298751831055 | KNN Loss: 5.618369102478027 | BCE Loss: 1.0602607727050781\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 6.664924621582031 | KNN Loss: 5.626993656158447 | BCE Loss: 1.0379307270050049\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 6.715508460998535 | KNN Loss: 5.676180362701416 | BCE Loss: 1.0393283367156982\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 6.748342514038086 | KNN Loss: 5.686265468597412 | BCE Loss: 1.0620770454406738\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 6.733236789703369 | KNN Loss: 5.624749660491943 | BCE Loss: 1.1084871292114258\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 6.667932510375977 | KNN Loss: 5.597422122955322 | BCE Loss: 1.0705103874206543\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 6.692958354949951 | KNN Loss: 5.64169454574585 | BCE Loss: 1.0512638092041016\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 6.659246921539307 | KNN Loss: 5.625386714935303 | BCE Loss: 1.033860206604004\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 6.697150707244873 | KNN Loss: 5.653909683227539 | BCE Loss: 1.0432411432266235\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 6.677496910095215 | KNN Loss: 5.634883880615234 | BCE Loss: 1.0426127910614014\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 6.668170928955078 | KNN Loss: 5.612023830413818 | BCE Loss: 1.0561470985412598\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 6.678357124328613 | KNN Loss: 5.614107608795166 | BCE Loss: 1.0642492771148682\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 6.7064208984375 | KNN Loss: 5.649046897888184 | BCE Loss: 1.0573742389678955\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 6.651521682739258 | KNN Loss: 5.598142147064209 | BCE Loss: 1.0533795356750488\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 6.656954765319824 | KNN Loss: 5.611705303192139 | BCE Loss: 1.0452492237091064\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 6.737102508544922 | KNN Loss: 5.6446356773376465 | BCE Loss: 1.0924668312072754\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 6.696780204772949 | KNN Loss: 5.6015191078186035 | BCE Loss: 1.0952608585357666\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 6.679477214813232 | KNN Loss: 5.6299238204956055 | BCE Loss: 1.0495535135269165\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 6.658111572265625 | KNN Loss: 5.611075401306152 | BCE Loss: 1.047036051750183\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 6.672725677490234 | KNN Loss: 5.614028453826904 | BCE Loss: 1.0586971044540405\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 6.675905227661133 | KNN Loss: 5.624195575714111 | BCE Loss: 1.0517094135284424\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 6.69073486328125 | KNN Loss: 5.5977044105529785 | BCE Loss: 1.0930306911468506\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 6.6634111404418945 | KNN Loss: 5.602121353149414 | BCE Loss: 1.0612897872924805\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 6.702548027038574 | KNN Loss: 5.643161296844482 | BCE Loss: 1.0593864917755127\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 6.663834571838379 | KNN Loss: 5.600553512573242 | BCE Loss: 1.0632809400558472\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 6.688806533813477 | KNN Loss: 5.640529155731201 | BCE Loss: 1.0482773780822754\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 6.618710517883301 | KNN Loss: 5.591443061828613 | BCE Loss: 1.0272676944732666\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 6.654616832733154 | KNN Loss: 5.607802867889404 | BCE Loss: 1.04681396484375\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 6.676575660705566 | KNN Loss: 5.604909896850586 | BCE Loss: 1.0716657638549805\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 6.691039085388184 | KNN Loss: 5.646200180053711 | BCE Loss: 1.0448386669158936\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 6.684062480926514 | KNN Loss: 5.630644798278809 | BCE Loss: 1.0534178018569946\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 6.736065864562988 | KNN Loss: 5.682641506195068 | BCE Loss: 1.0534241199493408\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 6.662736892700195 | KNN Loss: 5.627995491027832 | BCE Loss: 1.0347416400909424\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 6.688776016235352 | KNN Loss: 5.629853248596191 | BCE Loss: 1.0589227676391602\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 6.669187068939209 | KNN Loss: 5.607532501220703 | BCE Loss: 1.0616545677185059\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 6.673525810241699 | KNN Loss: 5.628585338592529 | BCE Loss: 1.0449402332305908\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 6.7222442626953125 | KNN Loss: 5.683733940124512 | BCE Loss: 1.0385102033615112\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 6.643204689025879 | KNN Loss: 5.600598335266113 | BCE Loss: 1.0426061153411865\n",
      "Epoch    62: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 6.648859977722168 | KNN Loss: 5.605772495269775 | BCE Loss: 1.0430872440338135\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 6.6529364585876465 | KNN Loss: 5.6021728515625 | BCE Loss: 1.050763726234436\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 6.674489974975586 | KNN Loss: 5.65419864654541 | BCE Loss: 1.0202915668487549\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 6.6632490158081055 | KNN Loss: 5.60685920715332 | BCE Loss: 1.0563900470733643\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 6.722591400146484 | KNN Loss: 5.672834873199463 | BCE Loss: 1.0497562885284424\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 6.7129082679748535 | KNN Loss: 5.624128341674805 | BCE Loss: 1.0887799263000488\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 6.646152019500732 | KNN Loss: 5.590463638305664 | BCE Loss: 1.0556883811950684\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 6.688093662261963 | KNN Loss: 5.608190536499023 | BCE Loss: 1.07990300655365\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 6.657998085021973 | KNN Loss: 5.600976943969727 | BCE Loss: 1.057020902633667\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 6.774190425872803 | KNN Loss: 5.694556713104248 | BCE Loss: 1.0796337127685547\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 6.67856502532959 | KNN Loss: 5.65471887588501 | BCE Loss: 1.02384614944458\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 6.650996685028076 | KNN Loss: 5.597786903381348 | BCE Loss: 1.0532097816467285\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 6.708539009094238 | KNN Loss: 5.650736331939697 | BCE Loss: 1.057802677154541\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 6.626789569854736 | KNN Loss: 5.59993839263916 | BCE Loss: 1.0268510580062866\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 6.723250865936279 | KNN Loss: 5.675397872924805 | BCE Loss: 1.0478529930114746\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 6.7020463943481445 | KNN Loss: 5.632113933563232 | BCE Loss: 1.069932222366333\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 6.679986000061035 | KNN Loss: 5.641556739807129 | BCE Loss: 1.0384293794631958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 6.693574905395508 | KNN Loss: 5.631575107574463 | BCE Loss: 1.062000036239624\n",
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 6.6827006340026855 | KNN Loss: 5.635884761810303 | BCE Loss: 1.0468159914016724\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 6.69195556640625 | KNN Loss: 5.6459879875183105 | BCE Loss: 1.0459678173065186\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 6.64630126953125 | KNN Loss: 5.613107204437256 | BCE Loss: 1.0331941843032837\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 6.692854881286621 | KNN Loss: 5.65071964263916 | BCE Loss: 1.0421350002288818\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 6.668036460876465 | KNN Loss: 5.6315155029296875 | BCE Loss: 1.0365209579467773\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 6.661787033081055 | KNN Loss: 5.6033549308776855 | BCE Loss: 1.0584322214126587\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 6.6590962409973145 | KNN Loss: 5.595683574676514 | BCE Loss: 1.0634127855300903\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 6.662289619445801 | KNN Loss: 5.61426305770874 | BCE Loss: 1.0480268001556396\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 6.708908557891846 | KNN Loss: 5.641857624053955 | BCE Loss: 1.0670509338378906\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 6.66147518157959 | KNN Loss: 5.608212947845459 | BCE Loss: 1.0532622337341309\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 6.67167329788208 | KNN Loss: 5.626034736633301 | BCE Loss: 1.0456385612487793\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 6.650462627410889 | KNN Loss: 5.628058433532715 | BCE Loss: 1.0224043130874634\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 6.711696624755859 | KNN Loss: 5.659032344818115 | BCE Loss: 1.0526645183563232\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 6.663998603820801 | KNN Loss: 5.621697902679443 | BCE Loss: 1.042300820350647\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 6.762918949127197 | KNN Loss: 5.712331295013428 | BCE Loss: 1.0505876541137695\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 6.655391693115234 | KNN Loss: 5.620662212371826 | BCE Loss: 1.034729242324829\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 6.653148651123047 | KNN Loss: 5.609440326690674 | BCE Loss: 1.043708086013794\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 6.653228759765625 | KNN Loss: 5.61100959777832 | BCE Loss: 1.0422194004058838\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 6.686826705932617 | KNN Loss: 5.634871959686279 | BCE Loss: 1.0519545078277588\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 6.681957244873047 | KNN Loss: 5.6213788986206055 | BCE Loss: 1.0605785846710205\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 6.7713470458984375 | KNN Loss: 5.700570583343506 | BCE Loss: 1.0707767009735107\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 6.72624397277832 | KNN Loss: 5.649608612060547 | BCE Loss: 1.0766355991363525\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 6.652041435241699 | KNN Loss: 5.6026740074157715 | BCE Loss: 1.0493675470352173\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 6.761774063110352 | KNN Loss: 5.693009376525879 | BCE Loss: 1.0687644481658936\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 6.742358207702637 | KNN Loss: 5.672137260437012 | BCE Loss: 1.070221185684204\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 6.651603698730469 | KNN Loss: 5.6070876121521 | BCE Loss: 1.0445163249969482\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 6.685112953186035 | KNN Loss: 5.639952659606934 | BCE Loss: 1.0451605319976807\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 6.700047016143799 | KNN Loss: 5.661262512207031 | BCE Loss: 1.038784384727478\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 6.690442085266113 | KNN Loss: 5.632831573486328 | BCE Loss: 1.0576107501983643\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 6.739327430725098 | KNN Loss: 5.674773693084717 | BCE Loss: 1.0645534992218018\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 6.629244804382324 | KNN Loss: 5.607797145843506 | BCE Loss: 1.0214475393295288\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 6.649449825286865 | KNN Loss: 5.612077236175537 | BCE Loss: 1.0373727083206177\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 6.614150524139404 | KNN Loss: 5.596827983856201 | BCE Loss: 1.0173225402832031\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 6.627585411071777 | KNN Loss: 5.599980354309082 | BCE Loss: 1.0276050567626953\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 6.659335613250732 | KNN Loss: 5.616657733917236 | BCE Loss: 1.0426779985427856\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 6.799830436706543 | KNN Loss: 5.699443340301514 | BCE Loss: 1.1003868579864502\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 6.679136276245117 | KNN Loss: 5.618234634399414 | BCE Loss: 1.0609018802642822\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 6.661194324493408 | KNN Loss: 5.618196487426758 | BCE Loss: 1.0429978370666504\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 6.719855308532715 | KNN Loss: 5.670451641082764 | BCE Loss: 1.0494035482406616\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 6.7112603187561035 | KNN Loss: 5.682024955749512 | BCE Loss: 1.0292354822158813\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 6.679069519042969 | KNN Loss: 5.624993801116943 | BCE Loss: 1.0540754795074463\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 6.688183784484863 | KNN Loss: 5.642873764038086 | BCE Loss: 1.0453102588653564\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 6.661309242248535 | KNN Loss: 5.600614547729492 | BCE Loss: 1.0606944561004639\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 6.706714630126953 | KNN Loss: 5.640007019042969 | BCE Loss: 1.0667074918746948\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 6.718916893005371 | KNN Loss: 5.670506000518799 | BCE Loss: 1.0484111309051514\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 6.707910060882568 | KNN Loss: 5.686744213104248 | BCE Loss: 1.0211657285690308\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 6.653594017028809 | KNN Loss: 5.618922233581543 | BCE Loss: 1.0346717834472656\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 6.721315383911133 | KNN Loss: 5.649086952209473 | BCE Loss: 1.0722283124923706\n",
      "Epoch    73: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 6.661243438720703 | KNN Loss: 5.602077960968018 | BCE Loss: 1.0591652393341064\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 6.659331321716309 | KNN Loss: 5.603801250457764 | BCE Loss: 1.0555298328399658\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 6.719927787780762 | KNN Loss: 5.656174182891846 | BCE Loss: 1.0637537240982056\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 6.659177780151367 | KNN Loss: 5.597057819366455 | BCE Loss: 1.0621201992034912\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 6.701503753662109 | KNN Loss: 5.6512837409973145 | BCE Loss: 1.050220251083374\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 6.7275567054748535 | KNN Loss: 5.693938255310059 | BCE Loss: 1.0336185693740845\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 6.676459789276123 | KNN Loss: 5.6208815574646 | BCE Loss: 1.0555782318115234\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 6.751543045043945 | KNN Loss: 5.720204830169678 | BCE Loss: 1.0313379764556885\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 6.65891695022583 | KNN Loss: 5.6086506843566895 | BCE Loss: 1.050266146659851\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 6.649059295654297 | KNN Loss: 5.606186389923096 | BCE Loss: 1.042872667312622\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 6.686664581298828 | KNN Loss: 5.654582500457764 | BCE Loss: 1.0320818424224854\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 6.659176826477051 | KNN Loss: 5.597289562225342 | BCE Loss: 1.061887264251709\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 6.672450065612793 | KNN Loss: 5.633477687835693 | BCE Loss: 1.0389723777770996\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 6.664216041564941 | KNN Loss: 5.603477954864502 | BCE Loss: 1.0607383251190186\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 6.633777141571045 | KNN Loss: 5.609858512878418 | BCE Loss: 1.0239187479019165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 6.672493934631348 | KNN Loss: 5.631849765777588 | BCE Loss: 1.0406444072723389\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 6.68533182144165 | KNN Loss: 5.639110565185547 | BCE Loss: 1.0462212562561035\n",
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 6.659724712371826 | KNN Loss: 5.6058831214904785 | BCE Loss: 1.0538417100906372\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 6.640063285827637 | KNN Loss: 5.595051288604736 | BCE Loss: 1.0450122356414795\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 6.692999839782715 | KNN Loss: 5.6311163902282715 | BCE Loss: 1.0618832111358643\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 6.657026290893555 | KNN Loss: 5.604086875915527 | BCE Loss: 1.0529394149780273\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 6.680678844451904 | KNN Loss: 5.628357887268066 | BCE Loss: 1.052320957183838\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 6.696191787719727 | KNN Loss: 5.639223098754883 | BCE Loss: 1.0569684505462646\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 6.701437473297119 | KNN Loss: 5.652669906616211 | BCE Loss: 1.0487675666809082\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 6.7149977684021 | KNN Loss: 5.681354522705078 | BCE Loss: 1.0336432456970215\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 6.683753490447998 | KNN Loss: 5.612497329711914 | BCE Loss: 1.071256160736084\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 6.6969709396362305 | KNN Loss: 5.643330097198486 | BCE Loss: 1.0536410808563232\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 6.654972076416016 | KNN Loss: 5.63466739654541 | BCE Loss: 1.020304799079895\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 6.621530055999756 | KNN Loss: 5.612729072570801 | BCE Loss: 1.008800983428955\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 6.662919044494629 | KNN Loss: 5.605180263519287 | BCE Loss: 1.0577387809753418\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 6.676946640014648 | KNN Loss: 5.64114236831665 | BCE Loss: 1.0358043909072876\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 6.8240580558776855 | KNN Loss: 5.790963172912598 | BCE Loss: 1.0330950021743774\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 6.699581146240234 | KNN Loss: 5.641746997833252 | BCE Loss: 1.0578340291976929\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 6.639806270599365 | KNN Loss: 5.594245433807373 | BCE Loss: 1.0455607175827026\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 6.692836761474609 | KNN Loss: 5.648820400238037 | BCE Loss: 1.0440161228179932\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 6.68266487121582 | KNN Loss: 5.597034931182861 | BCE Loss: 1.085630178451538\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 6.66993522644043 | KNN Loss: 5.615137577056885 | BCE Loss: 1.054797887802124\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 6.6559600830078125 | KNN Loss: 5.618053436279297 | BCE Loss: 1.0379064083099365\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 6.765091896057129 | KNN Loss: 5.720458507537842 | BCE Loss: 1.0446332693099976\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 6.685246467590332 | KNN Loss: 5.620372772216797 | BCE Loss: 1.0648738145828247\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 6.708530426025391 | KNN Loss: 5.658853054046631 | BCE Loss: 1.0496774911880493\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 6.654860019683838 | KNN Loss: 5.611097812652588 | BCE Loss: 1.0437623262405396\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 6.660595893859863 | KNN Loss: 5.597400188446045 | BCE Loss: 1.0631957054138184\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 6.6135759353637695 | KNN Loss: 5.5962748527526855 | BCE Loss: 1.017301321029663\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 6.641483783721924 | KNN Loss: 5.6065802574157715 | BCE Loss: 1.0349035263061523\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 6.6753926277160645 | KNN Loss: 5.628920078277588 | BCE Loss: 1.0464726686477661\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 6.672187805175781 | KNN Loss: 5.6191887855529785 | BCE Loss: 1.0529987812042236\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 6.67097806930542 | KNN Loss: 5.601768970489502 | BCE Loss: 1.069209098815918\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 6.660594940185547 | KNN Loss: 5.618772029876709 | BCE Loss: 1.041822910308838\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 6.718338489532471 | KNN Loss: 5.6867146492004395 | BCE Loss: 1.0316238403320312\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 6.657527446746826 | KNN Loss: 5.606587886810303 | BCE Loss: 1.050939679145813\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 6.649333953857422 | KNN Loss: 5.613588333129883 | BCE Loss: 1.035745620727539\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 6.667045593261719 | KNN Loss: 5.6161322593688965 | BCE Loss: 1.0509133338928223\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 6.648812770843506 | KNN Loss: 5.6258649826049805 | BCE Loss: 1.022947907447815\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 6.641534328460693 | KNN Loss: 5.611489772796631 | BCE Loss: 1.030044674873352\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 6.711440086364746 | KNN Loss: 5.664642333984375 | BCE Loss: 1.046797752380371\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 6.630832195281982 | KNN Loss: 5.615370750427246 | BCE Loss: 1.0154613256454468\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 6.655921459197998 | KNN Loss: 5.608221054077148 | BCE Loss: 1.04770028591156\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 6.700523853302002 | KNN Loss: 5.656088352203369 | BCE Loss: 1.0444353818893433\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 6.635045528411865 | KNN Loss: 5.593725681304932 | BCE Loss: 1.041319727897644\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 6.650320053100586 | KNN Loss: 5.601773738861084 | BCE Loss: 1.048546314239502\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 6.6538519859313965 | KNN Loss: 5.6207756996154785 | BCE Loss: 1.033076286315918\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 6.798579692840576 | KNN Loss: 5.767942905426025 | BCE Loss: 1.0306367874145508\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 6.674671173095703 | KNN Loss: 5.621103763580322 | BCE Loss: 1.05356764793396\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 6.64623498916626 | KNN Loss: 5.613430976867676 | BCE Loss: 1.0328038930892944\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 6.696530342102051 | KNN Loss: 5.639650821685791 | BCE Loss: 1.0568796396255493\n",
      "Epoch    84: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 6.717289924621582 | KNN Loss: 5.688671588897705 | BCE Loss: 1.0286180973052979\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 6.668124198913574 | KNN Loss: 5.626419544219971 | BCE Loss: 1.0417046546936035\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 6.660830020904541 | KNN Loss: 5.608676433563232 | BCE Loss: 1.052153468132019\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 6.714195251464844 | KNN Loss: 5.661029815673828 | BCE Loss: 1.0531655550003052\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 6.680338382720947 | KNN Loss: 5.609235763549805 | BCE Loss: 1.0711026191711426\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 6.682587146759033 | KNN Loss: 5.661657810211182 | BCE Loss: 1.0209293365478516\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 6.643056869506836 | KNN Loss: 5.597129821777344 | BCE Loss: 1.045926809310913\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 6.639959335327148 | KNN Loss: 5.598691463470459 | BCE Loss: 1.041267991065979\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 6.717343807220459 | KNN Loss: 5.660037040710449 | BCE Loss: 1.0573066473007202\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 6.658627510070801 | KNN Loss: 5.608468532562256 | BCE Loss: 1.050159215927124\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 6.67034387588501 | KNN Loss: 5.6184611320495605 | BCE Loss: 1.0518827438354492\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 6.673530578613281 | KNN Loss: 5.616048336029053 | BCE Loss: 1.057482123374939\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 6.680244445800781 | KNN Loss: 5.65020227432251 | BCE Loss: 1.0300421714782715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 6.664100170135498 | KNN Loss: 5.623289108276367 | BCE Loss: 1.0408110618591309\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 6.647641181945801 | KNN Loss: 5.602823734283447 | BCE Loss: 1.044817328453064\n",
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 6.627521514892578 | KNN Loss: 5.59689474105835 | BCE Loss: 1.030626893043518\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 6.645143985748291 | KNN Loss: 5.594326972961426 | BCE Loss: 1.0508170127868652\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 6.694102764129639 | KNN Loss: 5.666804790496826 | BCE Loss: 1.027298092842102\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 6.763680458068848 | KNN Loss: 5.734431266784668 | BCE Loss: 1.0292491912841797\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 6.7032976150512695 | KNN Loss: 5.658468723297119 | BCE Loss: 1.0448286533355713\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 6.6810078620910645 | KNN Loss: 5.637966632843018 | BCE Loss: 1.0430411100387573\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 6.674026012420654 | KNN Loss: 5.624911785125732 | BCE Loss: 1.0491143465042114\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 6.660022735595703 | KNN Loss: 5.600325584411621 | BCE Loss: 1.059696912765503\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 6.686999320983887 | KNN Loss: 5.606456279754639 | BCE Loss: 1.080542802810669\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 6.654998779296875 | KNN Loss: 5.620948314666748 | BCE Loss: 1.0340502262115479\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 6.637382984161377 | KNN Loss: 5.595988750457764 | BCE Loss: 1.0413942337036133\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 6.692631244659424 | KNN Loss: 5.6185078620910645 | BCE Loss: 1.0741233825683594\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 6.677927017211914 | KNN Loss: 5.6475443840026855 | BCE Loss: 1.0303828716278076\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 6.640900611877441 | KNN Loss: 5.596855640411377 | BCE Loss: 1.044045090675354\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 6.664096355438232 | KNN Loss: 5.595717906951904 | BCE Loss: 1.0683783292770386\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 6.658903121948242 | KNN Loss: 5.60721492767334 | BCE Loss: 1.0516881942749023\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 6.756054878234863 | KNN Loss: 5.703292369842529 | BCE Loss: 1.052762508392334\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 6.733115196228027 | KNN Loss: 5.667843341827393 | BCE Loss: 1.0652720928192139\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 6.610547065734863 | KNN Loss: 5.594945907592773 | BCE Loss: 1.0156009197235107\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 6.638375282287598 | KNN Loss: 5.600964546203613 | BCE Loss: 1.0374109745025635\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 6.659275054931641 | KNN Loss: 5.6256256103515625 | BCE Loss: 1.033649206161499\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 6.66408634185791 | KNN Loss: 5.633152484893799 | BCE Loss: 1.0309338569641113\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 6.649107456207275 | KNN Loss: 5.6017045974731445 | BCE Loss: 1.0474028587341309\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 6.65053653717041 | KNN Loss: 5.603610992431641 | BCE Loss: 1.0469253063201904\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 6.668252944946289 | KNN Loss: 5.603846073150635 | BCE Loss: 1.0644068717956543\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 6.687312126159668 | KNN Loss: 5.659712314605713 | BCE Loss: 1.0276000499725342\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 6.753515720367432 | KNN Loss: 5.698084354400635 | BCE Loss: 1.0554313659667969\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 6.7273054122924805 | KNN Loss: 5.669910430908203 | BCE Loss: 1.057395100593567\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 6.693453788757324 | KNN Loss: 5.668887138366699 | BCE Loss: 1.024566650390625\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 6.6857075691223145 | KNN Loss: 5.660398960113525 | BCE Loss: 1.0253087282180786\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 6.742664337158203 | KNN Loss: 5.681173801422119 | BCE Loss: 1.061490774154663\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 6.666367053985596 | KNN Loss: 5.626361846923828 | BCE Loss: 1.0400052070617676\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 6.648574352264404 | KNN Loss: 5.622237682342529 | BCE Loss: 1.0263365507125854\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 6.633785724639893 | KNN Loss: 5.592386722564697 | BCE Loss: 1.0413990020751953\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 6.698910713195801 | KNN Loss: 5.635157108306885 | BCE Loss: 1.063753366470337\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 6.623603820800781 | KNN Loss: 5.602980613708496 | BCE Loss: 1.0206233263015747\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 6.682918548583984 | KNN Loss: 5.621545791625977 | BCE Loss: 1.061372995376587\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 6.709148406982422 | KNN Loss: 5.64774227142334 | BCE Loss: 1.0614063739776611\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 6.681613922119141 | KNN Loss: 5.608817100524902 | BCE Loss: 1.0727968215942383\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 6.722265243530273 | KNN Loss: 5.638121128082275 | BCE Loss: 1.0841442346572876\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 6.761597156524658 | KNN Loss: 5.706866264343262 | BCE Loss: 1.0547308921813965\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 6.664485931396484 | KNN Loss: 5.5982136726379395 | BCE Loss: 1.0662720203399658\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 6.665164947509766 | KNN Loss: 5.61102294921875 | BCE Loss: 1.0541419982910156\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 6.672339916229248 | KNN Loss: 5.623363971710205 | BCE Loss: 1.0489760637283325\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 6.698637008666992 | KNN Loss: 5.663547039031982 | BCE Loss: 1.0350902080535889\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 6.660958766937256 | KNN Loss: 5.607970714569092 | BCE Loss: 1.0529881715774536\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 6.707544803619385 | KNN Loss: 5.664646625518799 | BCE Loss: 1.042898178100586\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 6.702683448791504 | KNN Loss: 5.622466564178467 | BCE Loss: 1.0802171230316162\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 6.632759094238281 | KNN Loss: 5.595874309539795 | BCE Loss: 1.0368846654891968\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 6.623678207397461 | KNN Loss: 5.6017537117004395 | BCE Loss: 1.0219247341156006\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 6.661688804626465 | KNN Loss: 5.622453212738037 | BCE Loss: 1.0392354726791382\n",
      "Epoch    95: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 6.745461940765381 | KNN Loss: 5.7009477615356445 | BCE Loss: 1.0445141792297363\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 6.744281768798828 | KNN Loss: 5.68231201171875 | BCE Loss: 1.0619696378707886\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 6.69843864440918 | KNN Loss: 5.66001558303833 | BCE Loss: 1.03842294216156\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 6.695107936859131 | KNN Loss: 5.645905494689941 | BCE Loss: 1.049202561378479\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 6.712989330291748 | KNN Loss: 5.65794563293457 | BCE Loss: 1.0550438165664673\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 6.67338228225708 | KNN Loss: 5.612136363983154 | BCE Loss: 1.0612459182739258\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 6.69199275970459 | KNN Loss: 5.642037868499756 | BCE Loss: 1.049954891204834\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 6.6846723556518555 | KNN Loss: 5.660141944885254 | BCE Loss: 1.0245304107666016\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 6.681728363037109 | KNN Loss: 5.6115641593933105 | BCE Loss: 1.070164442062378\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 6.659381866455078 | KNN Loss: 5.623591899871826 | BCE Loss: 1.035789966583252\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 6.65179967880249 | KNN Loss: 5.596085071563721 | BCE Loss: 1.05571448802948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 6.683658123016357 | KNN Loss: 5.6288743019104 | BCE Loss: 1.0547837018966675\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 6.722046852111816 | KNN Loss: 5.663301467895508 | BCE Loss: 1.0587456226348877\n",
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 6.70414924621582 | KNN Loss: 5.654600143432617 | BCE Loss: 1.0495493412017822\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 6.641899108886719 | KNN Loss: 5.594277381896973 | BCE Loss: 1.047621726989746\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 6.705392837524414 | KNN Loss: 5.687831401824951 | BCE Loss: 1.017561435699463\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 6.659313201904297 | KNN Loss: 5.62041711807251 | BCE Loss: 1.038896083831787\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 6.730095863342285 | KNN Loss: 5.668910980224609 | BCE Loss: 1.0611846446990967\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 6.609463691711426 | KNN Loss: 5.599545955657959 | BCE Loss: 1.0099174976348877\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 6.698131084442139 | KNN Loss: 5.644253253936768 | BCE Loss: 1.053877830505371\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 6.699639320373535 | KNN Loss: 5.651810646057129 | BCE Loss: 1.0478287935256958\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 6.687446594238281 | KNN Loss: 5.622687339782715 | BCE Loss: 1.0647591352462769\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 6.654521942138672 | KNN Loss: 5.611109733581543 | BCE Loss: 1.0434119701385498\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 6.637470722198486 | KNN Loss: 5.6013078689575195 | BCE Loss: 1.0361627340316772\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 6.643599510192871 | KNN Loss: 5.597280025482178 | BCE Loss: 1.0463192462921143\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 6.624241352081299 | KNN Loss: 5.599931240081787 | BCE Loss: 1.0243101119995117\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 6.640295028686523 | KNN Loss: 5.6034040451049805 | BCE Loss: 1.0368911027908325\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 6.703993797302246 | KNN Loss: 5.654266834259033 | BCE Loss: 1.0497267246246338\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 6.6979193687438965 | KNN Loss: 5.6389970779418945 | BCE Loss: 1.0589224100112915\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 6.634228229522705 | KNN Loss: 5.608725070953369 | BCE Loss: 1.025503158569336\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 6.6930389404296875 | KNN Loss: 5.631892204284668 | BCE Loss: 1.0611469745635986\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 6.646707057952881 | KNN Loss: 5.616326808929443 | BCE Loss: 1.030380129814148\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 6.634760856628418 | KNN Loss: 5.596615314483643 | BCE Loss: 1.0381455421447754\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 6.697575569152832 | KNN Loss: 5.634759902954102 | BCE Loss: 1.0628154277801514\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 6.648839950561523 | KNN Loss: 5.600886821746826 | BCE Loss: 1.0479531288146973\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 6.653435707092285 | KNN Loss: 5.597480773925781 | BCE Loss: 1.055955171585083\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 6.675816535949707 | KNN Loss: 5.636155128479004 | BCE Loss: 1.0396616458892822\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 6.768852710723877 | KNN Loss: 5.72951078414917 | BCE Loss: 1.039341926574707\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 6.784113883972168 | KNN Loss: 5.750601768493652 | BCE Loss: 1.0335123538970947\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 6.717832088470459 | KNN Loss: 5.686363697052002 | BCE Loss: 1.031468391418457\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 6.6527299880981445 | KNN Loss: 5.6077752113342285 | BCE Loss: 1.0449550151824951\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 6.647709369659424 | KNN Loss: 5.606216907501221 | BCE Loss: 1.0414924621582031\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 6.657337188720703 | KNN Loss: 5.607992649078369 | BCE Loss: 1.049344778060913\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 6.695973873138428 | KNN Loss: 5.636492729187012 | BCE Loss: 1.059481143951416\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 6.698469161987305 | KNN Loss: 5.678922176361084 | BCE Loss: 1.0195471048355103\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 6.67762565612793 | KNN Loss: 5.6199049949646 | BCE Loss: 1.0577205419540405\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 6.653146743774414 | KNN Loss: 5.620004653930664 | BCE Loss: 1.03314208984375\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 6.6376214027404785 | KNN Loss: 5.592432975769043 | BCE Loss: 1.0451884269714355\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 6.711145877838135 | KNN Loss: 5.668982982635498 | BCE Loss: 1.0421628952026367\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 6.629438877105713 | KNN Loss: 5.6044793128967285 | BCE Loss: 1.0249594449996948\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 6.649654865264893 | KNN Loss: 5.599719047546387 | BCE Loss: 1.0499358177185059\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 6.674689292907715 | KNN Loss: 5.598229885101318 | BCE Loss: 1.0764591693878174\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 6.684980392456055 | KNN Loss: 5.640864372253418 | BCE Loss: 1.0441157817840576\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 6.693548202514648 | KNN Loss: 5.653189182281494 | BCE Loss: 1.0403587818145752\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 6.667236328125 | KNN Loss: 5.624075412750244 | BCE Loss: 1.0431606769561768\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 6.619607925415039 | KNN Loss: 5.596773624420166 | BCE Loss: 1.0228345394134521\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 6.672304153442383 | KNN Loss: 5.631600379943848 | BCE Loss: 1.0407037734985352\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 6.691960334777832 | KNN Loss: 5.618249416351318 | BCE Loss: 1.0737106800079346\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 6.6882500648498535 | KNN Loss: 5.6351704597473145 | BCE Loss: 1.053079605102539\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 6.674625873565674 | KNN Loss: 5.623017311096191 | BCE Loss: 1.0516085624694824\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 6.6780900955200195 | KNN Loss: 5.615576267242432 | BCE Loss: 1.062514066696167\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 6.726845741271973 | KNN Loss: 5.702517986297607 | BCE Loss: 1.0243275165557861\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 6.646645545959473 | KNN Loss: 5.609312057495117 | BCE Loss: 1.0373334884643555\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 6.637417316436768 | KNN Loss: 5.595017910003662 | BCE Loss: 1.0423994064331055\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 6.66879940032959 | KNN Loss: 5.631532669067383 | BCE Loss: 1.0372669696807861\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 6.6976318359375 | KNN Loss: 5.645221710205078 | BCE Loss: 1.052410364151001\n",
      "Epoch   106: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 6.657440185546875 | KNN Loss: 5.602651119232178 | BCE Loss: 1.0547891855239868\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 6.662521839141846 | KNN Loss: 5.62147855758667 | BCE Loss: 1.0410432815551758\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 6.706905364990234 | KNN Loss: 5.651922225952148 | BCE Loss: 1.0549829006195068\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 6.634189605712891 | KNN Loss: 5.61214017868042 | BCE Loss: 1.0220494270324707\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 6.639682292938232 | KNN Loss: 5.599462985992432 | BCE Loss: 1.0402194261550903\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 6.690004348754883 | KNN Loss: 5.661245346069336 | BCE Loss: 1.0287590026855469\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 6.642917156219482 | KNN Loss: 5.610616683959961 | BCE Loss: 1.032300591468811\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 6.663079261779785 | KNN Loss: 5.629079818725586 | BCE Loss: 1.0339992046356201\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 6.637939453125 | KNN Loss: 5.624266624450684 | BCE Loss: 1.0136730670928955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 6.642608165740967 | KNN Loss: 5.6060051918029785 | BCE Loss: 1.0366028547286987\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 6.741386413574219 | KNN Loss: 5.677004337310791 | BCE Loss: 1.0643818378448486\n",
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 6.639183044433594 | KNN Loss: 5.590975761413574 | BCE Loss: 1.0482075214385986\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 6.624815940856934 | KNN Loss: 5.597254753112793 | BCE Loss: 1.0275614261627197\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 6.695888519287109 | KNN Loss: 5.635781764984131 | BCE Loss: 1.0601067543029785\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 6.660679340362549 | KNN Loss: 5.596388816833496 | BCE Loss: 1.0642904043197632\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 6.769515514373779 | KNN Loss: 5.711103916168213 | BCE Loss: 1.0584114789962769\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 6.655019760131836 | KNN Loss: 5.611400604248047 | BCE Loss: 1.0436193943023682\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 6.657588005065918 | KNN Loss: 5.596090793609619 | BCE Loss: 1.0614972114562988\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 6.686076641082764 | KNN Loss: 5.614279747009277 | BCE Loss: 1.0717968940734863\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 6.752228736877441 | KNN Loss: 5.694119930267334 | BCE Loss: 1.0581090450286865\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 6.651211261749268 | KNN Loss: 5.610655307769775 | BCE Loss: 1.0405558347702026\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 6.766939640045166 | KNN Loss: 5.740067958831787 | BCE Loss: 1.0268718004226685\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 6.663124084472656 | KNN Loss: 5.63889741897583 | BCE Loss: 1.0242266654968262\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 6.6523756980896 | KNN Loss: 5.618653297424316 | BCE Loss: 1.0337224006652832\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 6.705495834350586 | KNN Loss: 5.65164041519165 | BCE Loss: 1.0538556575775146\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 6.6420979499816895 | KNN Loss: 5.604941368103027 | BCE Loss: 1.037156581878662\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 6.642142295837402 | KNN Loss: 5.615252494812012 | BCE Loss: 1.0268898010253906\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 6.689289569854736 | KNN Loss: 5.635704040527344 | BCE Loss: 1.0535855293273926\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 6.62799072265625 | KNN Loss: 5.595361232757568 | BCE Loss: 1.0326294898986816\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 6.728292942047119 | KNN Loss: 5.687140464782715 | BCE Loss: 1.0411524772644043\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 6.654802322387695 | KNN Loss: 5.615788459777832 | BCE Loss: 1.0390137434005737\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 6.66359806060791 | KNN Loss: 5.632859230041504 | BCE Loss: 1.0307388305664062\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 6.651668071746826 | KNN Loss: 5.607476711273193 | BCE Loss: 1.0441914796829224\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 6.686996936798096 | KNN Loss: 5.650469779968262 | BCE Loss: 1.036527156829834\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 6.681147575378418 | KNN Loss: 5.605282783508301 | BCE Loss: 1.0758649110794067\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 6.681585311889648 | KNN Loss: 5.644284248352051 | BCE Loss: 1.0373013019561768\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 6.776342391967773 | KNN Loss: 5.685134410858154 | BCE Loss: 1.09120774269104\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 6.722633361816406 | KNN Loss: 5.629246711730957 | BCE Loss: 1.0933865308761597\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 6.643210411071777 | KNN Loss: 5.595652103424072 | BCE Loss: 1.0475581884384155\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 6.66989803314209 | KNN Loss: 5.605292797088623 | BCE Loss: 1.0646053552627563\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 6.663961887359619 | KNN Loss: 5.631717681884766 | BCE Loss: 1.0322442054748535\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 6.635682582855225 | KNN Loss: 5.608430862426758 | BCE Loss: 1.0272518396377563\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 6.644565105438232 | KNN Loss: 5.5974321365356445 | BCE Loss: 1.047132968902588\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 6.6533708572387695 | KNN Loss: 5.606262683868408 | BCE Loss: 1.0471081733703613\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 6.69054651260376 | KNN Loss: 5.642146587371826 | BCE Loss: 1.048399806022644\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 6.676093578338623 | KNN Loss: 5.6050214767456055 | BCE Loss: 1.0710721015930176\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 6.666919231414795 | KNN Loss: 5.611813068389893 | BCE Loss: 1.0551061630249023\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 6.657488822937012 | KNN Loss: 5.611802101135254 | BCE Loss: 1.0456866025924683\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 6.667788982391357 | KNN Loss: 5.62858247756958 | BCE Loss: 1.0392065048217773\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 6.6668901443481445 | KNN Loss: 5.620458602905273 | BCE Loss: 1.046431541442871\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 6.684453010559082 | KNN Loss: 5.638070583343506 | BCE Loss: 1.0463823080062866\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 6.682736396789551 | KNN Loss: 5.618680477142334 | BCE Loss: 1.064056158065796\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 6.673241138458252 | KNN Loss: 5.616001129150391 | BCE Loss: 1.0572398900985718\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 6.622511863708496 | KNN Loss: 5.60230016708374 | BCE Loss: 1.020211935043335\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 6.630951404571533 | KNN Loss: 5.605823516845703 | BCE Loss: 1.0251280069351196\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 6.675313949584961 | KNN Loss: 5.609299182891846 | BCE Loss: 1.0660150051116943\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 6.727806091308594 | KNN Loss: 5.667763710021973 | BCE Loss: 1.0600426197052002\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 6.719348430633545 | KNN Loss: 5.660951137542725 | BCE Loss: 1.0583971738815308\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 6.696924209594727 | KNN Loss: 5.660698890686035 | BCE Loss: 1.0362250804901123\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 6.722596168518066 | KNN Loss: 5.646528720855713 | BCE Loss: 1.076067566871643\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 6.665153503417969 | KNN Loss: 5.606266498565674 | BCE Loss: 1.0588867664337158\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 6.6527099609375 | KNN Loss: 5.598974704742432 | BCE Loss: 1.0537351369857788\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 6.657170295715332 | KNN Loss: 5.6105194091796875 | BCE Loss: 1.0466508865356445\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 6.753358364105225 | KNN Loss: 5.690173625946045 | BCE Loss: 1.0631847381591797\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 6.6568779945373535 | KNN Loss: 5.605095386505127 | BCE Loss: 1.0517827272415161\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 6.655467510223389 | KNN Loss: 5.615931987762451 | BCE Loss: 1.039535641670227\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 6.640259742736816 | KNN Loss: 5.597104072570801 | BCE Loss: 1.0431554317474365\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 6.6614460945129395 | KNN Loss: 5.620511531829834 | BCE Loss: 1.040934443473816\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 6.633091449737549 | KNN Loss: 5.598886966705322 | BCE Loss: 1.0342046022415161\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 6.676236629486084 | KNN Loss: 5.615435600280762 | BCE Loss: 1.0608011484146118\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 6.705781936645508 | KNN Loss: 5.637423992156982 | BCE Loss: 1.0683579444885254\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 6.648416519165039 | KNN Loss: 5.616824626922607 | BCE Loss: 1.0315918922424316\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 6.675179481506348 | KNN Loss: 5.622384548187256 | BCE Loss: 1.052795171737671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 6.678328514099121 | KNN Loss: 5.629777431488037 | BCE Loss: 1.048551082611084\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 6.655971527099609 | KNN Loss: 5.613584995269775 | BCE Loss: 1.0423866510391235\n",
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 6.6489949226379395 | KNN Loss: 5.593647480010986 | BCE Loss: 1.0553474426269531\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 6.629653453826904 | KNN Loss: 5.592667579650879 | BCE Loss: 1.0369858741760254\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 6.696819305419922 | KNN Loss: 5.645941734313965 | BCE Loss: 1.050877332687378\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 6.667258262634277 | KNN Loss: 5.598446846008301 | BCE Loss: 1.0688115358352661\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 6.6602325439453125 | KNN Loss: 5.6095051765441895 | BCE Loss: 1.0507274866104126\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 6.633196830749512 | KNN Loss: 5.603232383728027 | BCE Loss: 1.0299644470214844\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 6.686213493347168 | KNN Loss: 5.6176557540893555 | BCE Loss: 1.0685579776763916\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 6.652329921722412 | KNN Loss: 5.61176061630249 | BCE Loss: 1.0405691862106323\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 6.704190254211426 | KNN Loss: 5.659158706665039 | BCE Loss: 1.0450314283370972\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 6.676966667175293 | KNN Loss: 5.599673748016357 | BCE Loss: 1.0772931575775146\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 6.664817810058594 | KNN Loss: 5.634899616241455 | BCE Loss: 1.0299181938171387\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 6.676438808441162 | KNN Loss: 5.599327564239502 | BCE Loss: 1.0771111249923706\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 6.659681797027588 | KNN Loss: 5.59060525894165 | BCE Loss: 1.069076657295227\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 6.696911334991455 | KNN Loss: 5.639475345611572 | BCE Loss: 1.0574359893798828\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 6.640084743499756 | KNN Loss: 5.598118782043457 | BCE Loss: 1.0419659614562988\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 6.662818908691406 | KNN Loss: 5.5949578285217285 | BCE Loss: 1.0678613185882568\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 6.603504180908203 | KNN Loss: 5.599435329437256 | BCE Loss: 1.0040690898895264\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 6.683415412902832 | KNN Loss: 5.634881973266602 | BCE Loss: 1.0485336780548096\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 6.721085548400879 | KNN Loss: 5.67689847946167 | BCE Loss: 1.044187307357788\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 6.6887993812561035 | KNN Loss: 5.626165390014648 | BCE Loss: 1.062633991241455\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 6.633575439453125 | KNN Loss: 5.606429100036621 | BCE Loss: 1.0271461009979248\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 6.734340190887451 | KNN Loss: 5.677840709686279 | BCE Loss: 1.0564994812011719\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 6.664051055908203 | KNN Loss: 5.606435298919678 | BCE Loss: 1.0576159954071045\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 6.667801856994629 | KNN Loss: 5.59946346282959 | BCE Loss: 1.068338394165039\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 6.686870574951172 | KNN Loss: 5.640529632568359 | BCE Loss: 1.0463409423828125\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 6.6998395919799805 | KNN Loss: 5.637950897216797 | BCE Loss: 1.0618888139724731\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 6.717035293579102 | KNN Loss: 5.6527252197265625 | BCE Loss: 1.0643103122711182\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 6.7021989822387695 | KNN Loss: 5.6186628341674805 | BCE Loss: 1.0835363864898682\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 6.688138484954834 | KNN Loss: 5.664999485015869 | BCE Loss: 1.0231388807296753\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 6.628813743591309 | KNN Loss: 5.602951526641846 | BCE Loss: 1.0258619785308838\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 6.691118240356445 | KNN Loss: 5.6249260902404785 | BCE Loss: 1.0661919116973877\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 6.6562299728393555 | KNN Loss: 5.613632678985596 | BCE Loss: 1.0425970554351807\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 6.63444185256958 | KNN Loss: 5.603114604949951 | BCE Loss: 1.031327247619629\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 6.707243919372559 | KNN Loss: 5.660022258758545 | BCE Loss: 1.0472214221954346\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 6.635272026062012 | KNN Loss: 5.601064682006836 | BCE Loss: 1.0342071056365967\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 6.664359092712402 | KNN Loss: 5.611288070678711 | BCE Loss: 1.0530710220336914\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 6.667149066925049 | KNN Loss: 5.61875581741333 | BCE Loss: 1.0483932495117188\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 6.655622482299805 | KNN Loss: 5.601348876953125 | BCE Loss: 1.0542736053466797\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 6.6331257820129395 | KNN Loss: 5.60687780380249 | BCE Loss: 1.0262480974197388\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 6.69948673248291 | KNN Loss: 5.660102844238281 | BCE Loss: 1.0393836498260498\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 6.656423568725586 | KNN Loss: 5.611639499664307 | BCE Loss: 1.0447843074798584\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 6.618579387664795 | KNN Loss: 5.603161811828613 | BCE Loss: 1.0154175758361816\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 6.657415866851807 | KNN Loss: 5.629837989807129 | BCE Loss: 1.0275779962539673\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 6.608367919921875 | KNN Loss: 5.597814083099365 | BCE Loss: 1.0105538368225098\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 6.656148433685303 | KNN Loss: 5.592728137969971 | BCE Loss: 1.063420295715332\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 6.678164958953857 | KNN Loss: 5.624512195587158 | BCE Loss: 1.0536527633666992\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 6.6501054763793945 | KNN Loss: 5.5976080894470215 | BCE Loss: 1.0524972677230835\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 6.69407844543457 | KNN Loss: 5.635702610015869 | BCE Loss: 1.0583757162094116\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 6.65585994720459 | KNN Loss: 5.611210346221924 | BCE Loss: 1.044649362564087\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 6.761900901794434 | KNN Loss: 5.719761848449707 | BCE Loss: 1.0421388149261475\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 6.698565483093262 | KNN Loss: 5.641414165496826 | BCE Loss: 1.0571510791778564\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 6.735546112060547 | KNN Loss: 5.659440994262695 | BCE Loss: 1.076104998588562\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 6.707522869110107 | KNN Loss: 5.635770797729492 | BCE Loss: 1.0717520713806152\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 6.656707763671875 | KNN Loss: 5.621472358703613 | BCE Loss: 1.0352356433868408\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 6.652303695678711 | KNN Loss: 5.617323875427246 | BCE Loss: 1.0349798202514648\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 6.719568252563477 | KNN Loss: 5.660831451416016 | BCE Loss: 1.05873703956604\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 6.698976993560791 | KNN Loss: 5.6533002853393555 | BCE Loss: 1.045676589012146\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 6.649359703063965 | KNN Loss: 5.599120616912842 | BCE Loss: 1.0502393245697021\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 6.686511039733887 | KNN Loss: 5.658318042755127 | BCE Loss: 1.0281932353973389\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 6.642612457275391 | KNN Loss: 5.597796440124512 | BCE Loss: 1.0448157787322998\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 6.705584526062012 | KNN Loss: 5.669074058532715 | BCE Loss: 1.036510705947876\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 6.648643493652344 | KNN Loss: 5.596370220184326 | BCE Loss: 1.0522730350494385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 6.6838274002075195 | KNN Loss: 5.640278339385986 | BCE Loss: 1.0435492992401123\n",
      "Epoch   129: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 6.678582191467285 | KNN Loss: 5.610419273376465 | BCE Loss: 1.0681626796722412\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 6.720404148101807 | KNN Loss: 5.661571025848389 | BCE Loss: 1.0588332414627075\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 6.707280158996582 | KNN Loss: 5.63295841217041 | BCE Loss: 1.0743218660354614\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 6.618492603302002 | KNN Loss: 5.598173141479492 | BCE Loss: 1.0203193426132202\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 6.632275581359863 | KNN Loss: 5.598455905914307 | BCE Loss: 1.0338199138641357\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 6.628574848175049 | KNN Loss: 5.598291873931885 | BCE Loss: 1.030282974243164\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 6.6128339767456055 | KNN Loss: 5.598029613494873 | BCE Loss: 1.0148041248321533\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 6.702797889709473 | KNN Loss: 5.649080276489258 | BCE Loss: 1.0537176132202148\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 6.667117595672607 | KNN Loss: 5.6319098472595215 | BCE Loss: 1.035207748413086\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 6.681396484375 | KNN Loss: 5.61720609664917 | BCE Loss: 1.0641906261444092\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 6.753443241119385 | KNN Loss: 5.658345699310303 | BCE Loss: 1.0950974225997925\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 6.752743721008301 | KNN Loss: 5.698990821838379 | BCE Loss: 1.0537527799606323\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 6.632510185241699 | KNN Loss: 5.59929895401001 | BCE Loss: 1.0332112312316895\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 6.73385763168335 | KNN Loss: 5.688065052032471 | BCE Loss: 1.045792579650879\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 6.726325035095215 | KNN Loss: 5.685479640960693 | BCE Loss: 1.0408451557159424\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 6.628380298614502 | KNN Loss: 5.596832752227783 | BCE Loss: 1.0315474271774292\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 6.652272701263428 | KNN Loss: 5.61142110824585 | BCE Loss: 1.0408515930175781\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 6.71176290512085 | KNN Loss: 5.678777694702148 | BCE Loss: 1.0329852104187012\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 6.648410320281982 | KNN Loss: 5.616665363311768 | BCE Loss: 1.0317448377609253\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 6.740602016448975 | KNN Loss: 5.669973850250244 | BCE Loss: 1.070628046989441\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 6.627570152282715 | KNN Loss: 5.6035566329956055 | BCE Loss: 1.0240134000778198\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 6.675013542175293 | KNN Loss: 5.628658771514893 | BCE Loss: 1.0463550090789795\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 6.672106742858887 | KNN Loss: 5.599764347076416 | BCE Loss: 1.0723426342010498\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 6.710011959075928 | KNN Loss: 5.6837239265441895 | BCE Loss: 1.0262880325317383\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 6.674989700317383 | KNN Loss: 5.634915828704834 | BCE Loss: 1.0400737524032593\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 6.714069366455078 | KNN Loss: 5.660290718078613 | BCE Loss: 1.0537787675857544\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 6.6605329513549805 | KNN Loss: 5.609283924102783 | BCE Loss: 1.0512490272521973\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 6.689105987548828 | KNN Loss: 5.650295734405518 | BCE Loss: 1.0388104915618896\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 6.632913589477539 | KNN Loss: 5.6050705909729 | BCE Loss: 1.0278427600860596\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 6.66318416595459 | KNN Loss: 5.599671840667725 | BCE Loss: 1.0635123252868652\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 6.691687107086182 | KNN Loss: 5.634231090545654 | BCE Loss: 1.0574560165405273\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 6.655261039733887 | KNN Loss: 5.596943378448486 | BCE Loss: 1.05831778049469\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 6.6583356857299805 | KNN Loss: 5.592114448547363 | BCE Loss: 1.0662213563919067\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 6.627508640289307 | KNN Loss: 5.601075649261475 | BCE Loss: 1.0264331102371216\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 6.686025142669678 | KNN Loss: 5.613333702087402 | BCE Loss: 1.0726914405822754\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 6.651920318603516 | KNN Loss: 5.601649761199951 | BCE Loss: 1.0502707958221436\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 6.64164924621582 | KNN Loss: 5.60177755355835 | BCE Loss: 1.0398714542388916\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 6.670534133911133 | KNN Loss: 5.60992956161499 | BCE Loss: 1.060604453086853\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 6.671679973602295 | KNN Loss: 5.645265102386475 | BCE Loss: 1.0264148712158203\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 6.650856971740723 | KNN Loss: 5.624439716339111 | BCE Loss: 1.0264174938201904\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 6.664968013763428 | KNN Loss: 5.596145153045654 | BCE Loss: 1.0688228607177734\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 6.71185302734375 | KNN Loss: 5.642441749572754 | BCE Loss: 1.0694113969802856\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 6.635436534881592 | KNN Loss: 5.602157115936279 | BCE Loss: 1.0332794189453125\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 6.6914873123168945 | KNN Loss: 5.655118465423584 | BCE Loss: 1.0363688468933105\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 6.6611714363098145 | KNN Loss: 5.634545803070068 | BCE Loss: 1.0266255140304565\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 6.667785167694092 | KNN Loss: 5.640421390533447 | BCE Loss: 1.0273637771606445\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 6.7413434982299805 | KNN Loss: 5.681896686553955 | BCE Loss: 1.0594465732574463\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 6.699506759643555 | KNN Loss: 5.627305507659912 | BCE Loss: 1.0722014904022217\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 6.67185115814209 | KNN Loss: 5.648743152618408 | BCE Loss: 1.0231077671051025\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 6.644330024719238 | KNN Loss: 5.616003036499023 | BCE Loss: 1.028327226638794\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 6.668181419372559 | KNN Loss: 5.638388156890869 | BCE Loss: 1.0297932624816895\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 6.651070594787598 | KNN Loss: 5.597754955291748 | BCE Loss: 1.0533156394958496\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 6.7302141189575195 | KNN Loss: 5.65831184387207 | BCE Loss: 1.0719025135040283\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 6.6316819190979 | KNN Loss: 5.595231056213379 | BCE Loss: 1.0364508628845215\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 6.662147521972656 | KNN Loss: 5.619060516357422 | BCE Loss: 1.0430872440338135\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 6.656685829162598 | KNN Loss: 5.600743293762207 | BCE Loss: 1.0559422969818115\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 6.662776470184326 | KNN Loss: 5.600000858306885 | BCE Loss: 1.0627756118774414\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 6.6339311599731445 | KNN Loss: 5.595934867858887 | BCE Loss: 1.0379960536956787\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 6.7364115715026855 | KNN Loss: 5.673496723175049 | BCE Loss: 1.0629149675369263\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 6.664289474487305 | KNN Loss: 5.613278388977051 | BCE Loss: 1.0510108470916748\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 6.7811784744262695 | KNN Loss: 5.742656230926514 | BCE Loss: 1.0385220050811768\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 6.707716464996338 | KNN Loss: 5.649075984954834 | BCE Loss: 1.0586405992507935\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 6.650718688964844 | KNN Loss: 5.594743728637695 | BCE Loss: 1.0559749603271484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 6.7515716552734375 | KNN Loss: 5.6953020095825195 | BCE Loss: 1.0562697649002075\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 6.707491874694824 | KNN Loss: 5.649453163146973 | BCE Loss: 1.0580384731292725\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 6.681277275085449 | KNN Loss: 5.642602443695068 | BCE Loss: 1.03867506980896\n",
      "Epoch   140: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 6.649196624755859 | KNN Loss: 5.592665672302246 | BCE Loss: 1.0565309524536133\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 6.717127323150635 | KNN Loss: 5.6642937660217285 | BCE Loss: 1.0528335571289062\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 6.63722562789917 | KNN Loss: 5.596168518066406 | BCE Loss: 1.0410571098327637\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 6.703810214996338 | KNN Loss: 5.6550798416137695 | BCE Loss: 1.0487302541732788\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 6.654179573059082 | KNN Loss: 5.603606224060059 | BCE Loss: 1.0505731105804443\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 6.669405937194824 | KNN Loss: 5.623434066772461 | BCE Loss: 1.0459717512130737\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 6.6440911293029785 | KNN Loss: 5.600688457489014 | BCE Loss: 1.0434026718139648\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 6.650863170623779 | KNN Loss: 5.607987403869629 | BCE Loss: 1.04287588596344\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 6.702934265136719 | KNN Loss: 5.65314245223999 | BCE Loss: 1.049791932106018\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 6.770283222198486 | KNN Loss: 5.717064380645752 | BCE Loss: 1.0532187223434448\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 6.684605598449707 | KNN Loss: 5.6372270584106445 | BCE Loss: 1.0473785400390625\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 6.636177062988281 | KNN Loss: 5.609311103820801 | BCE Loss: 1.0268657207489014\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 6.670936584472656 | KNN Loss: 5.618968486785889 | BCE Loss: 1.0519680976867676\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 6.737767219543457 | KNN Loss: 5.655455589294434 | BCE Loss: 1.082311749458313\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 6.692791938781738 | KNN Loss: 5.616750717163086 | BCE Loss: 1.0760414600372314\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 6.69852876663208 | KNN Loss: 5.65684700012207 | BCE Loss: 1.0416818857192993\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 6.646744251251221 | KNN Loss: 5.609007835388184 | BCE Loss: 1.0377362966537476\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 6.604515075683594 | KNN Loss: 5.589143753051758 | BCE Loss: 1.015371322631836\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 6.7074174880981445 | KNN Loss: 5.6809163093566895 | BCE Loss: 1.026501178741455\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 6.6633524894714355 | KNN Loss: 5.634069442749023 | BCE Loss: 1.0292829275131226\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 6.689827919006348 | KNN Loss: 5.6542134284973145 | BCE Loss: 1.035614252090454\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 6.672765254974365 | KNN Loss: 5.623987674713135 | BCE Loss: 1.04877769947052\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 6.763468265533447 | KNN Loss: 5.692666530609131 | BCE Loss: 1.0708017349243164\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 6.6577324867248535 | KNN Loss: 5.596010684967041 | BCE Loss: 1.0617218017578125\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 6.681304931640625 | KNN Loss: 5.607982635498047 | BCE Loss: 1.0733224153518677\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 6.646763324737549 | KNN Loss: 5.594792366027832 | BCE Loss: 1.0519709587097168\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 6.751201629638672 | KNN Loss: 5.701337814331055 | BCE Loss: 1.0498640537261963\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 6.643991470336914 | KNN Loss: 5.605857849121094 | BCE Loss: 1.0381336212158203\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 6.620326995849609 | KNN Loss: 5.599358558654785 | BCE Loss: 1.0209686756134033\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 6.711737155914307 | KNN Loss: 5.654959678649902 | BCE Loss: 1.0567773580551147\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 6.659689426422119 | KNN Loss: 5.612398147583008 | BCE Loss: 1.0472912788391113\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 6.678925514221191 | KNN Loss: 5.6427321434021 | BCE Loss: 1.0361933708190918\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 6.674105167388916 | KNN Loss: 5.640650272369385 | BCE Loss: 1.0334550142288208\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 6.66450834274292 | KNN Loss: 5.638583660125732 | BCE Loss: 1.025924563407898\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 6.67941951751709 | KNN Loss: 5.601015567779541 | BCE Loss: 1.0784039497375488\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 6.702516555786133 | KNN Loss: 5.667729377746582 | BCE Loss: 1.0347869396209717\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 6.650349140167236 | KNN Loss: 5.618934631347656 | BCE Loss: 1.0314146280288696\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 6.690753936767578 | KNN Loss: 5.649505138397217 | BCE Loss: 1.0412489175796509\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 6.666278839111328 | KNN Loss: 5.608609199523926 | BCE Loss: 1.0576694011688232\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 6.702660083770752 | KNN Loss: 5.658973217010498 | BCE Loss: 1.0436867475509644\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 6.771501064300537 | KNN Loss: 5.691845893859863 | BCE Loss: 1.0796551704406738\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 6.736032009124756 | KNN Loss: 5.665594100952148 | BCE Loss: 1.0704379081726074\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 6.73618221282959 | KNN Loss: 5.67957878112793 | BCE Loss: 1.0566036701202393\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 6.753702640533447 | KNN Loss: 5.670595169067383 | BCE Loss: 1.083107352256775\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 6.642689228057861 | KNN Loss: 5.597541809082031 | BCE Loss: 1.0451475381851196\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 6.639841079711914 | KNN Loss: 5.613148212432861 | BCE Loss: 1.0266926288604736\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 6.636328220367432 | KNN Loss: 5.615532398223877 | BCE Loss: 1.0207959413528442\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 6.696884632110596 | KNN Loss: 5.67202091217041 | BCE Loss: 1.024863600730896\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 6.746410369873047 | KNN Loss: 5.69874906539917 | BCE Loss: 1.0476611852645874\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 6.631331920623779 | KNN Loss: 5.6127190589904785 | BCE Loss: 1.0186129808425903\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 6.644550323486328 | KNN Loss: 5.596769332885742 | BCE Loss: 1.0477811098098755\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 6.62507438659668 | KNN Loss: 5.604607105255127 | BCE Loss: 1.0204672813415527\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 6.642084121704102 | KNN Loss: 5.604848384857178 | BCE Loss: 1.0372354984283447\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 6.767083168029785 | KNN Loss: 5.709223747253418 | BCE Loss: 1.0578593015670776\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 6.6323018074035645 | KNN Loss: 5.594281196594238 | BCE Loss: 1.0380207300186157\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 6.6920647621154785 | KNN Loss: 5.65026330947876 | BCE Loss: 1.0418014526367188\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 6.682877540588379 | KNN Loss: 5.610799312591553 | BCE Loss: 1.0720781087875366\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 6.682957172393799 | KNN Loss: 5.644471645355225 | BCE Loss: 1.0384855270385742\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 6.6189985275268555 | KNN Loss: 5.60652494430542 | BCE Loss: 1.0124735832214355\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 6.7043914794921875 | KNN Loss: 5.652217864990234 | BCE Loss: 1.0521737337112427\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 6.657649040222168 | KNN Loss: 5.598630428314209 | BCE Loss: 1.0590183734893799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 6.66447114944458 | KNN Loss: 5.623578071594238 | BCE Loss: 1.0408930778503418\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 6.648674011230469 | KNN Loss: 5.615788459777832 | BCE Loss: 1.0328853130340576\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 6.769718647003174 | KNN Loss: 5.743386745452881 | BCE Loss: 1.026331901550293\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 6.6746368408203125 | KNN Loss: 5.597639083862305 | BCE Loss: 1.0769977569580078\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 6.636116981506348 | KNN Loss: 5.593885898590088 | BCE Loss: 1.0422309637069702\n",
      "Epoch   151: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 6.675058841705322 | KNN Loss: 5.6185808181762695 | BCE Loss: 1.0564779043197632\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 6.666398048400879 | KNN Loss: 5.64592170715332 | BCE Loss: 1.0204764604568481\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 6.667575836181641 | KNN Loss: 5.615875244140625 | BCE Loss: 1.0517008304595947\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 6.636735439300537 | KNN Loss: 5.590151309967041 | BCE Loss: 1.0465840101242065\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 6.657379627227783 | KNN Loss: 5.594563961029053 | BCE Loss: 1.06281578540802\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 6.631843566894531 | KNN Loss: 5.595820903778076 | BCE Loss: 1.0360225439071655\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 6.687681674957275 | KNN Loss: 5.630143642425537 | BCE Loss: 1.0575380325317383\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 6.641048431396484 | KNN Loss: 5.595513820648193 | BCE Loss: 1.045534610748291\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 6.665921688079834 | KNN Loss: 5.631738662719727 | BCE Loss: 1.0341829061508179\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 6.709372520446777 | KNN Loss: 5.635270118713379 | BCE Loss: 1.0741022825241089\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 6.6543731689453125 | KNN Loss: 5.617739677429199 | BCE Loss: 1.0366332530975342\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 6.658841133117676 | KNN Loss: 5.599939346313477 | BCE Loss: 1.0589017868041992\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 6.729020118713379 | KNN Loss: 5.672877788543701 | BCE Loss: 1.0561424493789673\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 6.764803886413574 | KNN Loss: 5.69351053237915 | BCE Loss: 1.071293592453003\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 6.724913120269775 | KNN Loss: 5.666604042053223 | BCE Loss: 1.0583090782165527\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 6.698550701141357 | KNN Loss: 5.657390594482422 | BCE Loss: 1.041159987449646\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 6.633749961853027 | KNN Loss: 5.591061115264893 | BCE Loss: 1.0426886081695557\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 6.656713485717773 | KNN Loss: 5.63519287109375 | BCE Loss: 1.0215208530426025\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 6.644216060638428 | KNN Loss: 5.599124431610107 | BCE Loss: 1.0450916290283203\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 6.613441467285156 | KNN Loss: 5.596268653869629 | BCE Loss: 1.0171725749969482\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 6.647698879241943 | KNN Loss: 5.605861186981201 | BCE Loss: 1.0418375730514526\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 6.6883063316345215 | KNN Loss: 5.662291049957275 | BCE Loss: 1.026015281677246\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 6.638463973999023 | KNN Loss: 5.609428882598877 | BCE Loss: 1.0290350914001465\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 6.642447471618652 | KNN Loss: 5.598913192749023 | BCE Loss: 1.043534278869629\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 6.732025146484375 | KNN Loss: 5.689654350280762 | BCE Loss: 1.0423709154129028\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 6.689703941345215 | KNN Loss: 5.635683536529541 | BCE Loss: 1.054020643234253\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 6.6700439453125 | KNN Loss: 5.600943565368652 | BCE Loss: 1.0691001415252686\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 6.663405895233154 | KNN Loss: 5.633499622344971 | BCE Loss: 1.0299062728881836\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 6.668073654174805 | KNN Loss: 5.629873752593994 | BCE Loss: 1.0381999015808105\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 6.671920299530029 | KNN Loss: 5.633012771606445 | BCE Loss: 1.038907527923584\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 6.765002250671387 | KNN Loss: 5.695179462432861 | BCE Loss: 1.0698230266571045\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 6.642541885375977 | KNN Loss: 5.598569393157959 | BCE Loss: 1.0439724922180176\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 6.730711936950684 | KNN Loss: 5.685313701629639 | BCE Loss: 1.045398235321045\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 6.648577690124512 | KNN Loss: 5.605685234069824 | BCE Loss: 1.042892575263977\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 6.8409624099731445 | KNN Loss: 5.760852336883545 | BCE Loss: 1.0801103115081787\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 6.662845611572266 | KNN Loss: 5.614180564880371 | BCE Loss: 1.0486648082733154\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 6.681779861450195 | KNN Loss: 5.630741596221924 | BCE Loss: 1.0510380268096924\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 6.664371490478516 | KNN Loss: 5.63979434967041 | BCE Loss: 1.0245773792266846\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 6.671492576599121 | KNN Loss: 5.630646705627441 | BCE Loss: 1.0408458709716797\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 6.677429676055908 | KNN Loss: 5.626895904541016 | BCE Loss: 1.0505337715148926\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 6.735715389251709 | KNN Loss: 5.688943386077881 | BCE Loss: 1.0467718839645386\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 6.614509582519531 | KNN Loss: 5.5948028564453125 | BCE Loss: 1.0197067260742188\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 6.641360282897949 | KNN Loss: 5.6090593338012695 | BCE Loss: 1.0323007106781006\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 6.718100547790527 | KNN Loss: 5.664376735687256 | BCE Loss: 1.0537240505218506\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 6.697043418884277 | KNN Loss: 5.642163276672363 | BCE Loss: 1.054880142211914\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 6.67962121963501 | KNN Loss: 5.651211738586426 | BCE Loss: 1.028409481048584\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 6.664614200592041 | KNN Loss: 5.598041534423828 | BCE Loss: 1.066572666168213\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 6.632203102111816 | KNN Loss: 5.602219581604004 | BCE Loss: 1.0299837589263916\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 6.720894813537598 | KNN Loss: 5.6619062423706055 | BCE Loss: 1.058988332748413\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 6.654799938201904 | KNN Loss: 5.6183061599731445 | BCE Loss: 1.0364937782287598\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 6.692961692810059 | KNN Loss: 5.6476006507873535 | BCE Loss: 1.0453612804412842\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 6.712643623352051 | KNN Loss: 5.637401580810547 | BCE Loss: 1.0752418041229248\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 6.688772201538086 | KNN Loss: 5.652960777282715 | BCE Loss: 1.0358113050460815\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 6.662019729614258 | KNN Loss: 5.623807430267334 | BCE Loss: 1.0382120609283447\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 6.738615989685059 | KNN Loss: 5.690710067749023 | BCE Loss: 1.0479058027267456\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 6.676769733428955 | KNN Loss: 5.613691329956055 | BCE Loss: 1.0630782842636108\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 6.642945289611816 | KNN Loss: 5.590265274047852 | BCE Loss: 1.0526800155639648\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 6.685551166534424 | KNN Loss: 5.628747940063477 | BCE Loss: 1.0568032264709473\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 6.715620040893555 | KNN Loss: 5.665759086608887 | BCE Loss: 1.049860954284668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 6.736371994018555 | KNN Loss: 5.700174331665039 | BCE Loss: 1.0361976623535156\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 6.644770622253418 | KNN Loss: 5.600738048553467 | BCE Loss: 1.044032335281372\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 6.67933988571167 | KNN Loss: 5.614498615264893 | BCE Loss: 1.0648412704467773\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 6.73223876953125 | KNN Loss: 5.656249523162842 | BCE Loss: 1.075989007949829\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 6.681676864624023 | KNN Loss: 5.630151748657227 | BCE Loss: 1.051525354385376\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 6.716960906982422 | KNN Loss: 5.653599262237549 | BCE Loss: 1.0633618831634521\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 6.643570899963379 | KNN Loss: 5.605375289916992 | BCE Loss: 1.0381954908370972\n",
      "Epoch   162: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 6.671864032745361 | KNN Loss: 5.6353864669799805 | BCE Loss: 1.0364775657653809\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 6.670549392700195 | KNN Loss: 5.624617576599121 | BCE Loss: 1.0459315776824951\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 6.68340539932251 | KNN Loss: 5.616422653198242 | BCE Loss: 1.0669827461242676\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 6.664059638977051 | KNN Loss: 5.600214958190918 | BCE Loss: 1.063844919204712\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 6.687263488769531 | KNN Loss: 5.627456188201904 | BCE Loss: 1.0598070621490479\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 6.629214286804199 | KNN Loss: 5.59771203994751 | BCE Loss: 1.0315024852752686\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 6.712983131408691 | KNN Loss: 5.659438133239746 | BCE Loss: 1.0535447597503662\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 6.703314781188965 | KNN Loss: 5.638320446014404 | BCE Loss: 1.0649940967559814\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 6.69204044342041 | KNN Loss: 5.643539905548096 | BCE Loss: 1.0485005378723145\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 6.717705726623535 | KNN Loss: 5.666120529174805 | BCE Loss: 1.0515854358673096\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 6.648320198059082 | KNN Loss: 5.595265865325928 | BCE Loss: 1.0530545711517334\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 6.628380298614502 | KNN Loss: 5.596823215484619 | BCE Loss: 1.0315569639205933\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 6.774477481842041 | KNN Loss: 5.717233657836914 | BCE Loss: 1.057243824005127\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 6.621965408325195 | KNN Loss: 5.620236873626709 | BCE Loss: 1.0017284154891968\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 6.667910575866699 | KNN Loss: 5.604694843292236 | BCE Loss: 1.0632154941558838\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 6.7028279304504395 | KNN Loss: 5.662614345550537 | BCE Loss: 1.0402135848999023\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 6.693887710571289 | KNN Loss: 5.652763843536377 | BCE Loss: 1.041123628616333\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 6.650572776794434 | KNN Loss: 5.642651081085205 | BCE Loss: 1.0079216957092285\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 6.696597099304199 | KNN Loss: 5.645129203796387 | BCE Loss: 1.0514676570892334\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 6.762423515319824 | KNN Loss: 5.723289489746094 | BCE Loss: 1.0391340255737305\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 6.678638458251953 | KNN Loss: 5.601482391357422 | BCE Loss: 1.0771560668945312\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 6.639346122741699 | KNN Loss: 5.614783763885498 | BCE Loss: 1.0245622396469116\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 6.674144744873047 | KNN Loss: 5.666106700897217 | BCE Loss: 1.00803804397583\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 6.674498558044434 | KNN Loss: 5.623663425445557 | BCE Loss: 1.0508348941802979\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 6.6942644119262695 | KNN Loss: 5.629305362701416 | BCE Loss: 1.0649592876434326\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 6.652122974395752 | KNN Loss: 5.637137413024902 | BCE Loss: 1.01498544216156\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 6.663276672363281 | KNN Loss: 5.5916056632995605 | BCE Loss: 1.0716712474822998\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 6.721317768096924 | KNN Loss: 5.654210090637207 | BCE Loss: 1.0671077966690063\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 6.666665077209473 | KNN Loss: 5.629617691040039 | BCE Loss: 1.037047266960144\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 6.654860496520996 | KNN Loss: 5.612419605255127 | BCE Loss: 1.0424407720565796\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 6.666792869567871 | KNN Loss: 5.630448818206787 | BCE Loss: 1.0363438129425049\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 6.7139177322387695 | KNN Loss: 5.654147148132324 | BCE Loss: 1.0597708225250244\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 6.690276622772217 | KNN Loss: 5.641779899597168 | BCE Loss: 1.0484966039657593\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 6.66295862197876 | KNN Loss: 5.621819019317627 | BCE Loss: 1.0411397218704224\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 6.6731085777282715 | KNN Loss: 5.622017860412598 | BCE Loss: 1.0510907173156738\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 6.678386211395264 | KNN Loss: 5.627502918243408 | BCE Loss: 1.050883173942566\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 6.654108047485352 | KNN Loss: 5.605068206787109 | BCE Loss: 1.0490398406982422\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 6.690812110900879 | KNN Loss: 5.639624118804932 | BCE Loss: 1.0511878728866577\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 6.656865119934082 | KNN Loss: 5.619415283203125 | BCE Loss: 1.037449836730957\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 6.6543378829956055 | KNN Loss: 5.5976409912109375 | BCE Loss: 1.056696891784668\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 6.6570940017700195 | KNN Loss: 5.616738319396973 | BCE Loss: 1.0403554439544678\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 6.649012088775635 | KNN Loss: 5.591963291168213 | BCE Loss: 1.0570486783981323\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 6.6851301193237305 | KNN Loss: 5.661893367767334 | BCE Loss: 1.0232367515563965\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 6.64007568359375 | KNN Loss: 5.596260070800781 | BCE Loss: 1.0438158512115479\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 6.682295799255371 | KNN Loss: 5.656020641326904 | BCE Loss: 1.026275396347046\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 6.659848213195801 | KNN Loss: 5.6156206130981445 | BCE Loss: 1.0442276000976562\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 6.620639324188232 | KNN Loss: 5.5960612297058105 | BCE Loss: 1.0245780944824219\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 6.683899879455566 | KNN Loss: 5.609322547912598 | BCE Loss: 1.0745773315429688\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 6.657814025878906 | KNN Loss: 5.6071014404296875 | BCE Loss: 1.0507123470306396\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 6.678757190704346 | KNN Loss: 5.631475448608398 | BCE Loss: 1.0472817420959473\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 6.644913673400879 | KNN Loss: 5.595602989196777 | BCE Loss: 1.0493106842041016\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 6.7672200202941895 | KNN Loss: 5.6981353759765625 | BCE Loss: 1.069084644317627\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 6.737497329711914 | KNN Loss: 5.676348686218262 | BCE Loss: 1.0611488819122314\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 6.669748306274414 | KNN Loss: 5.612765312194824 | BCE Loss: 1.0569831132888794\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 6.699906349182129 | KNN Loss: 5.667013645172119 | BCE Loss: 1.0328924655914307\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 6.6779890060424805 | KNN Loss: 5.635117530822754 | BCE Loss: 1.0428714752197266\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 6.642086029052734 | KNN Loss: 5.598964214324951 | BCE Loss: 1.0431219339370728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 6.672609329223633 | KNN Loss: 5.614200592041016 | BCE Loss: 1.0584087371826172\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 6.671189308166504 | KNN Loss: 5.604657173156738 | BCE Loss: 1.0665323734283447\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 6.685840606689453 | KNN Loss: 5.62651252746582 | BCE Loss: 1.0593281984329224\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 6.709750175476074 | KNN Loss: 5.644826889038086 | BCE Loss: 1.0649232864379883\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 6.632444381713867 | KNN Loss: 5.5940728187561035 | BCE Loss: 1.0383718013763428\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 6.663352012634277 | KNN Loss: 5.617891788482666 | BCE Loss: 1.0454599857330322\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 6.643396377563477 | KNN Loss: 5.596458435058594 | BCE Loss: 1.0469379425048828\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 6.7014055252075195 | KNN Loss: 5.6603922843933105 | BCE Loss: 1.041013479232788\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 6.665815353393555 | KNN Loss: 5.605064392089844 | BCE Loss: 1.060750961303711\n",
      "Epoch   173: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 6.639122009277344 | KNN Loss: 5.606744289398193 | BCE Loss: 1.0323776006698608\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 6.639405250549316 | KNN Loss: 5.591278553009033 | BCE Loss: 1.0481265783309937\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 6.742031574249268 | KNN Loss: 5.678843975067139 | BCE Loss: 1.063187599182129\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 6.679569721221924 | KNN Loss: 5.633551120758057 | BCE Loss: 1.0460184812545776\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 6.6232452392578125 | KNN Loss: 5.607382297515869 | BCE Loss: 1.0158629417419434\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 6.6715545654296875 | KNN Loss: 5.619617938995361 | BCE Loss: 1.0519368648529053\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 6.665870666503906 | KNN Loss: 5.623881816864014 | BCE Loss: 1.0419888496398926\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 6.683332443237305 | KNN Loss: 5.62843132019043 | BCE Loss: 1.054901123046875\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 6.683876991271973 | KNN Loss: 5.605292797088623 | BCE Loss: 1.0785839557647705\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 6.711459636688232 | KNN Loss: 5.652339458465576 | BCE Loss: 1.0591201782226562\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 6.665289402008057 | KNN Loss: 5.641988277435303 | BCE Loss: 1.023301124572754\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 6.635218143463135 | KNN Loss: 5.597415447235107 | BCE Loss: 1.0378025770187378\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 6.661466598510742 | KNN Loss: 5.60455846786499 | BCE Loss: 1.056908130645752\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 6.721520900726318 | KNN Loss: 5.6633501052856445 | BCE Loss: 1.0581707954406738\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 6.65581750869751 | KNN Loss: 5.608489036560059 | BCE Loss: 1.0473285913467407\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 6.6625590324401855 | KNN Loss: 5.620444297790527 | BCE Loss: 1.0421146154403687\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 6.663727760314941 | KNN Loss: 5.614835262298584 | BCE Loss: 1.0488922595977783\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 6.679018974304199 | KNN Loss: 5.633469104766846 | BCE Loss: 1.0455501079559326\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 6.688941955566406 | KNN Loss: 5.6395792961120605 | BCE Loss: 1.0493628978729248\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 6.668823719024658 | KNN Loss: 5.65461540222168 | BCE Loss: 1.014208197593689\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 6.6612653732299805 | KNN Loss: 5.594053268432617 | BCE Loss: 1.0672121047973633\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 6.6601762771606445 | KNN Loss: 5.602611064910889 | BCE Loss: 1.0575650930404663\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 6.67858362197876 | KNN Loss: 5.626425266265869 | BCE Loss: 1.0521583557128906\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 6.704809188842773 | KNN Loss: 5.642568111419678 | BCE Loss: 1.0622413158416748\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 6.657293319702148 | KNN Loss: 5.6181488037109375 | BCE Loss: 1.03914475440979\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 6.729760646820068 | KNN Loss: 5.673269748687744 | BCE Loss: 1.0564908981323242\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 6.662524223327637 | KNN Loss: 5.628750324249268 | BCE Loss: 1.0337740182876587\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 6.688207149505615 | KNN Loss: 5.628791332244873 | BCE Loss: 1.0594158172607422\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 6.621822357177734 | KNN Loss: 5.602489471435547 | BCE Loss: 1.0193326473236084\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 6.623385429382324 | KNN Loss: 5.593851566314697 | BCE Loss: 1.0295339822769165\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 6.633101940155029 | KNN Loss: 5.611288070678711 | BCE Loss: 1.0218137502670288\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 6.713271141052246 | KNN Loss: 5.63670015335083 | BCE Loss: 1.0765708684921265\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 6.687911510467529 | KNN Loss: 5.627461910247803 | BCE Loss: 1.0604496002197266\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 6.649956226348877 | KNN Loss: 5.6027421951293945 | BCE Loss: 1.0472140312194824\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 6.646188735961914 | KNN Loss: 5.589363098144531 | BCE Loss: 1.0568256378173828\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 6.704440116882324 | KNN Loss: 5.65190315246582 | BCE Loss: 1.052537202835083\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 6.629974842071533 | KNN Loss: 5.593015670776367 | BCE Loss: 1.0369592905044556\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 6.684796333312988 | KNN Loss: 5.606794834136963 | BCE Loss: 1.0780012607574463\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 6.6734232902526855 | KNN Loss: 5.611147880554199 | BCE Loss: 1.0622754096984863\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 6.6579718589782715 | KNN Loss: 5.615932464599609 | BCE Loss: 1.0420392751693726\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 6.64484977722168 | KNN Loss: 5.5999345779418945 | BCE Loss: 1.044914960861206\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 6.645691871643066 | KNN Loss: 5.600620746612549 | BCE Loss: 1.0450711250305176\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 6.643085956573486 | KNN Loss: 5.598489284515381 | BCE Loss: 1.0445966720581055\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 6.642681121826172 | KNN Loss: 5.607859134674072 | BCE Loss: 1.0348217487335205\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 6.622764587402344 | KNN Loss: 5.595871448516846 | BCE Loss: 1.026892900466919\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 6.653819561004639 | KNN Loss: 5.60962438583374 | BCE Loss: 1.0441951751708984\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 6.6758575439453125 | KNN Loss: 5.630519390106201 | BCE Loss: 1.0453383922576904\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 6.7513041496276855 | KNN Loss: 5.692102432250977 | BCE Loss: 1.059201717376709\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 6.698233127593994 | KNN Loss: 5.646556377410889 | BCE Loss: 1.051676630973816\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 6.6681952476501465 | KNN Loss: 5.597179412841797 | BCE Loss: 1.0710158348083496\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 6.62433385848999 | KNN Loss: 5.602989673614502 | BCE Loss: 1.0213443040847778\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 6.701796531677246 | KNN Loss: 5.6441330909729 | BCE Loss: 1.0576634407043457\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 6.679731369018555 | KNN Loss: 5.643949508666992 | BCE Loss: 1.035781979560852\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 6.671112060546875 | KNN Loss: 5.643989562988281 | BCE Loss: 1.0271224975585938\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 6.616648197174072 | KNN Loss: 5.60996675491333 | BCE Loss: 1.0066814422607422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 6.6579790115356445 | KNN Loss: 5.60611629486084 | BCE Loss: 1.0518629550933838\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 6.65657901763916 | KNN Loss: 5.617825031280518 | BCE Loss: 1.0387539863586426\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 6.627588748931885 | KNN Loss: 5.598017692565918 | BCE Loss: 1.0295711755752563\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 6.6400251388549805 | KNN Loss: 5.601463794708252 | BCE Loss: 1.038561224937439\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 6.749052047729492 | KNN Loss: 5.688633441925049 | BCE Loss: 1.0604186058044434\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 6.694167613983154 | KNN Loss: 5.645124912261963 | BCE Loss: 1.0490427017211914\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 6.670742034912109 | KNN Loss: 5.617212772369385 | BCE Loss: 1.0535290241241455\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 6.670945167541504 | KNN Loss: 5.626399993896484 | BCE Loss: 1.0445454120635986\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 6.664775371551514 | KNN Loss: 5.636768341064453 | BCE Loss: 1.0280070304870605\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 6.727332592010498 | KNN Loss: 5.662217140197754 | BCE Loss: 1.0651154518127441\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 6.6236572265625 | KNN Loss: 5.5966386795043945 | BCE Loss: 1.027018666267395\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 6.664429187774658 | KNN Loss: 5.600307464599609 | BCE Loss: 1.0641216039657593\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 6.717856407165527 | KNN Loss: 5.644248962402344 | BCE Loss: 1.0736072063446045\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 6.625753879547119 | KNN Loss: 5.6038126945495605 | BCE Loss: 1.0219411849975586\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 6.753641128540039 | KNN Loss: 5.658262252807617 | BCE Loss: 1.095379114151001\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 6.632236003875732 | KNN Loss: 5.605628490447998 | BCE Loss: 1.0266075134277344\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 6.664772987365723 | KNN Loss: 5.638820648193359 | BCE Loss: 1.0259523391723633\n",
      "Epoch   185: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 6.6804070472717285 | KNN Loss: 5.607364654541016 | BCE Loss: 1.0730422735214233\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 6.689183712005615 | KNN Loss: 5.656266212463379 | BCE Loss: 1.0329173803329468\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 6.657127380371094 | KNN Loss: 5.600590705871582 | BCE Loss: 1.0565366744995117\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 6.656096458435059 | KNN Loss: 5.616967678070068 | BCE Loss: 1.0391287803649902\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 6.737067222595215 | KNN Loss: 5.683952331542969 | BCE Loss: 1.0531151294708252\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 6.6528191566467285 | KNN Loss: 5.60991096496582 | BCE Loss: 1.0429081916809082\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 6.764526844024658 | KNN Loss: 5.71143102645874 | BCE Loss: 1.053095817565918\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 6.754683494567871 | KNN Loss: 5.679975509643555 | BCE Loss: 1.0747079849243164\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 6.61727237701416 | KNN Loss: 5.603858947753906 | BCE Loss: 1.0134135484695435\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 6.662663459777832 | KNN Loss: 5.589641571044922 | BCE Loss: 1.0730220079421997\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 6.618437767028809 | KNN Loss: 5.618157863616943 | BCE Loss: 1.0002799034118652\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 6.607665061950684 | KNN Loss: 5.595733642578125 | BCE Loss: 1.0119316577911377\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 6.613356590270996 | KNN Loss: 5.592741966247559 | BCE Loss: 1.020614743232727\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 6.72690486907959 | KNN Loss: 5.654723167419434 | BCE Loss: 1.0721819400787354\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 6.708086013793945 | KNN Loss: 5.688701152801514 | BCE Loss: 1.0193846225738525\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 6.64326810836792 | KNN Loss: 5.5957183837890625 | BCE Loss: 1.0475496053695679\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 6.777005672454834 | KNN Loss: 5.701283931732178 | BCE Loss: 1.0757217407226562\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 6.67635440826416 | KNN Loss: 5.655455589294434 | BCE Loss: 1.0208988189697266\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 6.679442882537842 | KNN Loss: 5.6168532371521 | BCE Loss: 1.0625896453857422\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 6.679513931274414 | KNN Loss: 5.624075412750244 | BCE Loss: 1.0554382801055908\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 6.748966217041016 | KNN Loss: 5.728437423706055 | BCE Loss: 1.0205285549163818\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 6.685235977172852 | KNN Loss: 5.686376571655273 | BCE Loss: 0.9988595247268677\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 6.765748500823975 | KNN Loss: 5.692728519439697 | BCE Loss: 1.073020100593567\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 6.707840919494629 | KNN Loss: 5.653284549713135 | BCE Loss: 1.0545566082000732\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 6.736195087432861 | KNN Loss: 5.690615177154541 | BCE Loss: 1.0455797910690308\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 6.664331436157227 | KNN Loss: 5.600230693817139 | BCE Loss: 1.0641005039215088\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 6.648670673370361 | KNN Loss: 5.617563247680664 | BCE Loss: 1.0311075448989868\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 6.673386573791504 | KNN Loss: 5.604063034057617 | BCE Loss: 1.0693237781524658\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 6.647952556610107 | KNN Loss: 5.640477657318115 | BCE Loss: 1.0074748992919922\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 6.648623466491699 | KNN Loss: 5.597387790679932 | BCE Loss: 1.0512359142303467\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 6.665970802307129 | KNN Loss: 5.60719108581543 | BCE Loss: 1.0587798357009888\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 6.659445762634277 | KNN Loss: 5.607876777648926 | BCE Loss: 1.0515689849853516\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 6.662370681762695 | KNN Loss: 5.615097522735596 | BCE Loss: 1.0472733974456787\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 6.678225517272949 | KNN Loss: 5.598057746887207 | BCE Loss: 1.0801677703857422\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 6.628705978393555 | KNN Loss: 5.611342906951904 | BCE Loss: 1.01736319065094\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 6.650676727294922 | KNN Loss: 5.611706733703613 | BCE Loss: 1.0389702320098877\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 6.796660423278809 | KNN Loss: 5.753376007080078 | BCE Loss: 1.0432844161987305\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 6.717009544372559 | KNN Loss: 5.684037685394287 | BCE Loss: 1.032971739768982\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 6.634853363037109 | KNN Loss: 5.597370624542236 | BCE Loss: 1.037482738494873\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 6.641563415527344 | KNN Loss: 5.5998969078063965 | BCE Loss: 1.0416665077209473\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 6.6771159172058105 | KNN Loss: 5.6489410400390625 | BCE Loss: 1.0281747579574585\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 6.690178394317627 | KNN Loss: 5.644681453704834 | BCE Loss: 1.045496940612793\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 6.635553359985352 | KNN Loss: 5.590827465057373 | BCE Loss: 1.0447258949279785\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 6.6610026359558105 | KNN Loss: 5.619056701660156 | BCE Loss: 1.0419460535049438\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 6.644459247589111 | KNN Loss: 5.6129302978515625 | BCE Loss: 1.0315290689468384\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 6.678731918334961 | KNN Loss: 5.619351387023926 | BCE Loss: 1.0593804121017456\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 6.737404823303223 | KNN Loss: 5.665832042694092 | BCE Loss: 1.0715726613998413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 6.779690742492676 | KNN Loss: 5.72421932220459 | BCE Loss: 1.055471658706665\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 6.629465103149414 | KNN Loss: 5.598260402679443 | BCE Loss: 1.0312045812606812\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 6.6223859786987305 | KNN Loss: 5.592004299163818 | BCE Loss: 1.0303819179534912\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 6.634976863861084 | KNN Loss: 5.60638952255249 | BCE Loss: 1.0285872220993042\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 6.666520595550537 | KNN Loss: 5.605529308319092 | BCE Loss: 1.0609912872314453\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 6.65282678604126 | KNN Loss: 5.595981121063232 | BCE Loss: 1.0568456649780273\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 6.672272205352783 | KNN Loss: 5.628424167633057 | BCE Loss: 1.0438481569290161\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 6.707898139953613 | KNN Loss: 5.636188507080078 | BCE Loss: 1.0717095136642456\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 6.668415546417236 | KNN Loss: 5.616914749145508 | BCE Loss: 1.051500678062439\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 6.658884525299072 | KNN Loss: 5.616307735443115 | BCE Loss: 1.042576789855957\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 6.632640838623047 | KNN Loss: 5.596400737762451 | BCE Loss: 1.0362398624420166\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 6.663797855377197 | KNN Loss: 5.604672431945801 | BCE Loss: 1.059125304222107\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 6.668992042541504 | KNN Loss: 5.634970188140869 | BCE Loss: 1.0340218544006348\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 6.695806503295898 | KNN Loss: 5.648952007293701 | BCE Loss: 1.0468543767929077\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 6.665831565856934 | KNN Loss: 5.622564315795898 | BCE Loss: 1.043267011642456\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 6.674539566040039 | KNN Loss: 5.63348388671875 | BCE Loss: 1.0410555601119995\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 6.668939590454102 | KNN Loss: 5.6474738121032715 | BCE Loss: 1.021465539932251\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 6.69448709487915 | KNN Loss: 5.647370338439941 | BCE Loss: 1.047116756439209\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 6.64917516708374 | KNN Loss: 5.619684219360352 | BCE Loss: 1.0294909477233887\n",
      "Epoch   196: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 6.648806571960449 | KNN Loss: 5.616371154785156 | BCE Loss: 1.0324352979660034\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 6.646324157714844 | KNN Loss: 5.603163719177246 | BCE Loss: 1.0431606769561768\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 6.678335666656494 | KNN Loss: 5.632655143737793 | BCE Loss: 1.0456805229187012\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 6.637142181396484 | KNN Loss: 5.602293014526367 | BCE Loss: 1.0348492860794067\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 6.651134014129639 | KNN Loss: 5.592639446258545 | BCE Loss: 1.0584945678710938\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 6.702698230743408 | KNN Loss: 5.660943984985352 | BCE Loss: 1.0417542457580566\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 6.655514717102051 | KNN Loss: 5.598714828491211 | BCE Loss: 1.0568000078201294\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 6.708099365234375 | KNN Loss: 5.6784281730651855 | BCE Loss: 1.0296709537506104\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 6.649130344390869 | KNN Loss: 5.600543975830078 | BCE Loss: 1.048586368560791\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 6.663550853729248 | KNN Loss: 5.60080099105835 | BCE Loss: 1.0627497434616089\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 6.6193342208862305 | KNN Loss: 5.59573221206665 | BCE Loss: 1.02360200881958\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 6.680067539215088 | KNN Loss: 5.600320339202881 | BCE Loss: 1.079747200012207\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 6.643580436706543 | KNN Loss: 5.595099449157715 | BCE Loss: 1.048480749130249\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 6.809861183166504 | KNN Loss: 5.732094764709473 | BCE Loss: 1.0777664184570312\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 6.636443138122559 | KNN Loss: 5.599850177764893 | BCE Loss: 1.0365931987762451\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 6.6315693855285645 | KNN Loss: 5.6007866859436035 | BCE Loss: 1.030782699584961\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 6.670008182525635 | KNN Loss: 5.64483118057251 | BCE Loss: 1.025177001953125\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 6.629948616027832 | KNN Loss: 5.605031967163086 | BCE Loss: 1.024916648864746\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 6.7132978439331055 | KNN Loss: 5.6840386390686035 | BCE Loss: 1.029259443283081\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 6.647128105163574 | KNN Loss: 5.597298622131348 | BCE Loss: 1.0498294830322266\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 6.707728862762451 | KNN Loss: 5.643126010894775 | BCE Loss: 1.0646028518676758\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 6.686473846435547 | KNN Loss: 5.6308112144470215 | BCE Loss: 1.0556628704071045\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 6.7658867835998535 | KNN Loss: 5.687341690063477 | BCE Loss: 1.0785449743270874\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 6.693219184875488 | KNN Loss: 5.64382266998291 | BCE Loss: 1.0493967533111572\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 6.629657745361328 | KNN Loss: 5.620612144470215 | BCE Loss: 1.0090458393096924\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 6.691397666931152 | KNN Loss: 5.620811462402344 | BCE Loss: 1.0705862045288086\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 6.627022743225098 | KNN Loss: 5.6020612716674805 | BCE Loss: 1.0249614715576172\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 6.6576619148254395 | KNN Loss: 5.644066333770752 | BCE Loss: 1.013595461845398\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 6.71647310256958 | KNN Loss: 5.622241020202637 | BCE Loss: 1.0942320823669434\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 6.679269790649414 | KNN Loss: 5.628564834594727 | BCE Loss: 1.0507049560546875\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 6.6501970291137695 | KNN Loss: 5.595996856689453 | BCE Loss: 1.0542000532150269\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 6.638999938964844 | KNN Loss: 5.591297626495361 | BCE Loss: 1.0477020740509033\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 6.6755828857421875 | KNN Loss: 5.627182483673096 | BCE Loss: 1.048400640487671\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 6.687437057495117 | KNN Loss: 5.639828205108643 | BCE Loss: 1.0476086139678955\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 6.631290435791016 | KNN Loss: 5.599889755249023 | BCE Loss: 1.031400442123413\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 6.668554306030273 | KNN Loss: 5.629519462585449 | BCE Loss: 1.0390350818634033\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 6.615110874176025 | KNN Loss: 5.595399379730225 | BCE Loss: 1.0197113752365112\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 6.636406421661377 | KNN Loss: 5.609415531158447 | BCE Loss: 1.0269908905029297\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 6.640134811401367 | KNN Loss: 5.621392726898193 | BCE Loss: 1.0187418460845947\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 6.652455806732178 | KNN Loss: 5.608784198760986 | BCE Loss: 1.0436714887619019\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 6.658387184143066 | KNN Loss: 5.597740650177002 | BCE Loss: 1.0606462955474854\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 6.70148229598999 | KNN Loss: 5.6581034660339355 | BCE Loss: 1.0433788299560547\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 6.70034122467041 | KNN Loss: 5.652218818664551 | BCE Loss: 1.0481224060058594\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 6.69149112701416 | KNN Loss: 5.64838981628418 | BCE Loss: 1.043101191520691\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 6.660608291625977 | KNN Loss: 5.611186981201172 | BCE Loss: 1.0494213104248047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 6.7042083740234375 | KNN Loss: 5.667392253875732 | BCE Loss: 1.0368163585662842\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 6.643509387969971 | KNN Loss: 5.610869884490967 | BCE Loss: 1.0326393842697144\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 6.670121192932129 | KNN Loss: 5.634101390838623 | BCE Loss: 1.036020040512085\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 6.665125846862793 | KNN Loss: 5.618555068969727 | BCE Loss: 1.0465705394744873\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 6.727392196655273 | KNN Loss: 5.683957099914551 | BCE Loss: 1.0434348583221436\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 6.670970916748047 | KNN Loss: 5.602113246917725 | BCE Loss: 1.0688579082489014\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 6.6792216300964355 | KNN Loss: 5.635622024536133 | BCE Loss: 1.0435994863510132\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 6.770442485809326 | KNN Loss: 5.7156147956848145 | BCE Loss: 1.0548275709152222\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 6.64434814453125 | KNN Loss: 5.60978889465332 | BCE Loss: 1.0345594882965088\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 6.6282057762146 | KNN Loss: 5.596848011016846 | BCE Loss: 1.0313578844070435\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 6.652903079986572 | KNN Loss: 5.604054927825928 | BCE Loss: 1.0488481521606445\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 6.64603328704834 | KNN Loss: 5.590826034545898 | BCE Loss: 1.0552072525024414\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 6.680426597595215 | KNN Loss: 5.635444164276123 | BCE Loss: 1.0449825525283813\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 6.639837741851807 | KNN Loss: 5.592151165008545 | BCE Loss: 1.0476865768432617\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 6.68466854095459 | KNN Loss: 5.610391139984131 | BCE Loss: 1.074277639389038\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 6.672096252441406 | KNN Loss: 5.6165947914123535 | BCE Loss: 1.0555012226104736\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 6.66855001449585 | KNN Loss: 5.605443477630615 | BCE Loss: 1.0631065368652344\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 6.686836242675781 | KNN Loss: 5.622823715209961 | BCE Loss: 1.0640127658843994\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 6.677252292633057 | KNN Loss: 5.656003475189209 | BCE Loss: 1.0212488174438477\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 6.69032096862793 | KNN Loss: 5.603030204772949 | BCE Loss: 1.0872905254364014\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 6.696052551269531 | KNN Loss: 5.679969787597656 | BCE Loss: 1.016083002090454\n",
      "Epoch   207: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 6.655805587768555 | KNN Loss: 5.601925373077393 | BCE Loss: 1.053879976272583\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 6.743081092834473 | KNN Loss: 5.647957801818848 | BCE Loss: 1.095123291015625\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 6.749734878540039 | KNN Loss: 5.698415279388428 | BCE Loss: 1.0513193607330322\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 6.654742240905762 | KNN Loss: 5.609558582305908 | BCE Loss: 1.045183777809143\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 6.670514106750488 | KNN Loss: 5.610133171081543 | BCE Loss: 1.0603811740875244\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 6.64599084854126 | KNN Loss: 5.6021294593811035 | BCE Loss: 1.0438613891601562\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 6.675926208496094 | KNN Loss: 5.650582313537598 | BCE Loss: 1.0253441333770752\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 6.767136096954346 | KNN Loss: 5.6838603019714355 | BCE Loss: 1.0832757949829102\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 6.6786298751831055 | KNN Loss: 5.62541389465332 | BCE Loss: 1.0532159805297852\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 6.6526408195495605 | KNN Loss: 5.607982158660889 | BCE Loss: 1.0446586608886719\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 6.627373695373535 | KNN Loss: 5.608281135559082 | BCE Loss: 1.0190926790237427\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 6.696999549865723 | KNN Loss: 5.651908874511719 | BCE Loss: 1.045090913772583\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 6.661304473876953 | KNN Loss: 5.614737510681152 | BCE Loss: 1.0465672016143799\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 6.66081428527832 | KNN Loss: 5.624953746795654 | BCE Loss: 1.035860300064087\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 6.749845027923584 | KNN Loss: 5.693462371826172 | BCE Loss: 1.0563825368881226\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 6.659698486328125 | KNN Loss: 5.5992021560668945 | BCE Loss: 1.0604965686798096\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 6.653630256652832 | KNN Loss: 5.622343063354492 | BCE Loss: 1.0312870740890503\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 6.630762100219727 | KNN Loss: 5.597158432006836 | BCE Loss: 1.0336034297943115\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 6.660390377044678 | KNN Loss: 5.618795871734619 | BCE Loss: 1.0415946245193481\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 6.645809173583984 | KNN Loss: 5.617274284362793 | BCE Loss: 1.0285351276397705\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 6.637202262878418 | KNN Loss: 5.60396146774292 | BCE Loss: 1.033240556716919\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 6.6738176345825195 | KNN Loss: 5.634294509887695 | BCE Loss: 1.0395231246948242\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 6.635298728942871 | KNN Loss: 5.595190048217773 | BCE Loss: 1.0401086807250977\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 6.7135396003723145 | KNN Loss: 5.673596382141113 | BCE Loss: 1.0399430990219116\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 6.65257453918457 | KNN Loss: 5.60332727432251 | BCE Loss: 1.049247145652771\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 6.70970344543457 | KNN Loss: 5.631592273712158 | BCE Loss: 1.078111171722412\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 6.6292829513549805 | KNN Loss: 5.598132610321045 | BCE Loss: 1.031150221824646\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 6.683733940124512 | KNN Loss: 5.620299339294434 | BCE Loss: 1.0634348392486572\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 6.680208206176758 | KNN Loss: 5.655982971191406 | BCE Loss: 1.0242253541946411\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 6.657756328582764 | KNN Loss: 5.634151935577393 | BCE Loss: 1.0236042737960815\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 6.663025379180908 | KNN Loss: 5.61936616897583 | BCE Loss: 1.0436592102050781\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 6.684292793273926 | KNN Loss: 5.608573913574219 | BCE Loss: 1.075718641281128\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 6.715825080871582 | KNN Loss: 5.664590358734131 | BCE Loss: 1.051234483718872\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 6.679967880249023 | KNN Loss: 5.651104927062988 | BCE Loss: 1.0288628339767456\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 6.673439979553223 | KNN Loss: 5.6525068283081055 | BCE Loss: 1.0209333896636963\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 6.699108600616455 | KNN Loss: 5.6471991539001465 | BCE Loss: 1.051909327507019\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 6.663216590881348 | KNN Loss: 5.615975856781006 | BCE Loss: 1.047240972518921\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 6.669373035430908 | KNN Loss: 5.6189656257629395 | BCE Loss: 1.0504074096679688\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 6.666977882385254 | KNN Loss: 5.6045050621032715 | BCE Loss: 1.0624725818634033\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 6.653375625610352 | KNN Loss: 5.604313373565674 | BCE Loss: 1.0490622520446777\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 6.647466659545898 | KNN Loss: 5.598549842834473 | BCE Loss: 1.0489165782928467\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 6.653532028198242 | KNN Loss: 5.605131149291992 | BCE Loss: 1.048401117324829\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 6.61215353012085 | KNN Loss: 5.6014628410339355 | BCE Loss: 1.010690689086914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 6.657923698425293 | KNN Loss: 5.608719348907471 | BCE Loss: 1.0492043495178223\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 6.640993118286133 | KNN Loss: 5.596783638000488 | BCE Loss: 1.0442092418670654\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 6.7549238204956055 | KNN Loss: 5.7053141593933105 | BCE Loss: 1.049609661102295\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 6.639946937561035 | KNN Loss: 5.597443580627441 | BCE Loss: 1.0425031185150146\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 6.697096824645996 | KNN Loss: 5.647896766662598 | BCE Loss: 1.0491999387741089\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 6.641568660736084 | KNN Loss: 5.60659122467041 | BCE Loss: 1.0349774360656738\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 6.70533561706543 | KNN Loss: 5.639509201049805 | BCE Loss: 1.065826416015625\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 6.696913719177246 | KNN Loss: 5.615418910980225 | BCE Loss: 1.081494688987732\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 6.664056777954102 | KNN Loss: 5.602831840515137 | BCE Loss: 1.061225175857544\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 6.646669387817383 | KNN Loss: 5.605761528015137 | BCE Loss: 1.040907621383667\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 6.6860456466674805 | KNN Loss: 5.644251823425293 | BCE Loss: 1.0417938232421875\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 6.678264617919922 | KNN Loss: 5.646169185638428 | BCE Loss: 1.0320956707000732\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 6.673174858093262 | KNN Loss: 5.6388397216796875 | BCE Loss: 1.0343351364135742\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 6.724078178405762 | KNN Loss: 5.648885250091553 | BCE Loss: 1.075192928314209\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 6.659944534301758 | KNN Loss: 5.645527362823486 | BCE Loss: 1.0144174098968506\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 6.722786903381348 | KNN Loss: 5.659868240356445 | BCE Loss: 1.0629185438156128\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 6.656352519989014 | KNN Loss: 5.595108509063721 | BCE Loss: 1.061244010925293\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 6.665558815002441 | KNN Loss: 5.6241326332092285 | BCE Loss: 1.0414259433746338\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 6.6707763671875 | KNN Loss: 5.605733871459961 | BCE Loss: 1.065042495727539\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 6.598997592926025 | KNN Loss: 5.592503070831299 | BCE Loss: 1.0064945220947266\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 6.675062656402588 | KNN Loss: 5.612529277801514 | BCE Loss: 1.0625333786010742\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 6.673397064208984 | KNN Loss: 5.603118419647217 | BCE Loss: 1.0702784061431885\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 6.682954788208008 | KNN Loss: 5.637024879455566 | BCE Loss: 1.0459299087524414\n",
      "Epoch   218: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 6.679990291595459 | KNN Loss: 5.613867282867432 | BCE Loss: 1.0661230087280273\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 6.669909477233887 | KNN Loss: 5.628855228424072 | BCE Loss: 1.041054368019104\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 6.693660259246826 | KNN Loss: 5.666180610656738 | BCE Loss: 1.0274795293807983\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 6.672795295715332 | KNN Loss: 5.613170623779297 | BCE Loss: 1.0596245527267456\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 6.688904285430908 | KNN Loss: 5.66699743270874 | BCE Loss: 1.0219069719314575\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 6.6136627197265625 | KNN Loss: 5.599026203155518 | BCE Loss: 1.0146363973617554\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 6.735272407531738 | KNN Loss: 5.663317680358887 | BCE Loss: 1.0719547271728516\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 6.638914585113525 | KNN Loss: 5.607785701751709 | BCE Loss: 1.0311288833618164\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 6.777859687805176 | KNN Loss: 5.720558166503906 | BCE Loss: 1.0573015213012695\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 6.6572418212890625 | KNN Loss: 5.600968360900879 | BCE Loss: 1.0562734603881836\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 6.615080833435059 | KNN Loss: 5.5930938720703125 | BCE Loss: 1.0219870805740356\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 6.649058818817139 | KNN Loss: 5.597274303436279 | BCE Loss: 1.051784634590149\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 6.6592206954956055 | KNN Loss: 5.607760906219482 | BCE Loss: 1.0514600276947021\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 6.644903182983398 | KNN Loss: 5.593454360961914 | BCE Loss: 1.0514485836029053\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 6.717059135437012 | KNN Loss: 5.627715110778809 | BCE Loss: 1.0893440246582031\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 6.680454254150391 | KNN Loss: 5.616101264953613 | BCE Loss: 1.0643532276153564\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 6.636350154876709 | KNN Loss: 5.601253032684326 | BCE Loss: 1.0350970029830933\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 6.6994524002075195 | KNN Loss: 5.651218414306641 | BCE Loss: 1.0482337474822998\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 6.629619598388672 | KNN Loss: 5.594241619110107 | BCE Loss: 1.035377860069275\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 6.618129730224609 | KNN Loss: 5.612866401672363 | BCE Loss: 1.005263328552246\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 6.67159366607666 | KNN Loss: 5.622042179107666 | BCE Loss: 1.0495517253875732\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 6.654201507568359 | KNN Loss: 5.597959518432617 | BCE Loss: 1.0562422275543213\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 6.646225929260254 | KNN Loss: 5.606113910675049 | BCE Loss: 1.040111780166626\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 6.651166915893555 | KNN Loss: 5.611112594604492 | BCE Loss: 1.040054202079773\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 6.643740177154541 | KNN Loss: 5.623833179473877 | BCE Loss: 1.019906997680664\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 6.636812210083008 | KNN Loss: 5.606158256530762 | BCE Loss: 1.030653953552246\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 6.647854804992676 | KNN Loss: 5.605716228485107 | BCE Loss: 1.0421388149261475\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 6.631807804107666 | KNN Loss: 5.602745056152344 | BCE Loss: 1.0290626287460327\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 6.718564033508301 | KNN Loss: 5.674365997314453 | BCE Loss: 1.0441980361938477\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 6.697424411773682 | KNN Loss: 5.630661487579346 | BCE Loss: 1.066762924194336\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 6.612491607666016 | KNN Loss: 5.593568325042725 | BCE Loss: 1.0189231634140015\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 6.7945556640625 | KNN Loss: 5.735652446746826 | BCE Loss: 1.058903455734253\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 6.686717987060547 | KNN Loss: 5.634938716888428 | BCE Loss: 1.05177903175354\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 6.701602458953857 | KNN Loss: 5.655519962310791 | BCE Loss: 1.0460824966430664\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 6.703769207000732 | KNN Loss: 5.64624547958374 | BCE Loss: 1.0575237274169922\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 6.660134792327881 | KNN Loss: 5.610263347625732 | BCE Loss: 1.0498714447021484\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 6.6522536277771 | KNN Loss: 5.5942816734313965 | BCE Loss: 1.0579719543457031\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 6.622801780700684 | KNN Loss: 5.594631195068359 | BCE Loss: 1.0281704664230347\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 6.670357704162598 | KNN Loss: 5.629129409790039 | BCE Loss: 1.0412285327911377\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 6.660871505737305 | KNN Loss: 5.6045637130737305 | BCE Loss: 1.0563077926635742\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 6.726458549499512 | KNN Loss: 5.676394939422607 | BCE Loss: 1.0500637292861938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 6.689937591552734 | KNN Loss: 5.631084442138672 | BCE Loss: 1.058853268623352\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 6.650698661804199 | KNN Loss: 5.60921049118042 | BCE Loss: 1.0414884090423584\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 6.738119125366211 | KNN Loss: 5.667929172515869 | BCE Loss: 1.070190191268921\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 6.695267677307129 | KNN Loss: 5.671316146850586 | BCE Loss: 1.0239512920379639\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 6.704167366027832 | KNN Loss: 5.607880115509033 | BCE Loss: 1.0962872505187988\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 6.656418800354004 | KNN Loss: 5.602319240570068 | BCE Loss: 1.0540997982025146\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 6.713015556335449 | KNN Loss: 5.676568508148193 | BCE Loss: 1.0364471673965454\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 6.6995391845703125 | KNN Loss: 5.6663994789123535 | BCE Loss: 1.033139944076538\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 6.638826370239258 | KNN Loss: 5.595310688018799 | BCE Loss: 1.043515682220459\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 6.668543815612793 | KNN Loss: 5.61946439743042 | BCE Loss: 1.049079179763794\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 6.628113746643066 | KNN Loss: 5.596036911010742 | BCE Loss: 1.0320768356323242\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 6.672134876251221 | KNN Loss: 5.616903781890869 | BCE Loss: 1.0552312135696411\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 6.624319553375244 | KNN Loss: 5.593439102172852 | BCE Loss: 1.0308804512023926\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 6.7482194900512695 | KNN Loss: 5.688952922821045 | BCE Loss: 1.0592668056488037\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 6.6678009033203125 | KNN Loss: 5.602880001068115 | BCE Loss: 1.0649209022521973\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 6.728254795074463 | KNN Loss: 5.688497543334961 | BCE Loss: 1.0397571325302124\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 6.62527322769165 | KNN Loss: 5.603172302246094 | BCE Loss: 1.0221009254455566\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 6.669851303100586 | KNN Loss: 5.602234840393066 | BCE Loss: 1.0676167011260986\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 6.760346412658691 | KNN Loss: 5.717006206512451 | BCE Loss: 1.0433399677276611\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 6.68745231628418 | KNN Loss: 5.639194488525391 | BCE Loss: 1.04825758934021\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 6.6551737785339355 | KNN Loss: 5.611153602600098 | BCE Loss: 1.0440202951431274\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 6.688227653503418 | KNN Loss: 5.629606246948242 | BCE Loss: 1.0586216449737549\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 6.6547722816467285 | KNN Loss: 5.605706214904785 | BCE Loss: 1.0490660667419434\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 6.633882522583008 | KNN Loss: 5.596172332763672 | BCE Loss: 1.037710428237915\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 6.637275218963623 | KNN Loss: 5.608722686767578 | BCE Loss: 1.028552532196045\n",
      "Epoch   229: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 6.731581211090088 | KNN Loss: 5.673763275146484 | BCE Loss: 1.057817816734314\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 6.68663215637207 | KNN Loss: 5.673675537109375 | BCE Loss: 1.0129566192626953\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 6.658757209777832 | KNN Loss: 5.600545883178711 | BCE Loss: 1.058211326599121\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 6.697882652282715 | KNN Loss: 5.6669020652771 | BCE Loss: 1.0309804677963257\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 6.652568340301514 | KNN Loss: 5.608484268188477 | BCE Loss: 1.044084072113037\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 6.676877021789551 | KNN Loss: 5.632871150970459 | BCE Loss: 1.044006109237671\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 6.75230598449707 | KNN Loss: 5.7211689949035645 | BCE Loss: 1.0311369895935059\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 6.706050872802734 | KNN Loss: 5.62896728515625 | BCE Loss: 1.0770833492279053\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 6.73610782623291 | KNN Loss: 5.67451286315918 | BCE Loss: 1.0615952014923096\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 6.651284217834473 | KNN Loss: 5.600679397583008 | BCE Loss: 1.0506045818328857\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 6.650909900665283 | KNN Loss: 5.605390548706055 | BCE Loss: 1.045519232749939\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 6.679967880249023 | KNN Loss: 5.656223297119141 | BCE Loss: 1.0237443447113037\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 6.6541290283203125 | KNN Loss: 5.614004611968994 | BCE Loss: 1.0401241779327393\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 6.6272382736206055 | KNN Loss: 5.5902323722839355 | BCE Loss: 1.0370056629180908\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 6.6486945152282715 | KNN Loss: 5.593399524688721 | BCE Loss: 1.0552951097488403\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 6.667003154754639 | KNN Loss: 5.600393772125244 | BCE Loss: 1.066609263420105\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 6.62436056137085 | KNN Loss: 5.594797134399414 | BCE Loss: 1.0295634269714355\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 6.6753435134887695 | KNN Loss: 5.592767238616943 | BCE Loss: 1.0825761556625366\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 6.733626365661621 | KNN Loss: 5.696592807769775 | BCE Loss: 1.0370337963104248\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 6.649813652038574 | KNN Loss: 5.600505828857422 | BCE Loss: 1.0493075847625732\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 6.663882255554199 | KNN Loss: 5.637446880340576 | BCE Loss: 1.0264356136322021\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 6.6902875900268555 | KNN Loss: 5.626368999481201 | BCE Loss: 1.0639185905456543\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 6.620182991027832 | KNN Loss: 5.60044002532959 | BCE Loss: 1.019742727279663\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 6.674657344818115 | KNN Loss: 5.632543087005615 | BCE Loss: 1.0421142578125\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 6.669346332550049 | KNN Loss: 5.604352951049805 | BCE Loss: 1.0649932622909546\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 6.646696090698242 | KNN Loss: 5.607316970825195 | BCE Loss: 1.039379358291626\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 6.714330673217773 | KNN Loss: 5.6891655921936035 | BCE Loss: 1.0251649618148804\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 6.766804218292236 | KNN Loss: 5.674027919769287 | BCE Loss: 1.0927762985229492\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 6.653879165649414 | KNN Loss: 5.640538692474365 | BCE Loss: 1.013340711593628\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 6.66657018661499 | KNN Loss: 5.6159539222717285 | BCE Loss: 1.0506162643432617\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 6.672205448150635 | KNN Loss: 5.625011444091797 | BCE Loss: 1.047194004058838\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 6.642928123474121 | KNN Loss: 5.596412181854248 | BCE Loss: 1.0465161800384521\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 6.627178192138672 | KNN Loss: 5.603271484375 | BCE Loss: 1.0239064693450928\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 6.719906806945801 | KNN Loss: 5.67628812789917 | BCE Loss: 1.04361891746521\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 6.742572784423828 | KNN Loss: 5.6977219581604 | BCE Loss: 1.0448507070541382\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 6.669445037841797 | KNN Loss: 5.660811424255371 | BCE Loss: 1.0086333751678467\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 6.696760177612305 | KNN Loss: 5.647082328796387 | BCE Loss: 1.049677848815918\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 6.689558982849121 | KNN Loss: 5.653206825256348 | BCE Loss: 1.0363521575927734\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 6.655139923095703 | KNN Loss: 5.6073222160339355 | BCE Loss: 1.0478179454803467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 6.648608684539795 | KNN Loss: 5.6197428703308105 | BCE Loss: 1.0288658142089844\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 6.810937881469727 | KNN Loss: 5.752829074859619 | BCE Loss: 1.058108925819397\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 6.680927276611328 | KNN Loss: 5.615092754364014 | BCE Loss: 1.0658347606658936\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 6.7010650634765625 | KNN Loss: 5.643383026123047 | BCE Loss: 1.0576822757720947\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 6.706835746765137 | KNN Loss: 5.63114070892334 | BCE Loss: 1.075695276260376\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 6.728301048278809 | KNN Loss: 5.6612653732299805 | BCE Loss: 1.067035436630249\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 6.652581214904785 | KNN Loss: 5.594162940979004 | BCE Loss: 1.0584180355072021\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 6.668451309204102 | KNN Loss: 5.625827789306641 | BCE Loss: 1.0426232814788818\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 6.683125972747803 | KNN Loss: 5.6562347412109375 | BCE Loss: 1.0268912315368652\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 6.675145626068115 | KNN Loss: 5.633177757263184 | BCE Loss: 1.0419678688049316\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 6.638258934020996 | KNN Loss: 5.610328197479248 | BCE Loss: 1.0279309749603271\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 6.631598472595215 | KNN Loss: 5.603954315185547 | BCE Loss: 1.027644395828247\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 6.6335248947143555 | KNN Loss: 5.592466354370117 | BCE Loss: 1.0410587787628174\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 6.671510696411133 | KNN Loss: 5.64938497543335 | BCE Loss: 1.0221256017684937\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 6.66572904586792 | KNN Loss: 5.624335765838623 | BCE Loss: 1.0413931608200073\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 6.646817684173584 | KNN Loss: 5.6120758056640625 | BCE Loss: 1.0347418785095215\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 6.693004608154297 | KNN Loss: 5.6256279945373535 | BCE Loss: 1.067376732826233\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 6.719809532165527 | KNN Loss: 5.686691761016846 | BCE Loss: 1.0331180095672607\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 6.668574810028076 | KNN Loss: 5.638427257537842 | BCE Loss: 1.0301475524902344\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 6.636234760284424 | KNN Loss: 5.619819641113281 | BCE Loss: 1.0164152383804321\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 6.627688407897949 | KNN Loss: 5.595023155212402 | BCE Loss: 1.0326652526855469\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 6.7084856033325195 | KNN Loss: 5.656015396118164 | BCE Loss: 1.052470326423645\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 6.630392074584961 | KNN Loss: 5.5987772941589355 | BCE Loss: 1.0316147804260254\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 6.80454158782959 | KNN Loss: 5.7458577156066895 | BCE Loss: 1.0586838722229004\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 6.670601844787598 | KNN Loss: 5.597158432006836 | BCE Loss: 1.0734436511993408\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 6.631338119506836 | KNN Loss: 5.597931385040283 | BCE Loss: 1.0334069728851318\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 6.695876121520996 | KNN Loss: 5.669463157653809 | BCE Loss: 1.026413083076477\n",
      "Epoch   240: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 6.690968990325928 | KNN Loss: 5.6188764572143555 | BCE Loss: 1.0720925331115723\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 6.593987941741943 | KNN Loss: 5.6006927490234375 | BCE Loss: 0.9932952523231506\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 6.651167392730713 | KNN Loss: 5.624917507171631 | BCE Loss: 1.0262500047683716\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 6.634110927581787 | KNN Loss: 5.598532676696777 | BCE Loss: 1.0355782508850098\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 6.664424896240234 | KNN Loss: 5.602520942687988 | BCE Loss: 1.061903715133667\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 6.810947418212891 | KNN Loss: 5.762152671813965 | BCE Loss: 1.0487945079803467\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 6.636282920837402 | KNN Loss: 5.605944633483887 | BCE Loss: 1.0303385257720947\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 6.680607318878174 | KNN Loss: 5.629405498504639 | BCE Loss: 1.0512017011642456\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 6.70272159576416 | KNN Loss: 5.642755031585693 | BCE Loss: 1.059966802597046\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 6.655605316162109 | KNN Loss: 5.600455284118652 | BCE Loss: 1.055150032043457\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 6.681946277618408 | KNN Loss: 5.617529392242432 | BCE Loss: 1.0644168853759766\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 6.668196678161621 | KNN Loss: 5.619210243225098 | BCE Loss: 1.0489864349365234\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 6.690075874328613 | KNN Loss: 5.648064613342285 | BCE Loss: 1.0420113801956177\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 6.631802558898926 | KNN Loss: 5.5916900634765625 | BCE Loss: 1.0401124954223633\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 6.708483695983887 | KNN Loss: 5.6573381423950195 | BCE Loss: 1.0511455535888672\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 6.652327060699463 | KNN Loss: 5.626887321472168 | BCE Loss: 1.0254396200180054\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 6.6534833908081055 | KNN Loss: 5.62349271774292 | BCE Loss: 1.0299906730651855\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 6.665668487548828 | KNN Loss: 5.633999824523926 | BCE Loss: 1.0316689014434814\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 6.6430487632751465 | KNN Loss: 5.618524074554443 | BCE Loss: 1.0245246887207031\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 6.680401802062988 | KNN Loss: 5.627764701843262 | BCE Loss: 1.0526368618011475\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 6.646103382110596 | KNN Loss: 5.619841575622559 | BCE Loss: 1.0262616872787476\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 6.688037395477295 | KNN Loss: 5.659697532653809 | BCE Loss: 1.0283398628234863\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 6.69219970703125 | KNN Loss: 5.650589942932129 | BCE Loss: 1.041609525680542\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 6.666811466217041 | KNN Loss: 5.598999500274658 | BCE Loss: 1.0678120851516724\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 6.668567180633545 | KNN Loss: 5.613483905792236 | BCE Loss: 1.0550832748413086\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 6.650874614715576 | KNN Loss: 5.6018781661987305 | BCE Loss: 1.0489964485168457\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 6.640710830688477 | KNN Loss: 5.612702369689941 | BCE Loss: 1.0280083417892456\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 6.7581329345703125 | KNN Loss: 5.679430961608887 | BCE Loss: 1.0787017345428467\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 6.707757949829102 | KNN Loss: 5.695581912994385 | BCE Loss: 1.0121760368347168\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 6.645481109619141 | KNN Loss: 5.614712238311768 | BCE Loss: 1.0307691097259521\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 6.679892063140869 | KNN Loss: 5.6151299476623535 | BCE Loss: 1.0647621154785156\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 6.657192707061768 | KNN Loss: 5.608592510223389 | BCE Loss: 1.0486000776290894\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 6.637603759765625 | KNN Loss: 5.6103010177612305 | BCE Loss: 1.0273027420043945\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 6.61820125579834 | KNN Loss: 5.605618953704834 | BCE Loss: 1.0125823020935059\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 6.6661810874938965 | KNN Loss: 5.615012168884277 | BCE Loss: 1.0511690378189087\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 6.698927879333496 | KNN Loss: 5.650798320770264 | BCE Loss: 1.0481293201446533\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 6.640589237213135 | KNN Loss: 5.623787879943848 | BCE Loss: 1.016801357269287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 6.6854400634765625 | KNN Loss: 5.608436584472656 | BCE Loss: 1.0770034790039062\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 6.6690473556518555 | KNN Loss: 5.621182918548584 | BCE Loss: 1.0478641986846924\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 6.645016193389893 | KNN Loss: 5.594892978668213 | BCE Loss: 1.0501232147216797\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 6.6126909255981445 | KNN Loss: 5.5986328125 | BCE Loss: 1.0140578746795654\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 6.687328815460205 | KNN Loss: 5.64163875579834 | BCE Loss: 1.0456899404525757\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 6.720671653747559 | KNN Loss: 5.638343334197998 | BCE Loss: 1.0823283195495605\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 6.695827484130859 | KNN Loss: 5.638443946838379 | BCE Loss: 1.0573837757110596\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 6.623012065887451 | KNN Loss: 5.617652416229248 | BCE Loss: 1.0053595304489136\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 6.713750839233398 | KNN Loss: 5.645522594451904 | BCE Loss: 1.0682281255722046\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 6.744811058044434 | KNN Loss: 5.686931610107422 | BCE Loss: 1.0578793287277222\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 6.66312837600708 | KNN Loss: 5.622662544250488 | BCE Loss: 1.0404658317565918\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 6.719195365905762 | KNN Loss: 5.678114414215088 | BCE Loss: 1.041081190109253\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 6.66249418258667 | KNN Loss: 5.59780740737915 | BCE Loss: 1.0646867752075195\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 6.695809841156006 | KNN Loss: 5.662657737731934 | BCE Loss: 1.0331521034240723\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 6.633457183837891 | KNN Loss: 5.594033718109131 | BCE Loss: 1.0394237041473389\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 6.663132667541504 | KNN Loss: 5.629082202911377 | BCE Loss: 1.034050703048706\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 6.729913234710693 | KNN Loss: 5.672711372375488 | BCE Loss: 1.0572017431259155\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 6.615937232971191 | KNN Loss: 5.592504501342773 | BCE Loss: 1.023432970046997\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 6.700758457183838 | KNN Loss: 5.646523952484131 | BCE Loss: 1.054234504699707\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 6.620596885681152 | KNN Loss: 5.607031345367432 | BCE Loss: 1.0135655403137207\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 6.630306720733643 | KNN Loss: 5.605889320373535 | BCE Loss: 1.0244172811508179\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 6.701971530914307 | KNN Loss: 5.647493362426758 | BCE Loss: 1.0544781684875488\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 6.65468692779541 | KNN Loss: 5.610335350036621 | BCE Loss: 1.044351577758789\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 6.635720252990723 | KNN Loss: 5.59789514541626 | BCE Loss: 1.0378249883651733\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 6.664528846740723 | KNN Loss: 5.614987373352051 | BCE Loss: 1.0495414733886719\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 6.682094097137451 | KNN Loss: 5.650196075439453 | BCE Loss: 1.0318981409072876\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 6.706409454345703 | KNN Loss: 5.649122714996338 | BCE Loss: 1.0572865009307861\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 6.67197322845459 | KNN Loss: 5.622450351715088 | BCE Loss: 1.0495227575302124\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 6.6625518798828125 | KNN Loss: 5.618607997894287 | BCE Loss: 1.0439436435699463\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 6.693544387817383 | KNN Loss: 5.633003234863281 | BCE Loss: 1.0605409145355225\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 6.689523696899414 | KNN Loss: 5.604507923126221 | BCE Loss: 1.0850157737731934\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 6.644718170166016 | KNN Loss: 5.606034278869629 | BCE Loss: 1.0386836528778076\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 6.8095903396606445 | KNN Loss: 5.765585422515869 | BCE Loss: 1.0440051555633545\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 6.649095058441162 | KNN Loss: 5.603984355926514 | BCE Loss: 1.0451107025146484\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 6.632334232330322 | KNN Loss: 5.597965717315674 | BCE Loss: 1.0343683958053589\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 6.630091667175293 | KNN Loss: 5.599060535430908 | BCE Loss: 1.0310310125350952\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 6.639429092407227 | KNN Loss: 5.6028032302856445 | BCE Loss: 1.0366261005401611\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 6.662993907928467 | KNN Loss: 5.61854362487793 | BCE Loss: 1.044450283050537\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 6.6804656982421875 | KNN Loss: 5.636949062347412 | BCE Loss: 1.0435166358947754\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 6.647439479827881 | KNN Loss: 5.608248710632324 | BCE Loss: 1.0391907691955566\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 6.6656646728515625 | KNN Loss: 5.6195292472839355 | BCE Loss: 1.046135663986206\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 6.684755325317383 | KNN Loss: 5.5982890129089355 | BCE Loss: 1.0864663124084473\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 6.623778343200684 | KNN Loss: 5.606995582580566 | BCE Loss: 1.0167826414108276\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 6.654900550842285 | KNN Loss: 5.6321821212768555 | BCE Loss: 1.0227181911468506\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 6.655038356781006 | KNN Loss: 5.641482353210449 | BCE Loss: 1.0135560035705566\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 6.6372880935668945 | KNN Loss: 5.615028381347656 | BCE Loss: 1.0222597122192383\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 6.69003963470459 | KNN Loss: 5.640996932983398 | BCE Loss: 1.0490424633026123\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 6.646695137023926 | KNN Loss: 5.60866641998291 | BCE Loss: 1.0380288362503052\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 6.64055871963501 | KNN Loss: 5.599629878997803 | BCE Loss: 1.040928840637207\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 6.695959091186523 | KNN Loss: 5.667751789093018 | BCE Loss: 1.0282071828842163\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 6.629598140716553 | KNN Loss: 5.598739147186279 | BCE Loss: 1.0308589935302734\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 6.644733905792236 | KNN Loss: 5.62759256362915 | BCE Loss: 1.017141342163086\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 6.794927597045898 | KNN Loss: 5.755046844482422 | BCE Loss: 1.0398805141448975\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 6.744967937469482 | KNN Loss: 5.6951422691345215 | BCE Loss: 1.049825668334961\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 6.686729431152344 | KNN Loss: 5.627195835113525 | BCE Loss: 1.0595338344573975\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 6.627438545227051 | KNN Loss: 5.618345737457275 | BCE Loss: 1.009092926979065\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 6.697290897369385 | KNN Loss: 5.623371601104736 | BCE Loss: 1.0739192962646484\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 6.660974025726318 | KNN Loss: 5.597121238708496 | BCE Loss: 1.0638526678085327\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 6.6852216720581055 | KNN Loss: 5.621715068817139 | BCE Loss: 1.0635063648223877\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 6.617536544799805 | KNN Loss: 5.592224597930908 | BCE Loss: 1.025311827659607\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 6.649816513061523 | KNN Loss: 5.625502109527588 | BCE Loss: 1.0243146419525146\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 6.7118821144104 | KNN Loss: 5.647553443908691 | BCE Loss: 1.064328670501709\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 6.700707912445068 | KNN Loss: 5.634781360626221 | BCE Loss: 1.0659265518188477\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 6.683131217956543 | KNN Loss: 5.625584602355957 | BCE Loss: 1.0575464963912964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 6.657567977905273 | KNN Loss: 5.606690406799316 | BCE Loss: 1.0508778095245361\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 6.669547080993652 | KNN Loss: 5.639656066894531 | BCE Loss: 1.029890775680542\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 6.631656646728516 | KNN Loss: 5.597186088562012 | BCE Loss: 1.0344703197479248\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 6.69064474105835 | KNN Loss: 5.653416633605957 | BCE Loss: 1.0372281074523926\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 6.7557268142700195 | KNN Loss: 5.697240829467773 | BCE Loss: 1.058485746383667\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 6.673306941986084 | KNN Loss: 5.616492748260498 | BCE Loss: 1.056814193725586\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 6.661456108093262 | KNN Loss: 5.616448402404785 | BCE Loss: 1.0450079441070557\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 6.6306352615356445 | KNN Loss: 5.603365421295166 | BCE Loss: 1.0272700786590576\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 6.753962993621826 | KNN Loss: 5.718681335449219 | BCE Loss: 1.0352815389633179\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 6.627508640289307 | KNN Loss: 5.603788375854492 | BCE Loss: 1.023720383644104\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 6.792522430419922 | KNN Loss: 5.737142562866211 | BCE Loss: 1.0553799867630005\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 6.638225078582764 | KNN Loss: 5.62701940536499 | BCE Loss: 1.0112056732177734\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 6.6640472412109375 | KNN Loss: 5.638609409332275 | BCE Loss: 1.0254380702972412\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 6.666865825653076 | KNN Loss: 5.612332344055176 | BCE Loss: 1.0545333623886108\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 6.7086501121521 | KNN Loss: 5.668224811553955 | BCE Loss: 1.040425181388855\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 6.678752422332764 | KNN Loss: 5.6045379638671875 | BCE Loss: 1.0742144584655762\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 6.666057109832764 | KNN Loss: 5.632498264312744 | BCE Loss: 1.0335588455200195\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 6.770090103149414 | KNN Loss: 5.725447654724121 | BCE Loss: 1.044642686843872\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 6.78934383392334 | KNN Loss: 5.758424758911133 | BCE Loss: 1.030919075012207\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 6.634736061096191 | KNN Loss: 5.600141525268555 | BCE Loss: 1.0345947742462158\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 6.625847816467285 | KNN Loss: 5.608513355255127 | BCE Loss: 1.0173345804214478\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 6.666741371154785 | KNN Loss: 5.639712333679199 | BCE Loss: 1.0270287990570068\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 6.687414169311523 | KNN Loss: 5.6612935066223145 | BCE Loss: 1.026120901107788\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 6.68904972076416 | KNN Loss: 5.63478946685791 | BCE Loss: 1.0542601346969604\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 6.652210712432861 | KNN Loss: 5.598514080047607 | BCE Loss: 1.053696632385254\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 6.701420783996582 | KNN Loss: 5.645834922790527 | BCE Loss: 1.0555857419967651\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 6.71860933303833 | KNN Loss: 5.66707181930542 | BCE Loss: 1.0515373945236206\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 6.637667655944824 | KNN Loss: 5.606688976287842 | BCE Loss: 1.0309785604476929\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 6.629610538482666 | KNN Loss: 5.604106903076172 | BCE Loss: 1.0255036354064941\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 6.644538879394531 | KNN Loss: 5.615586280822754 | BCE Loss: 1.028952717781067\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 6.7406005859375 | KNN Loss: 5.699384689331055 | BCE Loss: 1.0412158966064453\n",
      "Epoch   262: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 6.701383113861084 | KNN Loss: 5.6723127365112305 | BCE Loss: 1.029070258140564\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 6.691895484924316 | KNN Loss: 5.6102294921875 | BCE Loss: 1.0816657543182373\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 6.736742973327637 | KNN Loss: 5.704265117645264 | BCE Loss: 1.032477617263794\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 6.715538501739502 | KNN Loss: 5.663748264312744 | BCE Loss: 1.0517901182174683\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 6.689915657043457 | KNN Loss: 5.622220516204834 | BCE Loss: 1.0676952600479126\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 6.632468223571777 | KNN Loss: 5.592199325561523 | BCE Loss: 1.0402686595916748\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 6.689733505249023 | KNN Loss: 5.626429080963135 | BCE Loss: 1.0633045434951782\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 6.694840431213379 | KNN Loss: 5.6466240882873535 | BCE Loss: 1.0482165813446045\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 6.666393280029297 | KNN Loss: 5.640830993652344 | BCE Loss: 1.025562047958374\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 6.624070167541504 | KNN Loss: 5.593873023986816 | BCE Loss: 1.0301973819732666\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 6.652140140533447 | KNN Loss: 5.596888542175293 | BCE Loss: 1.0552515983581543\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 6.636812210083008 | KNN Loss: 5.623511791229248 | BCE Loss: 1.0133004188537598\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 6.680261611938477 | KNN Loss: 5.6271820068359375 | BCE Loss: 1.05307936668396\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 6.728124618530273 | KNN Loss: 5.674524784088135 | BCE Loss: 1.0535998344421387\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 6.779930114746094 | KNN Loss: 5.696020603179932 | BCE Loss: 1.083909273147583\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 6.68247652053833 | KNN Loss: 5.618618965148926 | BCE Loss: 1.0638574361801147\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 6.668018341064453 | KNN Loss: 5.6132731437683105 | BCE Loss: 1.054745078086853\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 6.6335296630859375 | KNN Loss: 5.607457637786865 | BCE Loss: 1.0260722637176514\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 6.644898891448975 | KNN Loss: 5.609199047088623 | BCE Loss: 1.035699725151062\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 6.648191928863525 | KNN Loss: 5.622550964355469 | BCE Loss: 1.025640845298767\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 6.6886887550354 | KNN Loss: 5.63840389251709 | BCE Loss: 1.0502849817276\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 6.676889419555664 | KNN Loss: 5.609042644500732 | BCE Loss: 1.0678465366363525\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 6.6903581619262695 | KNN Loss: 5.621737957000732 | BCE Loss: 1.068619966506958\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 6.630147457122803 | KNN Loss: 5.5970025062561035 | BCE Loss: 1.0331448316574097\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 6.652478218078613 | KNN Loss: 5.619938373565674 | BCE Loss: 1.0325398445129395\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 6.650148391723633 | KNN Loss: 5.621345520019531 | BCE Loss: 1.0288026332855225\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 6.702835559844971 | KNN Loss: 5.5992231369018555 | BCE Loss: 1.1036123037338257\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 6.705035209655762 | KNN Loss: 5.636763572692871 | BCE Loss: 1.0682716369628906\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 6.69119119644165 | KNN Loss: 5.634934902191162 | BCE Loss: 1.0562564134597778\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 6.640545845031738 | KNN Loss: 5.600957870483398 | BCE Loss: 1.0395878553390503\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 6.631444931030273 | KNN Loss: 5.597171306610107 | BCE Loss: 1.0342738628387451\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 6.6454973220825195 | KNN Loss: 5.606504917144775 | BCE Loss: 1.038992166519165\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 6.688220500946045 | KNN Loss: 5.61327600479126 | BCE Loss: 1.0749444961547852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 6.6496477127075195 | KNN Loss: 5.595839977264404 | BCE Loss: 1.0538079738616943\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 6.645152568817139 | KNN Loss: 5.618465423583984 | BCE Loss: 1.0266871452331543\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 6.624072074890137 | KNN Loss: 5.59575080871582 | BCE Loss: 1.0283215045928955\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 6.6714959144592285 | KNN Loss: 5.6270751953125 | BCE Loss: 1.044420838356018\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 6.670822620391846 | KNN Loss: 5.594536304473877 | BCE Loss: 1.0762863159179688\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 6.654727935791016 | KNN Loss: 5.601110935211182 | BCE Loss: 1.053617000579834\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 6.651573181152344 | KNN Loss: 5.612483978271484 | BCE Loss: 1.0390890836715698\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 6.658444404602051 | KNN Loss: 5.6038665771484375 | BCE Loss: 1.0545778274536133\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 6.741340160369873 | KNN Loss: 5.722785472869873 | BCE Loss: 1.0185546875\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 6.660134792327881 | KNN Loss: 5.647643089294434 | BCE Loss: 1.0124917030334473\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 6.6466779708862305 | KNN Loss: 5.600026607513428 | BCE Loss: 1.0466511249542236\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 6.700139045715332 | KNN Loss: 5.6434006690979 | BCE Loss: 1.0567381381988525\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 6.646606922149658 | KNN Loss: 5.595165252685547 | BCE Loss: 1.0514416694641113\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 6.725152492523193 | KNN Loss: 5.664382457733154 | BCE Loss: 1.0607699155807495\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 6.656040191650391 | KNN Loss: 5.598310947418213 | BCE Loss: 1.0577292442321777\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 6.647129058837891 | KNN Loss: 5.600149154663086 | BCE Loss: 1.0469800233840942\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 6.688935279846191 | KNN Loss: 5.634869575500488 | BCE Loss: 1.0540658235549927\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 6.666886329650879 | KNN Loss: 5.62319803237915 | BCE Loss: 1.043688178062439\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 6.717957496643066 | KNN Loss: 5.6562323570251465 | BCE Loss: 1.0617252588272095\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 6.684628486633301 | KNN Loss: 5.616662979125977 | BCE Loss: 1.0679655075073242\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 6.648300647735596 | KNN Loss: 5.599508285522461 | BCE Loss: 1.0487922430038452\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 6.745678901672363 | KNN Loss: 5.6811089515686035 | BCE Loss: 1.0645701885223389\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 6.759778022766113 | KNN Loss: 5.723784446716309 | BCE Loss: 1.0359933376312256\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 6.74184513092041 | KNN Loss: 5.6854248046875 | BCE Loss: 1.0564204454421997\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 6.627366542816162 | KNN Loss: 5.596535682678223 | BCE Loss: 1.030830979347229\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 6.640064239501953 | KNN Loss: 5.604678630828857 | BCE Loss: 1.0353857278823853\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 6.679779052734375 | KNN Loss: 5.635447025299072 | BCE Loss: 1.0443320274353027\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 6.713007926940918 | KNN Loss: 5.650049686431885 | BCE Loss: 1.0629582405090332\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 6.614895343780518 | KNN Loss: 5.590814113616943 | BCE Loss: 1.0240812301635742\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 6.685105323791504 | KNN Loss: 5.636781215667725 | BCE Loss: 1.0483242273330688\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 6.710997581481934 | KNN Loss: 5.621532917022705 | BCE Loss: 1.0894646644592285\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 6.617033004760742 | KNN Loss: 5.595032691955566 | BCE Loss: 1.0220005512237549\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 6.652298450469971 | KNN Loss: 5.639749526977539 | BCE Loss: 1.0125489234924316\n",
      "Epoch   273: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 6.656111717224121 | KNN Loss: 5.60468053817749 | BCE Loss: 1.0514311790466309\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 6.630954742431641 | KNN Loss: 5.601752758026123 | BCE Loss: 1.0292017459869385\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 6.633505821228027 | KNN Loss: 5.600060939788818 | BCE Loss: 1.033444881439209\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 6.717069625854492 | KNN Loss: 5.656759262084961 | BCE Loss: 1.0603103637695312\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 6.702602863311768 | KNN Loss: 5.617008686065674 | BCE Loss: 1.0855941772460938\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 6.630983352661133 | KNN Loss: 5.59614896774292 | BCE Loss: 1.034834384918213\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 6.689268112182617 | KNN Loss: 5.647134304046631 | BCE Loss: 1.0421340465545654\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 6.643956184387207 | KNN Loss: 5.597567081451416 | BCE Loss: 1.046388864517212\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 6.639585018157959 | KNN Loss: 5.6001152992248535 | BCE Loss: 1.039469599723816\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 6.662691116333008 | KNN Loss: 5.61059045791626 | BCE Loss: 1.0521008968353271\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 6.6680803298950195 | KNN Loss: 5.616393089294434 | BCE Loss: 1.051687479019165\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 6.724311828613281 | KNN Loss: 5.64950704574585 | BCE Loss: 1.0748049020767212\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 6.635366439819336 | KNN Loss: 5.594002723693848 | BCE Loss: 1.0413634777069092\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 6.629833698272705 | KNN Loss: 5.614473342895508 | BCE Loss: 1.0153602361679077\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 6.6147332191467285 | KNN Loss: 5.596437931060791 | BCE Loss: 1.018295168876648\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 6.743319034576416 | KNN Loss: 5.6273064613342285 | BCE Loss: 1.116012692451477\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 6.704830169677734 | KNN Loss: 5.637109756469727 | BCE Loss: 1.0677205324172974\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 6.663402557373047 | KNN Loss: 5.6146240234375 | BCE Loss: 1.0487785339355469\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 6.67681884765625 | KNN Loss: 5.63035249710083 | BCE Loss: 1.0464664697647095\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 6.697104454040527 | KNN Loss: 5.641793727874756 | BCE Loss: 1.055310845375061\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 6.631202220916748 | KNN Loss: 5.5915703773498535 | BCE Loss: 1.0396318435668945\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 6.69541597366333 | KNN Loss: 5.644680023193359 | BCE Loss: 1.0507358312606812\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 6.6040263175964355 | KNN Loss: 5.598249912261963 | BCE Loss: 1.0057765245437622\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 6.756875991821289 | KNN Loss: 5.67555570602417 | BCE Loss: 1.0813202857971191\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 6.645720958709717 | KNN Loss: 5.594430923461914 | BCE Loss: 1.0512899160385132\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 6.630348205566406 | KNN Loss: 5.617910861968994 | BCE Loss: 1.012437105178833\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 6.696487903594971 | KNN Loss: 5.664363384246826 | BCE Loss: 1.032124638557434\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 6.717777252197266 | KNN Loss: 5.677002906799316 | BCE Loss: 1.0407743453979492\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 6.723523139953613 | KNN Loss: 5.679503440856934 | BCE Loss: 1.0440194606781006\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 6.696062088012695 | KNN Loss: 5.638304233551025 | BCE Loss: 1.0577579736709595\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 6.6929731369018555 | KNN Loss: 5.619232177734375 | BCE Loss: 1.0737407207489014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 6.64456033706665 | KNN Loss: 5.612809181213379 | BCE Loss: 1.031751036643982\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 6.633317947387695 | KNN Loss: 5.6027679443359375 | BCE Loss: 1.030550241470337\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 6.683040618896484 | KNN Loss: 5.617974758148193 | BCE Loss: 1.0650660991668701\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 6.65429162979126 | KNN Loss: 5.619722843170166 | BCE Loss: 1.0345687866210938\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 6.691657543182373 | KNN Loss: 5.634257793426514 | BCE Loss: 1.0573997497558594\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 6.649815559387207 | KNN Loss: 5.618764400482178 | BCE Loss: 1.0310509204864502\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 6.642840385437012 | KNN Loss: 5.615389347076416 | BCE Loss: 1.0274512767791748\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 6.696717262268066 | KNN Loss: 5.637483596801758 | BCE Loss: 1.0592339038848877\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 6.652623653411865 | KNN Loss: 5.619147777557373 | BCE Loss: 1.0334758758544922\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 6.613446235656738 | KNN Loss: 5.59398889541626 | BCE Loss: 1.0194575786590576\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 6.711445331573486 | KNN Loss: 5.651191234588623 | BCE Loss: 1.0602539777755737\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 6.670305252075195 | KNN Loss: 5.633164405822754 | BCE Loss: 1.037140965461731\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 6.660277366638184 | KNN Loss: 5.5977783203125 | BCE Loss: 1.0624988079071045\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 6.643979072570801 | KNN Loss: 5.597238540649414 | BCE Loss: 1.0467407703399658\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 6.696634292602539 | KNN Loss: 5.634997844696045 | BCE Loss: 1.0616365671157837\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 6.647927761077881 | KNN Loss: 5.616965293884277 | BCE Loss: 1.0309624671936035\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 6.661383628845215 | KNN Loss: 5.59784460067749 | BCE Loss: 1.0635387897491455\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 6.6727705001831055 | KNN Loss: 5.643793106079102 | BCE Loss: 1.0289771556854248\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 6.687292098999023 | KNN Loss: 5.651472568511963 | BCE Loss: 1.0358192920684814\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 6.737852573394775 | KNN Loss: 5.67693567276001 | BCE Loss: 1.0609169006347656\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 6.645553112030029 | KNN Loss: 5.615303039550781 | BCE Loss: 1.0302499532699585\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 6.656378269195557 | KNN Loss: 5.606266498565674 | BCE Loss: 1.0501118898391724\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 6.683150291442871 | KNN Loss: 5.640565395355225 | BCE Loss: 1.0425848960876465\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 6.630316257476807 | KNN Loss: 5.6052327156066895 | BCE Loss: 1.0250835418701172\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 6.626980781555176 | KNN Loss: 5.613366603851318 | BCE Loss: 1.0136139392852783\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 6.708861351013184 | KNN Loss: 5.657443046569824 | BCE Loss: 1.0514185428619385\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 6.692576885223389 | KNN Loss: 5.621941089630127 | BCE Loss: 1.0706356763839722\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 6.640106678009033 | KNN Loss: 5.606586456298828 | BCE Loss: 1.033520221710205\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 6.678884983062744 | KNN Loss: 5.59923791885376 | BCE Loss: 1.0796470642089844\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 6.647599697113037 | KNN Loss: 5.6027045249938965 | BCE Loss: 1.0448951721191406\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 6.704768657684326 | KNN Loss: 5.646389484405518 | BCE Loss: 1.0583791732788086\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 6.664871692657471 | KNN Loss: 5.608283519744873 | BCE Loss: 1.056588053703308\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 6.677646160125732 | KNN Loss: 5.643793106079102 | BCE Loss: 1.0338529348373413\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 6.633236885070801 | KNN Loss: 5.592630386352539 | BCE Loss: 1.0406064987182617\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 6.716722011566162 | KNN Loss: 5.646862506866455 | BCE Loss: 1.0698593854904175\n",
      "Epoch   284: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 6.723271369934082 | KNN Loss: 5.706519603729248 | BCE Loss: 1.0167515277862549\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 6.667374610900879 | KNN Loss: 5.618577003479004 | BCE Loss: 1.048797607421875\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 6.6942596435546875 | KNN Loss: 5.631292343139648 | BCE Loss: 1.0629671812057495\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 6.696389198303223 | KNN Loss: 5.613913059234619 | BCE Loss: 1.082476258277893\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 6.703503608703613 | KNN Loss: 5.635744571685791 | BCE Loss: 1.0677592754364014\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 6.64734411239624 | KNN Loss: 5.60164213180542 | BCE Loss: 1.0457018613815308\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 6.644097328186035 | KNN Loss: 5.599637985229492 | BCE Loss: 1.044459342956543\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 6.645560264587402 | KNN Loss: 5.599501609802246 | BCE Loss: 1.0460586547851562\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 6.718021392822266 | KNN Loss: 5.683667182922363 | BCE Loss: 1.0343539714813232\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 6.661857604980469 | KNN Loss: 5.607655048370361 | BCE Loss: 1.0542025566101074\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 6.751815319061279 | KNN Loss: 5.706995010375977 | BCE Loss: 1.0448204278945923\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 6.62609338760376 | KNN Loss: 5.600623607635498 | BCE Loss: 1.0254697799682617\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 6.642937660217285 | KNN Loss: 5.598923683166504 | BCE Loss: 1.0440139770507812\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 6.667691707611084 | KNN Loss: 5.62194299697876 | BCE Loss: 1.0457487106323242\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 6.652976036071777 | KNN Loss: 5.594592094421387 | BCE Loss: 1.0583837032318115\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 6.654499053955078 | KNN Loss: 5.597769260406494 | BCE Loss: 1.0567295551300049\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 6.654444694519043 | KNN Loss: 5.639857292175293 | BCE Loss: 1.0145875215530396\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 6.746898651123047 | KNN Loss: 5.681978702545166 | BCE Loss: 1.0649198293685913\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 6.656264305114746 | KNN Loss: 5.6147236824035645 | BCE Loss: 1.0415408611297607\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 6.697917461395264 | KNN Loss: 5.651322364807129 | BCE Loss: 1.0465950965881348\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 6.709785461425781 | KNN Loss: 5.677901268005371 | BCE Loss: 1.031883955001831\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 6.665358543395996 | KNN Loss: 5.607960224151611 | BCE Loss: 1.0573983192443848\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 6.670161247253418 | KNN Loss: 5.609134674072266 | BCE Loss: 1.061026692390442\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 6.650778770446777 | KNN Loss: 5.61860990524292 | BCE Loss: 1.032168984413147\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 6.634680271148682 | KNN Loss: 5.602047443389893 | BCE Loss: 1.032632827758789\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 6.664983749389648 | KNN Loss: 5.59650182723999 | BCE Loss: 1.0684819221496582\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 6.700634956359863 | KNN Loss: 5.644558429718018 | BCE Loss: 1.0560765266418457\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 6.664041519165039 | KNN Loss: 5.60338830947876 | BCE Loss: 1.0606534481048584\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 6.680379867553711 | KNN Loss: 5.639594078063965 | BCE Loss: 1.0407860279083252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 6.771689414978027 | KNN Loss: 5.730332374572754 | BCE Loss: 1.0413570404052734\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 6.709698677062988 | KNN Loss: 5.6244940757751465 | BCE Loss: 1.0852043628692627\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 6.7591233253479 | KNN Loss: 5.699336528778076 | BCE Loss: 1.0597867965698242\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 6.687190055847168 | KNN Loss: 5.620724678039551 | BCE Loss: 1.0664656162261963\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 6.637509346008301 | KNN Loss: 5.600642204284668 | BCE Loss: 1.036867380142212\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 6.731346607208252 | KNN Loss: 5.693033218383789 | BCE Loss: 1.038313388824463\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 6.664642810821533 | KNN Loss: 5.597926139831543 | BCE Loss: 1.0667165517807007\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 6.682305335998535 | KNN Loss: 5.6266913414001465 | BCE Loss: 1.0556141138076782\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 6.677422046661377 | KNN Loss: 5.61354398727417 | BCE Loss: 1.063878059387207\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 6.656832695007324 | KNN Loss: 5.625508785247803 | BCE Loss: 1.0313236713409424\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 6.666133403778076 | KNN Loss: 5.6371026039123535 | BCE Loss: 1.0290309190750122\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 6.723276615142822 | KNN Loss: 5.679567337036133 | BCE Loss: 1.0437091588974\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 6.641147613525391 | KNN Loss: 5.602775573730469 | BCE Loss: 1.0383718013763428\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 6.669114112854004 | KNN Loss: 5.629045009613037 | BCE Loss: 1.0400691032409668\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 6.6463775634765625 | KNN Loss: 5.61376953125 | BCE Loss: 1.0326082706451416\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 6.6906023025512695 | KNN Loss: 5.640394687652588 | BCE Loss: 1.0502078533172607\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 6.687500476837158 | KNN Loss: 5.640380859375 | BCE Loss: 1.0471196174621582\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 6.690277099609375 | KNN Loss: 5.667277812957764 | BCE Loss: 1.0229991674423218\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 6.680466175079346 | KNN Loss: 5.65156364440918 | BCE Loss: 1.0289026498794556\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 6.680131912231445 | KNN Loss: 5.622615814208984 | BCE Loss: 1.0575159788131714\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 6.666280746459961 | KNN Loss: 5.603431701660156 | BCE Loss: 1.0628492832183838\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 6.673460960388184 | KNN Loss: 5.6116204261779785 | BCE Loss: 1.061840534210205\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 6.740987777709961 | KNN Loss: 5.690357685089111 | BCE Loss: 1.0506298542022705\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 6.658705234527588 | KNN Loss: 5.59495735168457 | BCE Loss: 1.0637478828430176\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 6.743352890014648 | KNN Loss: 5.707658290863037 | BCE Loss: 1.0356947183609009\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 6.7186784744262695 | KNN Loss: 5.696179389953613 | BCE Loss: 1.0224990844726562\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 6.66830587387085 | KNN Loss: 5.622802257537842 | BCE Loss: 1.0455034971237183\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 6.658657073974609 | KNN Loss: 5.620373249053955 | BCE Loss: 1.0382840633392334\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 6.76899528503418 | KNN Loss: 5.729526519775391 | BCE Loss: 1.0394690036773682\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 6.810711860656738 | KNN Loss: 5.754598140716553 | BCE Loss: 1.0561134815216064\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 6.743772506713867 | KNN Loss: 5.715705871582031 | BCE Loss: 1.028066873550415\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 6.654853820800781 | KNN Loss: 5.628942966461182 | BCE Loss: 1.0259108543395996\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 6.720773696899414 | KNN Loss: 5.674459457397461 | BCE Loss: 1.0463144779205322\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 6.771029472351074 | KNN Loss: 5.6935319900512695 | BCE Loss: 1.0774977207183838\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 6.7181830406188965 | KNN Loss: 5.681761264801025 | BCE Loss: 1.0364216566085815\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 6.700096130371094 | KNN Loss: 5.62910270690918 | BCE Loss: 1.070993185043335\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 6.670873641967773 | KNN Loss: 5.6050801277160645 | BCE Loss: 1.065793752670288\n",
      "Epoch   295: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 6.662669658660889 | KNN Loss: 5.596034049987793 | BCE Loss: 1.0666356086730957\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 6.662978172302246 | KNN Loss: 5.607808589935303 | BCE Loss: 1.0551694631576538\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 6.703555583953857 | KNN Loss: 5.6354193687438965 | BCE Loss: 1.068136215209961\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 6.674468994140625 | KNN Loss: 5.65369987487793 | BCE Loss: 1.0207693576812744\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 6.697656631469727 | KNN Loss: 5.654300212860107 | BCE Loss: 1.04335618019104\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 6.646181106567383 | KNN Loss: 5.600965976715088 | BCE Loss: 1.045215129852295\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 6.66656494140625 | KNN Loss: 5.612709045410156 | BCE Loss: 1.0538556575775146\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 6.623752593994141 | KNN Loss: 5.600231647491455 | BCE Loss: 1.0235211849212646\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 6.705496311187744 | KNN Loss: 5.610828876495361 | BCE Loss: 1.0946674346923828\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 6.6525139808654785 | KNN Loss: 5.600525379180908 | BCE Loss: 1.0519887208938599\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 6.652765274047852 | KNN Loss: 5.606042385101318 | BCE Loss: 1.046722650527954\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 6.626992225646973 | KNN Loss: 5.607944965362549 | BCE Loss: 1.0190471410751343\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 6.725877285003662 | KNN Loss: 5.667695045471191 | BCE Loss: 1.0581821203231812\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 6.722342491149902 | KNN Loss: 5.664845943450928 | BCE Loss: 1.0574965476989746\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 6.62180233001709 | KNN Loss: 5.590237617492676 | BCE Loss: 1.031564712524414\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 6.718160152435303 | KNN Loss: 5.639913558959961 | BCE Loss: 1.0782467126846313\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 6.632863998413086 | KNN Loss: 5.600426197052002 | BCE Loss: 1.032438039779663\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 6.654714584350586 | KNN Loss: 5.627099514007568 | BCE Loss: 1.027614951133728\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 6.728349208831787 | KNN Loss: 5.687324523925781 | BCE Loss: 1.0410248041152954\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 6.729082107543945 | KNN Loss: 5.669216156005859 | BCE Loss: 1.0598658323287964\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 6.692716598510742 | KNN Loss: 5.665499210357666 | BCE Loss: 1.0272172689437866\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 6.676555156707764 | KNN Loss: 5.618289947509766 | BCE Loss: 1.058265209197998\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 6.646554946899414 | KNN Loss: 5.5913777351379395 | BCE Loss: 1.0551772117614746\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 6.67620325088501 | KNN Loss: 5.6135783195495605 | BCE Loss: 1.0626249313354492\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 6.767343521118164 | KNN Loss: 5.712675094604492 | BCE Loss: 1.0546684265136719\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 6.697451114654541 | KNN Loss: 5.630797386169434 | BCE Loss: 1.0666537284851074\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 6.658297538757324 | KNN Loss: 5.629632472991943 | BCE Loss: 1.0286649465560913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 6.6573591232299805 | KNN Loss: 5.621152400970459 | BCE Loss: 1.036206603050232\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 6.644556999206543 | KNN Loss: 5.608725547790527 | BCE Loss: 1.0358316898345947\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 6.765723705291748 | KNN Loss: 5.718708038330078 | BCE Loss: 1.04701566696167\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 6.657115936279297 | KNN Loss: 5.594414234161377 | BCE Loss: 1.062701940536499\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 6.647456169128418 | KNN Loss: 5.620770454406738 | BCE Loss: 1.0266857147216797\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 6.661035060882568 | KNN Loss: 5.615723133087158 | BCE Loss: 1.0453120470046997\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 6.67130184173584 | KNN Loss: 5.6194987297058105 | BCE Loss: 1.0518031120300293\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 6.653002738952637 | KNN Loss: 5.616968631744385 | BCE Loss: 1.0360338687896729\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 6.691133975982666 | KNN Loss: 5.6361236572265625 | BCE Loss: 1.055010199546814\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 6.639162540435791 | KNN Loss: 5.601379871368408 | BCE Loss: 1.0377825498580933\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 6.653906345367432 | KNN Loss: 5.613804340362549 | BCE Loss: 1.0401020050048828\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 6.675021648406982 | KNN Loss: 5.62023401260376 | BCE Loss: 1.0547876358032227\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 6.647611618041992 | KNN Loss: 5.60263729095459 | BCE Loss: 1.0449745655059814\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 6.690118312835693 | KNN Loss: 5.60685920715332 | BCE Loss: 1.083259105682373\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 6.675365447998047 | KNN Loss: 5.628768444061279 | BCE Loss: 1.0465971231460571\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 6.653901100158691 | KNN Loss: 5.6017255783081055 | BCE Loss: 1.0521752834320068\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 6.700772285461426 | KNN Loss: 5.650243282318115 | BCE Loss: 1.0505291223526\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 6.651455879211426 | KNN Loss: 5.633096218109131 | BCE Loss: 1.0183597803115845\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 6.674160003662109 | KNN Loss: 5.622757434844971 | BCE Loss: 1.0514025688171387\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 6.699312210083008 | KNN Loss: 5.645987510681152 | BCE Loss: 1.0533246994018555\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 6.638211250305176 | KNN Loss: 5.600762367248535 | BCE Loss: 1.0374486446380615\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 6.657271385192871 | KNN Loss: 5.608494758605957 | BCE Loss: 1.048776388168335\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 6.708510875701904 | KNN Loss: 5.644798278808594 | BCE Loss: 1.0637125968933105\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 6.674577236175537 | KNN Loss: 5.618679523468018 | BCE Loss: 1.0558977127075195\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 6.680060386657715 | KNN Loss: 5.629144668579102 | BCE Loss: 1.0509159564971924\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 6.636678695678711 | KNN Loss: 5.608617305755615 | BCE Loss: 1.0280612707138062\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 6.70048189163208 | KNN Loss: 5.621391773223877 | BCE Loss: 1.0790901184082031\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 6.669124126434326 | KNN Loss: 5.608306884765625 | BCE Loss: 1.0608173608779907\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 6.67225456237793 | KNN Loss: 5.596653461456299 | BCE Loss: 1.0756009817123413\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 6.695164680480957 | KNN Loss: 5.649964332580566 | BCE Loss: 1.0452005863189697\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 6.653352737426758 | KNN Loss: 5.600316524505615 | BCE Loss: 1.0530359745025635\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 6.649074554443359 | KNN Loss: 5.608774662017822 | BCE Loss: 1.0402997732162476\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 6.658329010009766 | KNN Loss: 5.604674816131592 | BCE Loss: 1.0536539554595947\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 6.651538848876953 | KNN Loss: 5.606462001800537 | BCE Loss: 1.0450769662857056\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 6.730669975280762 | KNN Loss: 5.654615879058838 | BCE Loss: 1.076054334640503\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 6.642765998840332 | KNN Loss: 5.591102600097656 | BCE Loss: 1.0516633987426758\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 6.64339542388916 | KNN Loss: 5.611494064331055 | BCE Loss: 1.0319013595581055\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 6.664117813110352 | KNN Loss: 5.616881847381592 | BCE Loss: 1.0472357273101807\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 6.636318206787109 | KNN Loss: 5.60250186920166 | BCE Loss: 1.0338160991668701\n",
      "Epoch   306: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 6.661432266235352 | KNN Loss: 5.626972675323486 | BCE Loss: 1.0344598293304443\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 6.672830581665039 | KNN Loss: 5.604045867919922 | BCE Loss: 1.0687847137451172\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 6.70025110244751 | KNN Loss: 5.654470920562744 | BCE Loss: 1.0457801818847656\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 6.70180606842041 | KNN Loss: 5.637814998626709 | BCE Loss: 1.0639913082122803\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 6.695761203765869 | KNN Loss: 5.626866817474365 | BCE Loss: 1.0688942670822144\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 6.728964805603027 | KNN Loss: 5.677584171295166 | BCE Loss: 1.0513808727264404\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 6.646912574768066 | KNN Loss: 5.609849452972412 | BCE Loss: 1.0370633602142334\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 6.665300369262695 | KNN Loss: 5.620702743530273 | BCE Loss: 1.0445976257324219\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 6.63189697265625 | KNN Loss: 5.597482681274414 | BCE Loss: 1.0344141721725464\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 6.746556282043457 | KNN Loss: 5.673584461212158 | BCE Loss: 1.0729718208312988\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 6.640942573547363 | KNN Loss: 5.596615791320801 | BCE Loss: 1.044326663017273\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 6.647732734680176 | KNN Loss: 5.602496147155762 | BCE Loss: 1.045236587524414\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 6.694955348968506 | KNN Loss: 5.628738880157471 | BCE Loss: 1.0662164688110352\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 6.681144714355469 | KNN Loss: 5.628787040710449 | BCE Loss: 1.0523574352264404\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 6.698249816894531 | KNN Loss: 5.635230541229248 | BCE Loss: 1.0630192756652832\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 6.694222927093506 | KNN Loss: 5.649939060211182 | BCE Loss: 1.0442838668823242\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 6.665594100952148 | KNN Loss: 5.615332126617432 | BCE Loss: 1.0502617359161377\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 6.621876239776611 | KNN Loss: 5.604602813720703 | BCE Loss: 1.0172735452651978\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 6.6940765380859375 | KNN Loss: 5.653433322906494 | BCE Loss: 1.0406432151794434\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 6.640373229980469 | KNN Loss: 5.603087425231934 | BCE Loss: 1.0372860431671143\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 6.615899085998535 | KNN Loss: 5.594881057739258 | BCE Loss: 1.0210177898406982\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 6.710695266723633 | KNN Loss: 5.636562347412109 | BCE Loss: 1.0741326808929443\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 6.665886878967285 | KNN Loss: 5.599370002746582 | BCE Loss: 1.0665168762207031\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 6.679097652435303 | KNN Loss: 5.630233287811279 | BCE Loss: 1.0488642454147339\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 6.701274871826172 | KNN Loss: 5.636949062347412 | BCE Loss: 1.0643255710601807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 6.633763313293457 | KNN Loss: 5.593297958374023 | BCE Loss: 1.0404654741287231\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 6.754829406738281 | KNN Loss: 5.69495964050293 | BCE Loss: 1.059869647026062\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 6.694135665893555 | KNN Loss: 5.628715991973877 | BCE Loss: 1.0654199123382568\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 6.6176676750183105 | KNN Loss: 5.596700191497803 | BCE Loss: 1.0209676027297974\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 6.694865703582764 | KNN Loss: 5.660048961639404 | BCE Loss: 1.0348167419433594\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 6.619037628173828 | KNN Loss: 5.601258277893066 | BCE Loss: 1.0177795886993408\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 6.774803638458252 | KNN Loss: 5.738369464874268 | BCE Loss: 1.0364341735839844\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 6.612616539001465 | KNN Loss: 5.597124099731445 | BCE Loss: 1.0154924392700195\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 6.658266067504883 | KNN Loss: 5.617673873901367 | BCE Loss: 1.0405919551849365\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 6.651097297668457 | KNN Loss: 5.61748743057251 | BCE Loss: 1.0336098670959473\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 6.730699062347412 | KNN Loss: 5.671796798706055 | BCE Loss: 1.058902382850647\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 6.728343486785889 | KNN Loss: 5.67080545425415 | BCE Loss: 1.0575379133224487\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 6.679731845855713 | KNN Loss: 5.607266426086426 | BCE Loss: 1.0724655389785767\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 6.70001745223999 | KNN Loss: 5.638530731201172 | BCE Loss: 1.0614867210388184\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 6.658930778503418 | KNN Loss: 5.619757652282715 | BCE Loss: 1.0391731262207031\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 6.721282005310059 | KNN Loss: 5.663985729217529 | BCE Loss: 1.0572962760925293\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 6.663166522979736 | KNN Loss: 5.644693374633789 | BCE Loss: 1.0184731483459473\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 6.695517063140869 | KNN Loss: 5.643880367279053 | BCE Loss: 1.0516365766525269\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 6.6328630447387695 | KNN Loss: 5.599780559539795 | BCE Loss: 1.0330824851989746\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 6.63648796081543 | KNN Loss: 5.600114345550537 | BCE Loss: 1.0363738536834717\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 6.7158613204956055 | KNN Loss: 5.673163890838623 | BCE Loss: 1.0426974296569824\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 6.682792663574219 | KNN Loss: 5.620325088500977 | BCE Loss: 1.0624678134918213\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 6.660734176635742 | KNN Loss: 5.6264424324035645 | BCE Loss: 1.0342915058135986\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 6.643126487731934 | KNN Loss: 5.592804431915283 | BCE Loss: 1.0503220558166504\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 6.752788066864014 | KNN Loss: 5.689457893371582 | BCE Loss: 1.063330054283142\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 6.692525863647461 | KNN Loss: 5.607222080230713 | BCE Loss: 1.085303783416748\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 6.6795244216918945 | KNN Loss: 5.611776351928711 | BCE Loss: 1.0677480697631836\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 6.660649299621582 | KNN Loss: 5.650441646575928 | BCE Loss: 1.0102078914642334\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 6.649815559387207 | KNN Loss: 5.603760719299316 | BCE Loss: 1.0460546016693115\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 6.642529010772705 | KNN Loss: 5.614974498748779 | BCE Loss: 1.0275545120239258\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 6.663299083709717 | KNN Loss: 5.610530853271484 | BCE Loss: 1.0527682304382324\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 6.652155876159668 | KNN Loss: 5.593021392822266 | BCE Loss: 1.0591344833374023\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 6.624824523925781 | KNN Loss: 5.595686435699463 | BCE Loss: 1.0291380882263184\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 6.652071475982666 | KNN Loss: 5.608006954193115 | BCE Loss: 1.0440646409988403\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 6.684456825256348 | KNN Loss: 5.621160507202148 | BCE Loss: 1.0632961988449097\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 6.700029373168945 | KNN Loss: 5.656835556030273 | BCE Loss: 1.0431935787200928\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 6.691176414489746 | KNN Loss: 5.620488166809082 | BCE Loss: 1.070688009262085\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 6.717816352844238 | KNN Loss: 5.6641459465026855 | BCE Loss: 1.0536701679229736\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 6.618740558624268 | KNN Loss: 5.596923351287842 | BCE Loss: 1.0218170881271362\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 6.680844306945801 | KNN Loss: 5.633426666259766 | BCE Loss: 1.0474177598953247\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 6.684327602386475 | KNN Loss: 5.621732234954834 | BCE Loss: 1.062595248222351\n",
      "Epoch   317: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 6.6685638427734375 | KNN Loss: 5.601217269897461 | BCE Loss: 1.0673463344573975\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 6.670567512512207 | KNN Loss: 5.596256732940674 | BCE Loss: 1.0743110179901123\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 6.65876579284668 | KNN Loss: 5.5954742431640625 | BCE Loss: 1.063291311264038\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 6.655832290649414 | KNN Loss: 5.608129024505615 | BCE Loss: 1.0477030277252197\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 6.668533802032471 | KNN Loss: 5.6315789222717285 | BCE Loss: 1.0369548797607422\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 6.665383815765381 | KNN Loss: 5.614802360534668 | BCE Loss: 1.050581455230713\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 6.641848087310791 | KNN Loss: 5.608405113220215 | BCE Loss: 1.0334428548812866\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 6.626988410949707 | KNN Loss: 5.6057915687561035 | BCE Loss: 1.0211968421936035\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 6.68889856338501 | KNN Loss: 5.636778831481934 | BCE Loss: 1.0521196126937866\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 6.638162612915039 | KNN Loss: 5.599670886993408 | BCE Loss: 1.0384914875030518\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 6.670034885406494 | KNN Loss: 5.602916240692139 | BCE Loss: 1.067118525505066\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 6.643759727478027 | KNN Loss: 5.6031880378723145 | BCE Loss: 1.040571689605713\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 6.654790878295898 | KNN Loss: 5.617843151092529 | BCE Loss: 1.0369479656219482\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 6.604188442230225 | KNN Loss: 5.594293594360352 | BCE Loss: 1.009894847869873\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 6.795619010925293 | KNN Loss: 5.7317094802856445 | BCE Loss: 1.0639095306396484\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 6.643986701965332 | KNN Loss: 5.607019424438477 | BCE Loss: 1.0369675159454346\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 6.666203022003174 | KNN Loss: 5.606588840484619 | BCE Loss: 1.0596141815185547\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 6.6661295890808105 | KNN Loss: 5.605459690093994 | BCE Loss: 1.060670018196106\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 6.757590293884277 | KNN Loss: 5.713083744049072 | BCE Loss: 1.0445067882537842\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 6.668100357055664 | KNN Loss: 5.617695331573486 | BCE Loss: 1.0504047870635986\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 6.6630353927612305 | KNN Loss: 5.629186630249023 | BCE Loss: 1.0338490009307861\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 6.769522666931152 | KNN Loss: 5.724308490753174 | BCE Loss: 1.045214295387268\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 6.650460720062256 | KNN Loss: 5.604372978210449 | BCE Loss: 1.0460877418518066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 6.708527088165283 | KNN Loss: 5.649270534515381 | BCE Loss: 1.059256672859192\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 6.645556449890137 | KNN Loss: 5.595376014709473 | BCE Loss: 1.050180196762085\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 6.6319379806518555 | KNN Loss: 5.604128837585449 | BCE Loss: 1.0278091430664062\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 6.673250675201416 | KNN Loss: 5.598572731018066 | BCE Loss: 1.0746779441833496\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 6.7458906173706055 | KNN Loss: 5.669541835784912 | BCE Loss: 1.0763490200042725\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 6.619791030883789 | KNN Loss: 5.611082553863525 | BCE Loss: 1.0087083578109741\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 6.68602180480957 | KNN Loss: 5.656915664672852 | BCE Loss: 1.0291062593460083\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 6.647005558013916 | KNN Loss: 5.624006271362305 | BCE Loss: 1.0229992866516113\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 6.630483150482178 | KNN Loss: 5.60448694229126 | BCE Loss: 1.0259963274002075\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 6.672612190246582 | KNN Loss: 5.630108833312988 | BCE Loss: 1.0425034761428833\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 6.670381546020508 | KNN Loss: 5.6125807762146 | BCE Loss: 1.057800531387329\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 6.659061431884766 | KNN Loss: 5.602285385131836 | BCE Loss: 1.0567762851715088\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 6.679032325744629 | KNN Loss: 5.649073123931885 | BCE Loss: 1.029958963394165\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 6.671302795410156 | KNN Loss: 5.614190101623535 | BCE Loss: 1.057112455368042\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 6.736760139465332 | KNN Loss: 5.717348098754883 | BCE Loss: 1.0194120407104492\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 6.686921119689941 | KNN Loss: 5.626439094543457 | BCE Loss: 1.0604817867279053\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 6.6748833656311035 | KNN Loss: 5.648714542388916 | BCE Loss: 1.0261688232421875\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 6.659229755401611 | KNN Loss: 5.609473705291748 | BCE Loss: 1.0497560501098633\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 6.652630805969238 | KNN Loss: 5.606927394866943 | BCE Loss: 1.0457032918930054\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 6.661838531494141 | KNN Loss: 5.609553813934326 | BCE Loss: 1.052284836769104\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 6.64747953414917 | KNN Loss: 5.60451602935791 | BCE Loss: 1.0429636240005493\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 6.610299110412598 | KNN Loss: 5.601841449737549 | BCE Loss: 1.0084575414657593\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 6.656216144561768 | KNN Loss: 5.59524393081665 | BCE Loss: 1.0609723329544067\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 6.641962051391602 | KNN Loss: 5.60648775100708 | BCE Loss: 1.035474419593811\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 6.6845173835754395 | KNN Loss: 5.632802486419678 | BCE Loss: 1.0517148971557617\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 6.703696250915527 | KNN Loss: 5.6479878425598145 | BCE Loss: 1.0557081699371338\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 6.741098403930664 | KNN Loss: 5.654675006866455 | BCE Loss: 1.0864232778549194\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 6.619013786315918 | KNN Loss: 5.6003737449646 | BCE Loss: 1.0186399221420288\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 6.618244171142578 | KNN Loss: 5.592884540557861 | BCE Loss: 1.0253593921661377\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 6.630425453186035 | KNN Loss: 5.6005988121032715 | BCE Loss: 1.0298266410827637\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 6.658562660217285 | KNN Loss: 5.635997772216797 | BCE Loss: 1.0225651264190674\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 6.779868125915527 | KNN Loss: 5.726515769958496 | BCE Loss: 1.0533521175384521\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 6.659609794616699 | KNN Loss: 5.611764430999756 | BCE Loss: 1.0478456020355225\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 6.650238513946533 | KNN Loss: 5.606893062591553 | BCE Loss: 1.04334557056427\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 6.685023784637451 | KNN Loss: 5.621097087860107 | BCE Loss: 1.0639266967773438\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 6.657617568969727 | KNN Loss: 5.59473991394043 | BCE Loss: 1.0628774166107178\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 6.669836044311523 | KNN Loss: 5.618436336517334 | BCE Loss: 1.0513994693756104\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 6.654012680053711 | KNN Loss: 5.606784820556641 | BCE Loss: 1.0472280979156494\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 6.67907190322876 | KNN Loss: 5.623318195343018 | BCE Loss: 1.0557537078857422\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 6.715912818908691 | KNN Loss: 5.656308650970459 | BCE Loss: 1.0596044063568115\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 6.716983795166016 | KNN Loss: 5.669431686401367 | BCE Loss: 1.0475521087646484\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 6.649941921234131 | KNN Loss: 5.617618083953857 | BCE Loss: 1.0323238372802734\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 6.646703720092773 | KNN Loss: 5.621713638305664 | BCE Loss: 1.0249900817871094\n",
      "Epoch   328: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 6.6169281005859375 | KNN Loss: 5.592538833618164 | BCE Loss: 1.0243891477584839\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 6.626298904418945 | KNN Loss: 5.594640731811523 | BCE Loss: 1.0316579341888428\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 6.66301155090332 | KNN Loss: 5.618034839630127 | BCE Loss: 1.0449767112731934\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 6.701761245727539 | KNN Loss: 5.6536126136779785 | BCE Loss: 1.0481483936309814\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 6.6643147468566895 | KNN Loss: 5.637723922729492 | BCE Loss: 1.0265909433364868\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 6.682584285736084 | KNN Loss: 5.638323783874512 | BCE Loss: 1.0442605018615723\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 6.695713043212891 | KNN Loss: 5.638551235198975 | BCE Loss: 1.057161808013916\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 6.657718181610107 | KNN Loss: 5.601617813110352 | BCE Loss: 1.0561003684997559\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 6.644341468811035 | KNN Loss: 5.599366188049316 | BCE Loss: 1.0449752807617188\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 6.639103889465332 | KNN Loss: 5.610225677490234 | BCE Loss: 1.028878092765808\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 6.659489154815674 | KNN Loss: 5.5972137451171875 | BCE Loss: 1.0622755289077759\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 6.645391464233398 | KNN Loss: 5.631584644317627 | BCE Loss: 1.0138068199157715\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 6.669389247894287 | KNN Loss: 5.646518707275391 | BCE Loss: 1.0228705406188965\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 6.648786544799805 | KNN Loss: 5.60065221786499 | BCE Loss: 1.0481340885162354\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 6.644440650939941 | KNN Loss: 5.6043853759765625 | BCE Loss: 1.040055274963379\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 6.6618523597717285 | KNN Loss: 5.604277610778809 | BCE Loss: 1.0575746297836304\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 6.6428303718566895 | KNN Loss: 5.601161479949951 | BCE Loss: 1.0416690111160278\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 6.646573543548584 | KNN Loss: 5.595157623291016 | BCE Loss: 1.0514158010482788\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 6.741649150848389 | KNN Loss: 5.68233585357666 | BCE Loss: 1.0593132972717285\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 6.648365020751953 | KNN Loss: 5.619485855102539 | BCE Loss: 1.028878927230835\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 6.656537055969238 | KNN Loss: 5.629430770874023 | BCE Loss: 1.0271060466766357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 6.7315473556518555 | KNN Loss: 5.689108848571777 | BCE Loss: 1.0424386262893677\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 6.647971153259277 | KNN Loss: 5.602514743804932 | BCE Loss: 1.0454561710357666\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 6.70372200012207 | KNN Loss: 5.6313066482543945 | BCE Loss: 1.0724151134490967\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 6.655806541442871 | KNN Loss: 5.602598667144775 | BCE Loss: 1.0532081127166748\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 6.665623664855957 | KNN Loss: 5.626487731933594 | BCE Loss: 1.0391356945037842\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 6.710417747497559 | KNN Loss: 5.678835391998291 | BCE Loss: 1.0315825939178467\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 6.741229057312012 | KNN Loss: 5.651371479034424 | BCE Loss: 1.089857578277588\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 6.70876407623291 | KNN Loss: 5.661553382873535 | BCE Loss: 1.047210454940796\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 6.684823513031006 | KNN Loss: 5.645313739776611 | BCE Loss: 1.039509892463684\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 6.688681602478027 | KNN Loss: 5.602791786193848 | BCE Loss: 1.0858895778656006\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 6.667075157165527 | KNN Loss: 5.62163782119751 | BCE Loss: 1.0454370975494385\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 6.6903791427612305 | KNN Loss: 5.630531311035156 | BCE Loss: 1.0598475933074951\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 6.635544776916504 | KNN Loss: 5.5943779945373535 | BCE Loss: 1.0411670207977295\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 6.668172836303711 | KNN Loss: 5.6426873207092285 | BCE Loss: 1.0254857540130615\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 6.661268711090088 | KNN Loss: 5.616804122924805 | BCE Loss: 1.0444645881652832\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 6.6474609375 | KNN Loss: 5.591826438903809 | BCE Loss: 1.0556347370147705\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 6.647789001464844 | KNN Loss: 5.594227313995361 | BCE Loss: 1.0535619258880615\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 6.783976078033447 | KNN Loss: 5.755843639373779 | BCE Loss: 1.0281323194503784\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 6.6301093101501465 | KNN Loss: 5.598874092102051 | BCE Loss: 1.0312352180480957\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 6.665536403656006 | KNN Loss: 5.603107452392578 | BCE Loss: 1.0624289512634277\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 6.685797691345215 | KNN Loss: 5.628713607788086 | BCE Loss: 1.057084083557129\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 6.647941589355469 | KNN Loss: 5.604095935821533 | BCE Loss: 1.0438454151153564\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 6.624122142791748 | KNN Loss: 5.5992279052734375 | BCE Loss: 1.0248942375183105\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 6.729942321777344 | KNN Loss: 5.6574602127075195 | BCE Loss: 1.0724821090698242\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 6.672562599182129 | KNN Loss: 5.641453742980957 | BCE Loss: 1.0311086177825928\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 6.6448187828063965 | KNN Loss: 5.622047424316406 | BCE Loss: 1.0227713584899902\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 6.676420211791992 | KNN Loss: 5.630734920501709 | BCE Loss: 1.0456855297088623\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 6.726620674133301 | KNN Loss: 5.67501163482666 | BCE Loss: 1.0516092777252197\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 6.644811153411865 | KNN Loss: 5.623366355895996 | BCE Loss: 1.0214449167251587\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 6.664512634277344 | KNN Loss: 5.639810085296631 | BCE Loss: 1.024702548980713\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 6.666477203369141 | KNN Loss: 5.6630730628967285 | BCE Loss: 1.003403902053833\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 6.687339782714844 | KNN Loss: 5.633432388305664 | BCE Loss: 1.0539075136184692\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 6.670844078063965 | KNN Loss: 5.663739204406738 | BCE Loss: 1.007104754447937\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 6.70348596572876 | KNN Loss: 5.659343242645264 | BCE Loss: 1.0441428422927856\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 6.673616409301758 | KNN Loss: 5.617397308349609 | BCE Loss: 1.0562188625335693\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 6.620883941650391 | KNN Loss: 5.5940070152282715 | BCE Loss: 1.02687668800354\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 6.634446144104004 | KNN Loss: 5.603946208953857 | BCE Loss: 1.0304996967315674\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 6.711855888366699 | KNN Loss: 5.672237396240234 | BCE Loss: 1.0396182537078857\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 6.641622543334961 | KNN Loss: 5.628549575805664 | BCE Loss: 1.013073205947876\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 6.646251678466797 | KNN Loss: 5.607396125793457 | BCE Loss: 1.0388555526733398\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 6.654338359832764 | KNN Loss: 5.614856719970703 | BCE Loss: 1.0394816398620605\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 6.67454719543457 | KNN Loss: 5.618838310241699 | BCE Loss: 1.0557091236114502\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 6.651458263397217 | KNN Loss: 5.608501434326172 | BCE Loss: 1.042956829071045\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 6.697833061218262 | KNN Loss: 5.612973690032959 | BCE Loss: 1.0848591327667236\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 6.678903579711914 | KNN Loss: 5.675042152404785 | BCE Loss: 1.003861665725708\n",
      "Epoch   339: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 6.71917724609375 | KNN Loss: 5.696117401123047 | BCE Loss: 1.0230597257614136\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 6.686036109924316 | KNN Loss: 5.651702404022217 | BCE Loss: 1.0343338251113892\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 6.692131996154785 | KNN Loss: 5.654839515686035 | BCE Loss: 1.037292242050171\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 6.660240650177002 | KNN Loss: 5.612785339355469 | BCE Loss: 1.0474551916122437\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 6.662037372589111 | KNN Loss: 5.606977462768555 | BCE Loss: 1.0550599098205566\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 6.688518524169922 | KNN Loss: 5.630831241607666 | BCE Loss: 1.0576872825622559\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 6.6734514236450195 | KNN Loss: 5.616764068603516 | BCE Loss: 1.056687593460083\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 6.752884864807129 | KNN Loss: 5.746933460235596 | BCE Loss: 1.0059516429901123\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 6.781553268432617 | KNN Loss: 5.717006683349609 | BCE Loss: 1.064546823501587\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 6.687533378601074 | KNN Loss: 5.637614727020264 | BCE Loss: 1.0499186515808105\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 6.624154090881348 | KNN Loss: 5.60820198059082 | BCE Loss: 1.0159519910812378\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 6.611462593078613 | KNN Loss: 5.6023077964782715 | BCE Loss: 1.0091547966003418\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 6.645814895629883 | KNN Loss: 5.615415096282959 | BCE Loss: 1.0303997993469238\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 6.7333197593688965 | KNN Loss: 5.692891597747803 | BCE Loss: 1.0404280424118042\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 6.756831645965576 | KNN Loss: 5.699320316314697 | BCE Loss: 1.057511329650879\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 6.656172275543213 | KNN Loss: 5.5992865562438965 | BCE Loss: 1.0568856000900269\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 6.72736930847168 | KNN Loss: 5.649493217468262 | BCE Loss: 1.077876091003418\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 6.671334743499756 | KNN Loss: 5.608583450317383 | BCE Loss: 1.0627511739730835\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 6.655282974243164 | KNN Loss: 5.626035213470459 | BCE Loss: 1.0292476415634155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 6.6407976150512695 | KNN Loss: 5.592962741851807 | BCE Loss: 1.047835111618042\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 6.6979241371154785 | KNN Loss: 5.63923978805542 | BCE Loss: 1.0586844682693481\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 6.700593948364258 | KNN Loss: 5.651535987854004 | BCE Loss: 1.0490577220916748\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 6.670642852783203 | KNN Loss: 5.606019973754883 | BCE Loss: 1.0646231174468994\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 6.668867588043213 | KNN Loss: 5.600662708282471 | BCE Loss: 1.0682048797607422\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 6.66922664642334 | KNN Loss: 5.607205867767334 | BCE Loss: 1.062021017074585\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 6.658748149871826 | KNN Loss: 5.604900360107422 | BCE Loss: 1.0538479089736938\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 6.762999534606934 | KNN Loss: 5.697632312774658 | BCE Loss: 1.0653672218322754\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 6.650981903076172 | KNN Loss: 5.620885848999023 | BCE Loss: 1.0300958156585693\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 6.673485279083252 | KNN Loss: 5.642637729644775 | BCE Loss: 1.030847430229187\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 6.645937919616699 | KNN Loss: 5.601288795471191 | BCE Loss: 1.0446488857269287\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 6.664641380310059 | KNN Loss: 5.633708477020264 | BCE Loss: 1.030933141708374\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 6.67641019821167 | KNN Loss: 5.652267932891846 | BCE Loss: 1.0241422653198242\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 6.672021389007568 | KNN Loss: 5.614238262176514 | BCE Loss: 1.0577832460403442\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 6.6386847496032715 | KNN Loss: 5.619193077087402 | BCE Loss: 1.0194916725158691\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 6.742149353027344 | KNN Loss: 5.70031213760376 | BCE Loss: 1.0418369770050049\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 6.654806137084961 | KNN Loss: 5.605942249298096 | BCE Loss: 1.0488638877868652\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 6.646601676940918 | KNN Loss: 5.603422164916992 | BCE Loss: 1.0431795120239258\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 6.665268898010254 | KNN Loss: 5.596478462219238 | BCE Loss: 1.0687904357910156\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 6.636975288391113 | KNN Loss: 5.6077680587768555 | BCE Loss: 1.0292072296142578\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 6.668379783630371 | KNN Loss: 5.595643997192383 | BCE Loss: 1.0727359056472778\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 6.652064800262451 | KNN Loss: 5.613795757293701 | BCE Loss: 1.0382691621780396\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 6.673087120056152 | KNN Loss: 5.621706485748291 | BCE Loss: 1.0513806343078613\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 6.753923416137695 | KNN Loss: 5.708530426025391 | BCE Loss: 1.0453927516937256\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 6.651152610778809 | KNN Loss: 5.604558944702148 | BCE Loss: 1.0465936660766602\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 6.678652763366699 | KNN Loss: 5.657460689544678 | BCE Loss: 1.021191954612732\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 6.706320762634277 | KNN Loss: 5.653698444366455 | BCE Loss: 1.0526224374771118\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 6.680131435394287 | KNN Loss: 5.64752721786499 | BCE Loss: 1.0326042175292969\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 6.715555191040039 | KNN Loss: 5.670895576477051 | BCE Loss: 1.0446598529815674\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 6.653311729431152 | KNN Loss: 5.5938897132873535 | BCE Loss: 1.0594220161437988\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 6.726415634155273 | KNN Loss: 5.691189289093018 | BCE Loss: 1.0352263450622559\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 6.715020179748535 | KNN Loss: 5.653932571411133 | BCE Loss: 1.0610876083374023\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 6.646911144256592 | KNN Loss: 5.609526634216309 | BCE Loss: 1.0373846292495728\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 6.699846267700195 | KNN Loss: 5.679692268371582 | BCE Loss: 1.0201537609100342\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 6.640732765197754 | KNN Loss: 5.616364002227783 | BCE Loss: 1.0243685245513916\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 6.728382110595703 | KNN Loss: 5.682849407196045 | BCE Loss: 1.045532464981079\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 6.663409233093262 | KNN Loss: 5.622247695922852 | BCE Loss: 1.0411616563796997\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 6.6319732666015625 | KNN Loss: 5.596029281616211 | BCE Loss: 1.0359442234039307\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 6.750516414642334 | KNN Loss: 5.719211101531982 | BCE Loss: 1.0313053131103516\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 6.738390922546387 | KNN Loss: 5.67660665512085 | BCE Loss: 1.061784029006958\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 6.629641532897949 | KNN Loss: 5.592979431152344 | BCE Loss: 1.036661982536316\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 6.631609916687012 | KNN Loss: 5.598077774047852 | BCE Loss: 1.033531904220581\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 6.650446891784668 | KNN Loss: 5.601805686950684 | BCE Loss: 1.048641324043274\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 6.64766788482666 | KNN Loss: 5.615767478942871 | BCE Loss: 1.03190016746521\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 6.642110824584961 | KNN Loss: 5.609127521514893 | BCE Loss: 1.0329830646514893\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 6.628662586212158 | KNN Loss: 5.607028007507324 | BCE Loss: 1.021634578704834\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 6.711279392242432 | KNN Loss: 5.669569969177246 | BCE Loss: 1.0417094230651855\n",
      "Epoch   350: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 6.636693000793457 | KNN Loss: 5.5938897132873535 | BCE Loss: 1.0428035259246826\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 6.723494529724121 | KNN Loss: 5.670223712921143 | BCE Loss: 1.0532708168029785\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 6.721138954162598 | KNN Loss: 5.658965110778809 | BCE Loss: 1.062173843383789\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 6.680335998535156 | KNN Loss: 5.61256742477417 | BCE Loss: 1.0677684545516968\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 6.715959548950195 | KNN Loss: 5.664826393127441 | BCE Loss: 1.051133155822754\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 6.690445899963379 | KNN Loss: 5.66827392578125 | BCE Loss: 1.022171974182129\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 6.675487995147705 | KNN Loss: 5.6229071617126465 | BCE Loss: 1.052580714225769\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 6.662515640258789 | KNN Loss: 5.633817195892334 | BCE Loss: 1.028698205947876\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 6.634342193603516 | KNN Loss: 5.599291801452637 | BCE Loss: 1.035050392150879\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 6.6469340324401855 | KNN Loss: 5.601261138916016 | BCE Loss: 1.04567289352417\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 6.647681713104248 | KNN Loss: 5.59110689163208 | BCE Loss: 1.0565747022628784\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 6.672245025634766 | KNN Loss: 5.608793258666992 | BCE Loss: 1.0634516477584839\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 6.641852378845215 | KNN Loss: 5.6049418449401855 | BCE Loss: 1.0369102954864502\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 6.735709190368652 | KNN Loss: 5.668083667755127 | BCE Loss: 1.0676252841949463\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 6.6539692878723145 | KNN Loss: 5.591919422149658 | BCE Loss: 1.0620499849319458\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 6.645332336425781 | KNN Loss: 5.594425201416016 | BCE Loss: 1.0509073734283447\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 6.6745710372924805 | KNN Loss: 5.647494792938232 | BCE Loss: 1.027076244354248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 6.6544647216796875 | KNN Loss: 5.615653991699219 | BCE Loss: 1.0388107299804688\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 6.666289806365967 | KNN Loss: 5.6069254875183105 | BCE Loss: 1.0593643188476562\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 6.682476997375488 | KNN Loss: 5.646657943725586 | BCE Loss: 1.035819172859192\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 6.756270408630371 | KNN Loss: 5.703603267669678 | BCE Loss: 1.0526673793792725\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 6.6673479080200195 | KNN Loss: 5.609593868255615 | BCE Loss: 1.0577540397644043\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 6.625819206237793 | KNN Loss: 5.597403526306152 | BCE Loss: 1.0284159183502197\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 6.714117050170898 | KNN Loss: 5.66993522644043 | BCE Loss: 1.0441818237304688\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 6.725008010864258 | KNN Loss: 5.658242702484131 | BCE Loss: 1.066765308380127\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 6.655300140380859 | KNN Loss: 5.599615573883057 | BCE Loss: 1.0556845664978027\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 6.638298511505127 | KNN Loss: 5.592414855957031 | BCE Loss: 1.0458836555480957\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 6.709959983825684 | KNN Loss: 5.67646598815918 | BCE Loss: 1.0334937572479248\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 6.648039817810059 | KNN Loss: 5.601714134216309 | BCE Loss: 1.0463258028030396\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 6.69471549987793 | KNN Loss: 5.646024703979492 | BCE Loss: 1.0486905574798584\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 6.648701190948486 | KNN Loss: 5.6238789558410645 | BCE Loss: 1.0248222351074219\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 6.629310607910156 | KNN Loss: 5.609652996063232 | BCE Loss: 1.019657850265503\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 6.643802165985107 | KNN Loss: 5.600877285003662 | BCE Loss: 1.0429248809814453\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 6.67281436920166 | KNN Loss: 5.599111080169678 | BCE Loss: 1.0737030506134033\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 6.670706748962402 | KNN Loss: 5.616050720214844 | BCE Loss: 1.0546557903289795\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 6.612863540649414 | KNN Loss: 5.60322904586792 | BCE Loss: 1.0096344947814941\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 6.661067962646484 | KNN Loss: 5.618586540222168 | BCE Loss: 1.0424811840057373\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 6.7553300857543945 | KNN Loss: 5.681726455688477 | BCE Loss: 1.0736035108566284\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 6.679555892944336 | KNN Loss: 5.646489143371582 | BCE Loss: 1.0330665111541748\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 6.694669723510742 | KNN Loss: 5.655519008636475 | BCE Loss: 1.039150595664978\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 6.742839813232422 | KNN Loss: 5.6621222496032715 | BCE Loss: 1.0807175636291504\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 6.6431074142456055 | KNN Loss: 5.603233814239502 | BCE Loss: 1.039873719215393\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 6.706787109375 | KNN Loss: 5.6616692543029785 | BCE Loss: 1.0451176166534424\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 6.643103122711182 | KNN Loss: 5.593964576721191 | BCE Loss: 1.0491386651992798\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 6.674458026885986 | KNN Loss: 5.641410827636719 | BCE Loss: 1.0330471992492676\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 6.715982913970947 | KNN Loss: 5.6658735275268555 | BCE Loss: 1.0501093864440918\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 6.681130409240723 | KNN Loss: 5.626828670501709 | BCE Loss: 1.0543017387390137\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 6.664963722229004 | KNN Loss: 5.616211414337158 | BCE Loss: 1.0487520694732666\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 6.666082382202148 | KNN Loss: 5.619819164276123 | BCE Loss: 1.0462634563446045\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 6.658764362335205 | KNN Loss: 5.603793621063232 | BCE Loss: 1.054970622062683\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 6.695466995239258 | KNN Loss: 5.622559070587158 | BCE Loss: 1.0729081630706787\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 6.68768310546875 | KNN Loss: 5.619174957275391 | BCE Loss: 1.068508267402649\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 6.680596351623535 | KNN Loss: 5.63598108291626 | BCE Loss: 1.0446155071258545\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 6.66478157043457 | KNN Loss: 5.617677211761475 | BCE Loss: 1.0471043586730957\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 6.657992362976074 | KNN Loss: 5.6183624267578125 | BCE Loss: 1.0396298170089722\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 6.674851417541504 | KNN Loss: 5.60900354385376 | BCE Loss: 1.0658478736877441\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 6.685514450073242 | KNN Loss: 5.642401218414307 | BCE Loss: 1.0431134700775146\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 6.709840774536133 | KNN Loss: 5.64605188369751 | BCE Loss: 1.0637890100479126\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 6.673017501831055 | KNN Loss: 5.617613792419434 | BCE Loss: 1.0554038286209106\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 6.648682594299316 | KNN Loss: 5.63648796081543 | BCE Loss: 1.0121943950653076\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 6.654787063598633 | KNN Loss: 5.620181560516357 | BCE Loss: 1.0346055030822754\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 6.680094242095947 | KNN Loss: 5.621714115142822 | BCE Loss: 1.058380126953125\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 6.634124279022217 | KNN Loss: 5.606739044189453 | BCE Loss: 1.0273852348327637\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 6.768953323364258 | KNN Loss: 5.699297904968262 | BCE Loss: 1.069655418395996\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 6.6874918937683105 | KNN Loss: 5.607745170593262 | BCE Loss: 1.0797466039657593\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 6.711947441101074 | KNN Loss: 5.631039142608643 | BCE Loss: 1.0809080600738525\n",
      "Epoch   361: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 6.659214019775391 | KNN Loss: 5.601433277130127 | BCE Loss: 1.0577809810638428\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 6.663749694824219 | KNN Loss: 5.6196393966674805 | BCE Loss: 1.0441104173660278\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 6.738240718841553 | KNN Loss: 5.687899589538574 | BCE Loss: 1.050341248512268\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 6.664486408233643 | KNN Loss: 5.626681804656982 | BCE Loss: 1.0378044843673706\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 6.694374084472656 | KNN Loss: 5.62086820602417 | BCE Loss: 1.0735058784484863\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 6.68637752532959 | KNN Loss: 5.653938293457031 | BCE Loss: 1.032439112663269\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 6.672004699707031 | KNN Loss: 5.60502290725708 | BCE Loss: 1.0669817924499512\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 6.639395713806152 | KNN Loss: 5.611459732055664 | BCE Loss: 1.0279357433319092\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 6.677700519561768 | KNN Loss: 5.616366386413574 | BCE Loss: 1.0613341331481934\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 6.655440330505371 | KNN Loss: 5.604040145874023 | BCE Loss: 1.0513999462127686\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 6.655884265899658 | KNN Loss: 5.597165107727051 | BCE Loss: 1.0587190389633179\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 6.73544979095459 | KNN Loss: 5.664856910705566 | BCE Loss: 1.0705931186676025\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 6.6549530029296875 | KNN Loss: 5.605438709259033 | BCE Loss: 1.0495142936706543\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 6.667742729187012 | KNN Loss: 5.618465423583984 | BCE Loss: 1.0492775440216064\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 6.668243885040283 | KNN Loss: 5.630584716796875 | BCE Loss: 1.0376591682434082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 6.748856544494629 | KNN Loss: 5.713974952697754 | BCE Loss: 1.0348814725875854\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 6.731478691101074 | KNN Loss: 5.663523197174072 | BCE Loss: 1.0679552555084229\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 6.686615943908691 | KNN Loss: 5.676231384277344 | BCE Loss: 1.010384440422058\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 6.656398773193359 | KNN Loss: 5.6079254150390625 | BCE Loss: 1.0484731197357178\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 6.695870876312256 | KNN Loss: 5.652743339538574 | BCE Loss: 1.0431275367736816\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 6.622135639190674 | KNN Loss: 5.591168403625488 | BCE Loss: 1.030967354774475\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 6.6399736404418945 | KNN Loss: 5.591217517852783 | BCE Loss: 1.0487563610076904\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 6.634133338928223 | KNN Loss: 5.611938953399658 | BCE Loss: 1.0221943855285645\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 6.673995018005371 | KNN Loss: 5.639779090881348 | BCE Loss: 1.0342161655426025\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 6.702981472015381 | KNN Loss: 5.657914161682129 | BCE Loss: 1.0450671911239624\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 6.685697555541992 | KNN Loss: 5.6346564292907715 | BCE Loss: 1.0510411262512207\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 6.61625337600708 | KNN Loss: 5.59387731552124 | BCE Loss: 1.0223760604858398\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 6.681158065795898 | KNN Loss: 5.616794586181641 | BCE Loss: 1.064363718032837\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 6.671107769012451 | KNN Loss: 5.622520446777344 | BCE Loss: 1.0485873222351074\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 6.630137920379639 | KNN Loss: 5.595301151275635 | BCE Loss: 1.034836769104004\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 6.708030700683594 | KNN Loss: 5.635929107666016 | BCE Loss: 1.0721018314361572\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 6.674252986907959 | KNN Loss: 5.5971221923828125 | BCE Loss: 1.0771307945251465\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 6.744387149810791 | KNN Loss: 5.687745094299316 | BCE Loss: 1.0566420555114746\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 6.669257640838623 | KNN Loss: 5.62839412689209 | BCE Loss: 1.0408636331558228\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 6.669373512268066 | KNN Loss: 5.594456195831299 | BCE Loss: 1.0749174356460571\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 6.66240119934082 | KNN Loss: 5.6121602058410645 | BCE Loss: 1.050241231918335\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 6.681219577789307 | KNN Loss: 5.6515069007873535 | BCE Loss: 1.0297126770019531\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 6.715252876281738 | KNN Loss: 5.6366400718688965 | BCE Loss: 1.0786125659942627\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 6.686704158782959 | KNN Loss: 5.611804485321045 | BCE Loss: 1.0748997926712036\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 6.673335075378418 | KNN Loss: 5.62245512008667 | BCE Loss: 1.050879955291748\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 6.652651786804199 | KNN Loss: 5.599400997161865 | BCE Loss: 1.0532506704330444\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 6.639517784118652 | KNN Loss: 5.618954181671143 | BCE Loss: 1.0205638408660889\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 6.68631649017334 | KNN Loss: 5.644069194793701 | BCE Loss: 1.0422471761703491\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 6.703829765319824 | KNN Loss: 5.6555938720703125 | BCE Loss: 1.0482358932495117\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 6.710392951965332 | KNN Loss: 5.6446027755737305 | BCE Loss: 1.0657899379730225\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 6.696866989135742 | KNN Loss: 5.614432334899902 | BCE Loss: 1.0824344158172607\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 6.626304626464844 | KNN Loss: 5.591840744018555 | BCE Loss: 1.03446364402771\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 6.702548980712891 | KNN Loss: 5.654296875 | BCE Loss: 1.0482521057128906\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 6.685189723968506 | KNN Loss: 5.665465354919434 | BCE Loss: 1.0197243690490723\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 6.687721252441406 | KNN Loss: 5.646626949310303 | BCE Loss: 1.0410945415496826\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 6.660107612609863 | KNN Loss: 5.597391605377197 | BCE Loss: 1.0627162456512451\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 6.653419494628906 | KNN Loss: 5.618408203125 | BCE Loss: 1.0350110530853271\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 6.660910606384277 | KNN Loss: 5.629239559173584 | BCE Loss: 1.0316709280014038\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 6.719187259674072 | KNN Loss: 5.675233364105225 | BCE Loss: 1.0439538955688477\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 6.6272501945495605 | KNN Loss: 5.59719181060791 | BCE Loss: 1.0300583839416504\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 6.658141136169434 | KNN Loss: 5.597995281219482 | BCE Loss: 1.060145616531372\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 6.692800998687744 | KNN Loss: 5.614858627319336 | BCE Loss: 1.0779423713684082\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 6.795137405395508 | KNN Loss: 5.748263835906982 | BCE Loss: 1.0468738079071045\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 6.703062057495117 | KNN Loss: 5.664112567901611 | BCE Loss: 1.038949728012085\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 6.655134201049805 | KNN Loss: 5.606433868408203 | BCE Loss: 1.0487003326416016\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 6.637073516845703 | KNN Loss: 5.595717906951904 | BCE Loss: 1.0413556098937988\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 6.712013244628906 | KNN Loss: 5.682575225830078 | BCE Loss: 1.029437780380249\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 6.642242431640625 | KNN Loss: 5.587718963623047 | BCE Loss: 1.0545233488082886\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 6.665714263916016 | KNN Loss: 5.596179485321045 | BCE Loss: 1.0695350170135498\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 6.669924736022949 | KNN Loss: 5.61673641204834 | BCE Loss: 1.0531880855560303\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 6.706765174865723 | KNN Loss: 5.663556098937988 | BCE Loss: 1.0432089567184448\n",
      "Epoch   372: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 6.6460700035095215 | KNN Loss: 5.6176910400390625 | BCE Loss: 1.0283790826797485\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 6.749819755554199 | KNN Loss: 5.6727166175842285 | BCE Loss: 1.0771028995513916\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 6.703553199768066 | KNN Loss: 5.6678385734558105 | BCE Loss: 1.035714864730835\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 6.693122863769531 | KNN Loss: 5.6131134033203125 | BCE Loss: 1.0800092220306396\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 6.703190803527832 | KNN Loss: 5.66510009765625 | BCE Loss: 1.0380909442901611\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 6.661281585693359 | KNN Loss: 5.598499298095703 | BCE Loss: 1.0627820491790771\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 6.695895195007324 | KNN Loss: 5.644951820373535 | BCE Loss: 1.0509436130523682\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 6.6667160987854 | KNN Loss: 5.637659072875977 | BCE Loss: 1.0290570259094238\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 6.667909145355225 | KNN Loss: 5.610504150390625 | BCE Loss: 1.0574051141738892\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 6.663041591644287 | KNN Loss: 5.60493803024292 | BCE Loss: 1.0581034421920776\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 6.650472640991211 | KNN Loss: 5.631227970123291 | BCE Loss: 1.0192444324493408\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 6.673365592956543 | KNN Loss: 5.612085342407227 | BCE Loss: 1.0612802505493164\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 6.62322998046875 | KNN Loss: 5.6034746170043945 | BCE Loss: 1.0197551250457764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 6.652017116546631 | KNN Loss: 5.598711967468262 | BCE Loss: 1.0533052682876587\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 6.752880096435547 | KNN Loss: 5.684060573577881 | BCE Loss: 1.068819284439087\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 6.690841197967529 | KNN Loss: 5.640128135681152 | BCE Loss: 1.050713062286377\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 6.649293422698975 | KNN Loss: 5.597191333770752 | BCE Loss: 1.0521020889282227\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 6.703794956207275 | KNN Loss: 5.662990093231201 | BCE Loss: 1.0408047437667847\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 6.639779090881348 | KNN Loss: 5.619815349578857 | BCE Loss: 1.0199635028839111\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 6.6576056480407715 | KNN Loss: 5.597208023071289 | BCE Loss: 1.060397744178772\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 6.750003814697266 | KNN Loss: 5.682858943939209 | BCE Loss: 1.0671451091766357\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 6.676051616668701 | KNN Loss: 5.612072467803955 | BCE Loss: 1.063979148864746\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 6.685969829559326 | KNN Loss: 5.615054607391357 | BCE Loss: 1.0709152221679688\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 6.682109355926514 | KNN Loss: 5.655039310455322 | BCE Loss: 1.0270700454711914\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 6.720731735229492 | KNN Loss: 5.66322660446167 | BCE Loss: 1.0575053691864014\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 6.756964206695557 | KNN Loss: 5.715360164642334 | BCE Loss: 1.0416041612625122\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 6.632020950317383 | KNN Loss: 5.611608028411865 | BCE Loss: 1.0204126834869385\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 6.704848766326904 | KNN Loss: 5.6429901123046875 | BCE Loss: 1.0618585348129272\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 6.6509175300598145 | KNN Loss: 5.625041484832764 | BCE Loss: 1.0258761644363403\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 6.7129716873168945 | KNN Loss: 5.675588130950928 | BCE Loss: 1.0373833179473877\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 6.664219856262207 | KNN Loss: 5.605432510375977 | BCE Loss: 1.0587875843048096\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 6.681465148925781 | KNN Loss: 5.643637180328369 | BCE Loss: 1.0378278493881226\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 6.622901916503906 | KNN Loss: 5.596829891204834 | BCE Loss: 1.0260722637176514\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 6.657071590423584 | KNN Loss: 5.613243579864502 | BCE Loss: 1.0438278913497925\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 6.658004283905029 | KNN Loss: 5.597141742706299 | BCE Loss: 1.06086266040802\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 6.718287944793701 | KNN Loss: 5.698434829711914 | BCE Loss: 1.0198529958724976\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 6.611390113830566 | KNN Loss: 5.60150671005249 | BCE Loss: 1.0098835229873657\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 6.661133766174316 | KNN Loss: 5.621949672698975 | BCE Loss: 1.039184331893921\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 6.657225608825684 | KNN Loss: 5.596397876739502 | BCE Loss: 1.0608277320861816\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 6.745512962341309 | KNN Loss: 5.66180944442749 | BCE Loss: 1.0837035179138184\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 6.663754940032959 | KNN Loss: 5.607615947723389 | BCE Loss: 1.0561389923095703\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 6.643582344055176 | KNN Loss: 5.603082180023193 | BCE Loss: 1.0404999256134033\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 6.772604465484619 | KNN Loss: 5.748316287994385 | BCE Loss: 1.0242880582809448\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 6.717473983764648 | KNN Loss: 5.64258337020874 | BCE Loss: 1.074890375137329\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 6.640983581542969 | KNN Loss: 5.608607292175293 | BCE Loss: 1.0323765277862549\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 6.69701623916626 | KNN Loss: 5.661534786224365 | BCE Loss: 1.0354814529418945\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 6.73988676071167 | KNN Loss: 5.699288845062256 | BCE Loss: 1.0405980348587036\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 6.643973350524902 | KNN Loss: 5.594228744506836 | BCE Loss: 1.0497448444366455\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 6.685489654541016 | KNN Loss: 5.629450798034668 | BCE Loss: 1.056038737297058\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 6.716485977172852 | KNN Loss: 5.648199081420898 | BCE Loss: 1.068286657333374\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 6.668778896331787 | KNN Loss: 5.62031364440918 | BCE Loss: 1.0484651327133179\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 6.703735828399658 | KNN Loss: 5.669078350067139 | BCE Loss: 1.03465735912323\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 6.714170932769775 | KNN Loss: 5.648559093475342 | BCE Loss: 1.065611720085144\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 6.667435646057129 | KNN Loss: 5.619149208068848 | BCE Loss: 1.0482861995697021\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 6.651639461517334 | KNN Loss: 5.596737384796143 | BCE Loss: 1.0549020767211914\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 6.728923797607422 | KNN Loss: 5.684206962585449 | BCE Loss: 1.0447168350219727\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 6.666338920593262 | KNN Loss: 5.638766288757324 | BCE Loss: 1.0275728702545166\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 6.7691168785095215 | KNN Loss: 5.718081951141357 | BCE Loss: 1.0510348081588745\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 6.68211555480957 | KNN Loss: 5.609492301940918 | BCE Loss: 1.0726230144500732\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 6.667538642883301 | KNN Loss: 5.628594398498535 | BCE Loss: 1.0389442443847656\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 6.6637959480285645 | KNN Loss: 5.60958194732666 | BCE Loss: 1.0542140007019043\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 6.719845771789551 | KNN Loss: 5.655250549316406 | BCE Loss: 1.0645952224731445\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 6.790127277374268 | KNN Loss: 5.733288764953613 | BCE Loss: 1.0568385124206543\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 6.729063510894775 | KNN Loss: 5.684685707092285 | BCE Loss: 1.0443779230117798\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 6.707733154296875 | KNN Loss: 5.641483306884766 | BCE Loss: 1.0662500858306885\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 6.627964973449707 | KNN Loss: 5.602639198303223 | BCE Loss: 1.0253255367279053\n",
      "Epoch   383: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 6.654873847961426 | KNN Loss: 5.6169538497924805 | BCE Loss: 1.0379201173782349\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 6.62350606918335 | KNN Loss: 5.592121601104736 | BCE Loss: 1.0313843488693237\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 6.684126377105713 | KNN Loss: 5.623211860656738 | BCE Loss: 1.060914397239685\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 6.759000778198242 | KNN Loss: 5.675699710845947 | BCE Loss: 1.0833011865615845\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 6.76397180557251 | KNN Loss: 5.696238040924072 | BCE Loss: 1.0677337646484375\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 6.643563747406006 | KNN Loss: 5.591578960418701 | BCE Loss: 1.0519847869873047\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 6.667315483093262 | KNN Loss: 5.637059688568115 | BCE Loss: 1.0302555561065674\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 6.752341270446777 | KNN Loss: 5.68410587310791 | BCE Loss: 1.0682356357574463\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 6.682241439819336 | KNN Loss: 5.630125999450684 | BCE Loss: 1.0521152019500732\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 6.707929611206055 | KNN Loss: 5.642156600952148 | BCE Loss: 1.0657731294631958\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 6.73034143447876 | KNN Loss: 5.674572944641113 | BCE Loss: 1.055768609046936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 6.6073126792907715 | KNN Loss: 5.604601860046387 | BCE Loss: 1.0027108192443848\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 6.708436965942383 | KNN Loss: 5.645650863647461 | BCE Loss: 1.062786340713501\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 6.681609153747559 | KNN Loss: 5.634986877441406 | BCE Loss: 1.0466221570968628\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 6.784908294677734 | KNN Loss: 5.7243218421936035 | BCE Loss: 1.06058669090271\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 6.7209062576293945 | KNN Loss: 5.648448944091797 | BCE Loss: 1.0724575519561768\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 6.649965286254883 | KNN Loss: 5.621917247772217 | BCE Loss: 1.028047800064087\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 6.666098594665527 | KNN Loss: 5.623793601989746 | BCE Loss: 1.0423049926757812\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 6.6520185470581055 | KNN Loss: 5.636840343475342 | BCE Loss: 1.0151782035827637\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 6.630082130432129 | KNN Loss: 5.594217300415039 | BCE Loss: 1.035865068435669\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 6.679377555847168 | KNN Loss: 5.610389232635498 | BCE Loss: 1.06898832321167\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 6.713834285736084 | KNN Loss: 5.655773162841797 | BCE Loss: 1.058061122894287\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 6.671945571899414 | KNN Loss: 5.61693811416626 | BCE Loss: 1.0550075769424438\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 6.6816606521606445 | KNN Loss: 5.612865924835205 | BCE Loss: 1.0687947273254395\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 6.722050189971924 | KNN Loss: 5.670594215393066 | BCE Loss: 1.0514559745788574\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 6.7115702629089355 | KNN Loss: 5.666834831237793 | BCE Loss: 1.044735312461853\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 6.694048881530762 | KNN Loss: 5.650721073150635 | BCE Loss: 1.043327808380127\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 6.633902549743652 | KNN Loss: 5.599292278289795 | BCE Loss: 1.0346105098724365\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 6.689965724945068 | KNN Loss: 5.623407363891602 | BCE Loss: 1.0665583610534668\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 6.645519733428955 | KNN Loss: 5.631269931793213 | BCE Loss: 1.0142498016357422\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 6.6426544189453125 | KNN Loss: 5.604098796844482 | BCE Loss: 1.038555383682251\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 6.635643005371094 | KNN Loss: 5.598149299621582 | BCE Loss: 1.0374938249588013\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 6.666707515716553 | KNN Loss: 5.637220859527588 | BCE Loss: 1.0294865369796753\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 6.637515068054199 | KNN Loss: 5.601555824279785 | BCE Loss: 1.0359594821929932\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 6.675142765045166 | KNN Loss: 5.608013153076172 | BCE Loss: 1.0671296119689941\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 6.625288009643555 | KNN Loss: 5.591591835021973 | BCE Loss: 1.0336964130401611\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 6.696615219116211 | KNN Loss: 5.655587673187256 | BCE Loss: 1.041027307510376\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 6.659214973449707 | KNN Loss: 5.598955154418945 | BCE Loss: 1.0602600574493408\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 6.670526027679443 | KNN Loss: 5.628640651702881 | BCE Loss: 1.0418853759765625\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 6.67916202545166 | KNN Loss: 5.631863594055176 | BCE Loss: 1.0472981929779053\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 6.683788776397705 | KNN Loss: 5.615894794464111 | BCE Loss: 1.0678938627243042\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 6.653647422790527 | KNN Loss: 5.601642608642578 | BCE Loss: 1.0520045757293701\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 6.745265960693359 | KNN Loss: 5.679659843444824 | BCE Loss: 1.0656063556671143\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 6.628055572509766 | KNN Loss: 5.595717430114746 | BCE Loss: 1.0323381423950195\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 6.691380500793457 | KNN Loss: 5.669931411743164 | BCE Loss: 1.0214488506317139\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 6.682001113891602 | KNN Loss: 5.614916801452637 | BCE Loss: 1.0670840740203857\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 6.744887351989746 | KNN Loss: 5.713564395904541 | BCE Loss: 1.0313231945037842\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 6.744771957397461 | KNN Loss: 5.667478561401367 | BCE Loss: 1.0772936344146729\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 6.6716485023498535 | KNN Loss: 5.601288318634033 | BCE Loss: 1.0703601837158203\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 6.742918014526367 | KNN Loss: 5.704578876495361 | BCE Loss: 1.038339376449585\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 6.690222263336182 | KNN Loss: 5.632707595825195 | BCE Loss: 1.0575146675109863\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 6.648122787475586 | KNN Loss: 5.612109661102295 | BCE Loss: 1.0360132455825806\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 6.66706657409668 | KNN Loss: 5.629868507385254 | BCE Loss: 1.0371979475021362\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 6.645156383514404 | KNN Loss: 5.592362403869629 | BCE Loss: 1.0527938604354858\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 6.690494537353516 | KNN Loss: 5.636764049530029 | BCE Loss: 1.0537307262420654\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 6.709625244140625 | KNN Loss: 5.627915859222412 | BCE Loss: 1.081709623336792\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 6.712685585021973 | KNN Loss: 5.654475212097168 | BCE Loss: 1.0582103729248047\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 6.663391590118408 | KNN Loss: 5.615965366363525 | BCE Loss: 1.0474262237548828\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 6.693120956420898 | KNN Loss: 5.639270305633545 | BCE Loss: 1.053850769996643\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 6.703639030456543 | KNN Loss: 5.6662116050720215 | BCE Loss: 1.037427306175232\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 6.699235439300537 | KNN Loss: 5.625218868255615 | BCE Loss: 1.0740165710449219\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 6.666350364685059 | KNN Loss: 5.619298934936523 | BCE Loss: 1.0470513105392456\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 6.632335186004639 | KNN Loss: 5.597938060760498 | BCE Loss: 1.034397006034851\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 6.734112739562988 | KNN Loss: 5.680746555328369 | BCE Loss: 1.0533661842346191\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 6.667780876159668 | KNN Loss: 5.610596656799316 | BCE Loss: 1.0571844577789307\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 6.669127941131592 | KNN Loss: 5.633399486541748 | BCE Loss: 1.0357283353805542\n",
      "Epoch   394: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 6.674540042877197 | KNN Loss: 5.636200904846191 | BCE Loss: 1.0383390188217163\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 6.634649276733398 | KNN Loss: 5.617263317108154 | BCE Loss: 1.0173860788345337\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 6.637714385986328 | KNN Loss: 5.593119144439697 | BCE Loss: 1.0445952415466309\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 6.6396050453186035 | KNN Loss: 5.592532157897949 | BCE Loss: 1.0470727682113647\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 6.685739517211914 | KNN Loss: 5.63969087600708 | BCE Loss: 1.0460484027862549\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 6.6521453857421875 | KNN Loss: 5.622666835784912 | BCE Loss: 1.029478669166565\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 6.716564178466797 | KNN Loss: 5.652379989624023 | BCE Loss: 1.0641844272613525\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 6.6189422607421875 | KNN Loss: 5.6007537841796875 | BCE Loss: 1.0181884765625\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 6.630707740783691 | KNN Loss: 5.604414463043213 | BCE Loss: 1.026293396949768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 6.664648056030273 | KNN Loss: 5.601632118225098 | BCE Loss: 1.0630161762237549\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 6.735088348388672 | KNN Loss: 5.659753322601318 | BCE Loss: 1.0753347873687744\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 6.685731410980225 | KNN Loss: 5.635324954986572 | BCE Loss: 1.0504064559936523\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 6.645852565765381 | KNN Loss: 5.64146614074707 | BCE Loss: 1.0043865442276\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 6.66829252243042 | KNN Loss: 5.60400915145874 | BCE Loss: 1.0642833709716797\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 6.7177276611328125 | KNN Loss: 5.633070468902588 | BCE Loss: 1.0846569538116455\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 6.699888706207275 | KNN Loss: 5.634925842285156 | BCE Loss: 1.0649627447128296\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 6.615666389465332 | KNN Loss: 5.592231273651123 | BCE Loss: 1.023435354232788\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 6.651393890380859 | KNN Loss: 5.598862171173096 | BCE Loss: 1.0525314807891846\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 6.765689849853516 | KNN Loss: 5.699488162994385 | BCE Loss: 1.0662014484405518\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 6.665740489959717 | KNN Loss: 5.625022888183594 | BCE Loss: 1.0407174825668335\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 6.657607078552246 | KNN Loss: 5.62623405456543 | BCE Loss: 1.0313727855682373\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 6.619741439819336 | KNN Loss: 5.592527866363525 | BCE Loss: 1.0272133350372314\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 6.658266067504883 | KNN Loss: 5.634552478790283 | BCE Loss: 1.02371346950531\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 6.73159122467041 | KNN Loss: 5.68842887878418 | BCE Loss: 1.0431623458862305\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 6.679122447967529 | KNN Loss: 5.603903770446777 | BCE Loss: 1.075218677520752\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 6.650636672973633 | KNN Loss: 5.627260208129883 | BCE Loss: 1.023376226425171\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 6.675345420837402 | KNN Loss: 5.621974945068359 | BCE Loss: 1.0533702373504639\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 6.688059329986572 | KNN Loss: 5.626136779785156 | BCE Loss: 1.0619224309921265\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 6.638097286224365 | KNN Loss: 5.598120212554932 | BCE Loss: 1.0399770736694336\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 6.651674270629883 | KNN Loss: 5.608133792877197 | BCE Loss: 1.0435404777526855\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 6.632613182067871 | KNN Loss: 5.596465110778809 | BCE Loss: 1.036148190498352\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 6.6419758796691895 | KNN Loss: 5.595029830932617 | BCE Loss: 1.0469460487365723\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 6.740359306335449 | KNN Loss: 5.655849456787109 | BCE Loss: 1.0845098495483398\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 6.70979118347168 | KNN Loss: 5.65230131149292 | BCE Loss: 1.0574896335601807\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 6.699878215789795 | KNN Loss: 5.672837257385254 | BCE Loss: 1.027040958404541\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 6.654042720794678 | KNN Loss: 5.609679698944092 | BCE Loss: 1.044363021850586\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 6.6748456954956055 | KNN Loss: 5.620927810668945 | BCE Loss: 1.053917646408081\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 6.706971168518066 | KNN Loss: 5.626704692840576 | BCE Loss: 1.0802664756774902\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 6.654532432556152 | KNN Loss: 5.599981307983398 | BCE Loss: 1.054551124572754\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 6.671012878417969 | KNN Loss: 5.6103339195251465 | BCE Loss: 1.0606789588928223\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 6.653599739074707 | KNN Loss: 5.599309921264648 | BCE Loss: 1.0542895793914795\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 6.691052436828613 | KNN Loss: 5.646066188812256 | BCE Loss: 1.0449862480163574\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 6.652227878570557 | KNN Loss: 5.611209869384766 | BCE Loss: 1.041018009185791\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 6.704257011413574 | KNN Loss: 5.625185966491699 | BCE Loss: 1.079071044921875\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 6.636155128479004 | KNN Loss: 5.599492073059082 | BCE Loss: 1.0366630554199219\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 6.674945831298828 | KNN Loss: 5.638065338134766 | BCE Loss: 1.036880373954773\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 6.6761884689331055 | KNN Loss: 5.621294975280762 | BCE Loss: 1.0548932552337646\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 6.654170036315918 | KNN Loss: 5.6276726722717285 | BCE Loss: 1.0264976024627686\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 6.698248863220215 | KNN Loss: 5.638751029968262 | BCE Loss: 1.0594979524612427\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 6.641991138458252 | KNN Loss: 5.615440845489502 | BCE Loss: 1.02655029296875\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 6.760124206542969 | KNN Loss: 5.6613593101501465 | BCE Loss: 1.0987646579742432\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 6.647682189941406 | KNN Loss: 5.596913814544678 | BCE Loss: 1.0507686138153076\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 6.6419219970703125 | KNN Loss: 5.594221591949463 | BCE Loss: 1.0477006435394287\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 6.612484455108643 | KNN Loss: 5.600282669067383 | BCE Loss: 1.0122017860412598\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 6.676674842834473 | KNN Loss: 5.616532802581787 | BCE Loss: 1.0601418018341064\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 6.73252010345459 | KNN Loss: 5.669006824493408 | BCE Loss: 1.0635135173797607\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 6.668880939483643 | KNN Loss: 5.615087985992432 | BCE Loss: 1.053792953491211\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 6.6816558837890625 | KNN Loss: 5.640637397766113 | BCE Loss: 1.0410182476043701\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 6.703766822814941 | KNN Loss: 5.6949462890625 | BCE Loss: 1.008820652961731\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 6.710001468658447 | KNN Loss: 5.662196159362793 | BCE Loss: 1.0478051900863647\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 6.709647178649902 | KNN Loss: 5.683412551879883 | BCE Loss: 1.0262348651885986\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 6.660274982452393 | KNN Loss: 5.620107650756836 | BCE Loss: 1.040167212486267\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 6.713457107543945 | KNN Loss: 5.665046691894531 | BCE Loss: 1.048410177230835\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 6.675315856933594 | KNN Loss: 5.622628211975098 | BCE Loss: 1.052687644958496\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 6.633533954620361 | KNN Loss: 5.60552453994751 | BCE Loss: 1.0280094146728516\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 6.6508002281188965 | KNN Loss: 5.6058526039123535 | BCE Loss: 1.0449477434158325\n",
      "Epoch   405: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 6.601719379425049 | KNN Loss: 5.5992326736450195 | BCE Loss: 1.0024868249893188\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 6.6697869300842285 | KNN Loss: 5.632606506347656 | BCE Loss: 1.0371803045272827\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 6.699190616607666 | KNN Loss: 5.655299663543701 | BCE Loss: 1.0438908338546753\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 6.687713623046875 | KNN Loss: 5.646937370300293 | BCE Loss: 1.0407761335372925\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 6.689090251922607 | KNN Loss: 5.614646911621094 | BCE Loss: 1.0744433403015137\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 6.640868186950684 | KNN Loss: 5.594352722167969 | BCE Loss: 1.0465152263641357\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 6.677053928375244 | KNN Loss: 5.6300530433654785 | BCE Loss: 1.0470008850097656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 6.66158390045166 | KNN Loss: 5.600961685180664 | BCE Loss: 1.060621976852417\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 6.678431510925293 | KNN Loss: 5.6140546798706055 | BCE Loss: 1.064376950263977\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 6.646390914916992 | KNN Loss: 5.597099304199219 | BCE Loss: 1.0492918491363525\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 6.648588180541992 | KNN Loss: 5.602494239807129 | BCE Loss: 1.0460937023162842\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 6.659420490264893 | KNN Loss: 5.5979838371276855 | BCE Loss: 1.061436653137207\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 6.691220283508301 | KNN Loss: 5.651219367980957 | BCE Loss: 1.0400011539459229\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 6.658292293548584 | KNN Loss: 5.615538597106934 | BCE Loss: 1.0427536964416504\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 6.682666778564453 | KNN Loss: 5.622068405151367 | BCE Loss: 1.060598373413086\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 6.721179008483887 | KNN Loss: 5.6750664710998535 | BCE Loss: 1.0461125373840332\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 6.7097368240356445 | KNN Loss: 5.648505210876465 | BCE Loss: 1.0612318515777588\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 6.694424629211426 | KNN Loss: 5.615363597869873 | BCE Loss: 1.0790612697601318\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 6.711170196533203 | KNN Loss: 5.652915000915527 | BCE Loss: 1.0582554340362549\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 6.624973297119141 | KNN Loss: 5.594061374664307 | BCE Loss: 1.0309118032455444\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 6.673749923706055 | KNN Loss: 5.613497734069824 | BCE Loss: 1.06025230884552\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 6.693068504333496 | KNN Loss: 5.635779857635498 | BCE Loss: 1.0572887659072876\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 6.742388725280762 | KNN Loss: 5.6888813972473145 | BCE Loss: 1.0535074472427368\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 6.780937194824219 | KNN Loss: 5.699579238891602 | BCE Loss: 1.081357717514038\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 6.655703544616699 | KNN Loss: 5.597049713134766 | BCE Loss: 1.058653712272644\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 6.672998905181885 | KNN Loss: 5.5967864990234375 | BCE Loss: 1.0762124061584473\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 6.635396957397461 | KNN Loss: 5.599541664123535 | BCE Loss: 1.0358552932739258\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 6.670774936676025 | KNN Loss: 5.632062911987305 | BCE Loss: 1.0387119054794312\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 6.714676856994629 | KNN Loss: 5.6633758544921875 | BCE Loss: 1.0513008832931519\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 6.666955947875977 | KNN Loss: 5.658924579620361 | BCE Loss: 1.0080314874649048\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 6.709776878356934 | KNN Loss: 5.667142868041992 | BCE Loss: 1.0426337718963623\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 6.635059833526611 | KNN Loss: 5.606574058532715 | BCE Loss: 1.0284857749938965\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 6.662386894226074 | KNN Loss: 5.627384662628174 | BCE Loss: 1.0350022315979004\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 6.657711982727051 | KNN Loss: 5.628137588500977 | BCE Loss: 1.0295742750167847\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 6.661288738250732 | KNN Loss: 5.6193623542785645 | BCE Loss: 1.0419265031814575\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 6.702216148376465 | KNN Loss: 5.657862663269043 | BCE Loss: 1.0443532466888428\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 6.663506507873535 | KNN Loss: 5.613186359405518 | BCE Loss: 1.0503199100494385\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 6.656998634338379 | KNN Loss: 5.623958587646484 | BCE Loss: 1.0330398082733154\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 6.694449424743652 | KNN Loss: 5.650684356689453 | BCE Loss: 1.0437650680541992\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 6.676719665527344 | KNN Loss: 5.628687858581543 | BCE Loss: 1.0480320453643799\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 6.6781206130981445 | KNN Loss: 5.651645660400391 | BCE Loss: 1.0264750719070435\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 6.653086185455322 | KNN Loss: 5.608851432800293 | BCE Loss: 1.0442347526550293\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 6.634934425354004 | KNN Loss: 5.600773334503174 | BCE Loss: 1.0341613292694092\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 6.671962738037109 | KNN Loss: 5.654963970184326 | BCE Loss: 1.0169987678527832\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 6.701605796813965 | KNN Loss: 5.652493476867676 | BCE Loss: 1.049112319946289\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 6.6717634201049805 | KNN Loss: 5.625601768493652 | BCE Loss: 1.0461618900299072\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 6.767168045043945 | KNN Loss: 5.716720104217529 | BCE Loss: 1.050447702407837\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 6.67296028137207 | KNN Loss: 5.632652759552002 | BCE Loss: 1.040307641029358\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 6.700206756591797 | KNN Loss: 5.61176872253418 | BCE Loss: 1.0884382724761963\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 6.686081409454346 | KNN Loss: 5.622588157653809 | BCE Loss: 1.0634933710098267\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 6.654305934906006 | KNN Loss: 5.608155250549316 | BCE Loss: 1.0461506843566895\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 6.65252161026001 | KNN Loss: 5.612335205078125 | BCE Loss: 1.0401864051818848\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 6.642208099365234 | KNN Loss: 5.594334602355957 | BCE Loss: 1.047873616218567\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 6.653027057647705 | KNN Loss: 5.600067615509033 | BCE Loss: 1.0529593229293823\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 6.726815223693848 | KNN Loss: 5.684622764587402 | BCE Loss: 1.0421925783157349\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 6.70551872253418 | KNN Loss: 5.65695858001709 | BCE Loss: 1.048560380935669\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 6.613734722137451 | KNN Loss: 5.608076095581055 | BCE Loss: 1.0056586265563965\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 6.664206504821777 | KNN Loss: 5.62542200088501 | BCE Loss: 1.0387847423553467\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 6.664824485778809 | KNN Loss: 5.6095380783081055 | BCE Loss: 1.0552865266799927\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 6.658921718597412 | KNN Loss: 5.592336177825928 | BCE Loss: 1.0665854215621948\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 6.724440097808838 | KNN Loss: 5.6576666831970215 | BCE Loss: 1.066773533821106\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 6.855364799499512 | KNN Loss: 5.794226169586182 | BCE Loss: 1.0611388683319092\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 6.724455833435059 | KNN Loss: 5.674971580505371 | BCE Loss: 1.0494842529296875\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 6.678102016448975 | KNN Loss: 5.635066509246826 | BCE Loss: 1.043035626411438\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 6.645377159118652 | KNN Loss: 5.609236240386963 | BCE Loss: 1.0361409187316895\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 6.640937805175781 | KNN Loss: 5.594302654266357 | BCE Loss: 1.0466350317001343\n",
      "Epoch   416: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 6.703550338745117 | KNN Loss: 5.648698329925537 | BCE Loss: 1.0548522472381592\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 6.69185733795166 | KNN Loss: 5.639392852783203 | BCE Loss: 1.052464246749878\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 6.63784122467041 | KNN Loss: 5.608044147491455 | BCE Loss: 1.029797077178955\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 6.688571929931641 | KNN Loss: 5.630733966827393 | BCE Loss: 1.057837724685669\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 6.659295558929443 | KNN Loss: 5.619068145751953 | BCE Loss: 1.0402274131774902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 6.676491737365723 | KNN Loss: 5.619062423706055 | BCE Loss: 1.0574290752410889\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 6.673384666442871 | KNN Loss: 5.642085552215576 | BCE Loss: 1.031299352645874\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 6.652766227722168 | KNN Loss: 5.604642868041992 | BCE Loss: 1.0481233596801758\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 6.658535003662109 | KNN Loss: 5.612541198730469 | BCE Loss: 1.0459939241409302\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 6.6328864097595215 | KNN Loss: 5.593512535095215 | BCE Loss: 1.0393739938735962\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 6.700678825378418 | KNN Loss: 5.637027263641357 | BCE Loss: 1.063651442527771\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 6.645586013793945 | KNN Loss: 5.613584041595459 | BCE Loss: 1.0320019721984863\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 6.699481010437012 | KNN Loss: 5.65203857421875 | BCE Loss: 1.0474424362182617\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 6.613862037658691 | KNN Loss: 5.589376449584961 | BCE Loss: 1.0244858264923096\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 6.6831793785095215 | KNN Loss: 5.615623950958252 | BCE Loss: 1.06755530834198\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 6.676878452301025 | KNN Loss: 5.654409408569336 | BCE Loss: 1.0224690437316895\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 6.6961798667907715 | KNN Loss: 5.653880596160889 | BCE Loss: 1.0422992706298828\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 6.612710475921631 | KNN Loss: 5.592250823974609 | BCE Loss: 1.0204596519470215\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 6.7075676918029785 | KNN Loss: 5.678376197814941 | BCE Loss: 1.0291916131973267\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 6.679068088531494 | KNN Loss: 5.619269847869873 | BCE Loss: 1.059798240661621\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 6.634589195251465 | KNN Loss: 5.5941548347473145 | BCE Loss: 1.0404343605041504\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 6.687741279602051 | KNN Loss: 5.608700275421143 | BCE Loss: 1.0790408849716187\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 6.637361526489258 | KNN Loss: 5.602717399597168 | BCE Loss: 1.0346438884735107\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 6.669730186462402 | KNN Loss: 5.616624355316162 | BCE Loss: 1.0531058311462402\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 6.666778564453125 | KNN Loss: 5.611480236053467 | BCE Loss: 1.0552985668182373\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 6.712879180908203 | KNN Loss: 5.670835971832275 | BCE Loss: 1.0420429706573486\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 6.654326915740967 | KNN Loss: 5.602390766143799 | BCE Loss: 1.0519360303878784\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 6.627549171447754 | KNN Loss: 5.592325210571289 | BCE Loss: 1.0352239608764648\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 6.660938739776611 | KNN Loss: 5.612936496734619 | BCE Loss: 1.0480022430419922\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 6.763617515563965 | KNN Loss: 5.706121921539307 | BCE Loss: 1.0574958324432373\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 6.693612098693848 | KNN Loss: 5.630891799926758 | BCE Loss: 1.062720537185669\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 6.775570392608643 | KNN Loss: 5.720118522644043 | BCE Loss: 1.0554519891738892\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 6.812992095947266 | KNN Loss: 5.751854419708252 | BCE Loss: 1.0611376762390137\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 6.622341156005859 | KNN Loss: 5.595064640045166 | BCE Loss: 1.0272767543792725\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 6.743680477142334 | KNN Loss: 5.707698345184326 | BCE Loss: 1.0359821319580078\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 6.674013137817383 | KNN Loss: 5.636984348297119 | BCE Loss: 1.0370287895202637\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 6.650061130523682 | KNN Loss: 5.6094255447387695 | BCE Loss: 1.040635585784912\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 6.624805450439453 | KNN Loss: 5.603623390197754 | BCE Loss: 1.0211818218231201\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 6.712553977966309 | KNN Loss: 5.7142415046691895 | BCE Loss: 0.9983127117156982\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 6.650762557983398 | KNN Loss: 5.6082634925842285 | BCE Loss: 1.0424988269805908\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 6.68962287902832 | KNN Loss: 5.636402606964111 | BCE Loss: 1.053220272064209\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 6.6380462646484375 | KNN Loss: 5.594954490661621 | BCE Loss: 1.0430917739868164\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 6.644305229187012 | KNN Loss: 5.598110198974609 | BCE Loss: 1.0461949110031128\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 6.635756969451904 | KNN Loss: 5.603453159332275 | BCE Loss: 1.032303810119629\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 6.716059684753418 | KNN Loss: 5.660757064819336 | BCE Loss: 1.0553028583526611\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 6.6815690994262695 | KNN Loss: 5.613661766052246 | BCE Loss: 1.0679070949554443\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 6.657658576965332 | KNN Loss: 5.623831748962402 | BCE Loss: 1.0338265895843506\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 6.701691627502441 | KNN Loss: 5.65291690826416 | BCE Loss: 1.0487747192382812\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 6.628076553344727 | KNN Loss: 5.597212791442871 | BCE Loss: 1.0308635234832764\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 6.656734466552734 | KNN Loss: 5.59780740737915 | BCE Loss: 1.0589268207550049\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 6.648290157318115 | KNN Loss: 5.602290153503418 | BCE Loss: 1.0460000038146973\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 6.64166259765625 | KNN Loss: 5.599202632904053 | BCE Loss: 1.0424597263336182\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 6.634557723999023 | KNN Loss: 5.602081298828125 | BCE Loss: 1.0324761867523193\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 6.69247579574585 | KNN Loss: 5.638201713562012 | BCE Loss: 1.054274082183838\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 6.650032997131348 | KNN Loss: 5.621570110321045 | BCE Loss: 1.0284628868103027\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 6.696746349334717 | KNN Loss: 5.645760536193848 | BCE Loss: 1.0509859323501587\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 6.689170837402344 | KNN Loss: 5.627784729003906 | BCE Loss: 1.0613858699798584\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 6.674295425415039 | KNN Loss: 5.617753028869629 | BCE Loss: 1.056542158126831\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 6.6691765785217285 | KNN Loss: 5.6168951988220215 | BCE Loss: 1.0522812604904175\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 6.638418674468994 | KNN Loss: 5.608945369720459 | BCE Loss: 1.0294734239578247\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 6.642777442932129 | KNN Loss: 5.619117259979248 | BCE Loss: 1.0236601829528809\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 6.608614921569824 | KNN Loss: 5.593014717102051 | BCE Loss: 1.0156002044677734\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 6.701094150543213 | KNN Loss: 5.676703929901123 | BCE Loss: 1.0243903398513794\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 6.717280387878418 | KNN Loss: 5.65130615234375 | BCE Loss: 1.0659739971160889\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 6.674173355102539 | KNN Loss: 5.626156806945801 | BCE Loss: 1.0480163097381592\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 6.629389762878418 | KNN Loss: 5.596220970153809 | BCE Loss: 1.0331690311431885\n",
      "Epoch   427: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 6.618026256561279 | KNN Loss: 5.594293594360352 | BCE Loss: 1.0237327814102173\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 6.788670539855957 | KNN Loss: 5.742307186126709 | BCE Loss: 1.0463635921478271\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 6.711291790008545 | KNN Loss: 5.631022930145264 | BCE Loss: 1.0802688598632812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 6.667026519775391 | KNN Loss: 5.599555015563965 | BCE Loss: 1.0674715042114258\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 6.639980316162109 | KNN Loss: 5.5939531326293945 | BCE Loss: 1.0460271835327148\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 6.638461112976074 | KNN Loss: 5.618919372558594 | BCE Loss: 1.01954185962677\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 6.645467281341553 | KNN Loss: 5.609829425811768 | BCE Loss: 1.0356378555297852\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 6.8503851890563965 | KNN Loss: 5.8300909996032715 | BCE Loss: 1.020294189453125\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 6.652978897094727 | KNN Loss: 5.600870609283447 | BCE Loss: 1.0521084070205688\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 6.624970436096191 | KNN Loss: 5.5981831550598145 | BCE Loss: 1.026787519454956\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 6.645907402038574 | KNN Loss: 5.614967346191406 | BCE Loss: 1.030940055847168\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 6.682102680206299 | KNN Loss: 5.629920959472656 | BCE Loss: 1.0521817207336426\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 6.679204940795898 | KNN Loss: 5.635208606719971 | BCE Loss: 1.0439963340759277\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 6.647904396057129 | KNN Loss: 5.594803333282471 | BCE Loss: 1.053100824356079\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 6.663073539733887 | KNN Loss: 5.605264663696289 | BCE Loss: 1.0578086376190186\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 6.691054344177246 | KNN Loss: 5.6372785568237305 | BCE Loss: 1.0537759065628052\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 6.650005340576172 | KNN Loss: 5.607243061065674 | BCE Loss: 1.042762041091919\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 6.654809951782227 | KNN Loss: 5.638513565063477 | BCE Loss: 1.01629638671875\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 6.601883411407471 | KNN Loss: 5.588898658752441 | BCE Loss: 1.0129846334457397\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 6.650125026702881 | KNN Loss: 5.619350910186768 | BCE Loss: 1.0307742357254028\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 6.66185188293457 | KNN Loss: 5.61805534362793 | BCE Loss: 1.0437967777252197\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 6.665511131286621 | KNN Loss: 5.610853672027588 | BCE Loss: 1.0546576976776123\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 6.7164306640625 | KNN Loss: 5.632793426513672 | BCE Loss: 1.0836372375488281\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 6.8202080726623535 | KNN Loss: 5.762852668762207 | BCE Loss: 1.057355284690857\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 6.677833080291748 | KNN Loss: 5.629968166351318 | BCE Loss: 1.0478650331497192\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 6.653659820556641 | KNN Loss: 5.598577499389648 | BCE Loss: 1.0550825595855713\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 6.662351608276367 | KNN Loss: 5.647068977355957 | BCE Loss: 1.015282392501831\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 6.662380218505859 | KNN Loss: 5.603825092315674 | BCE Loss: 1.0585548877716064\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 6.621468544006348 | KNN Loss: 5.599143981933594 | BCE Loss: 1.0223243236541748\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 6.647367477416992 | KNN Loss: 5.600143909454346 | BCE Loss: 1.0472233295440674\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 6.670130729675293 | KNN Loss: 5.617812156677246 | BCE Loss: 1.0523185729980469\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 6.636477947235107 | KNN Loss: 5.613603591918945 | BCE Loss: 1.0228742361068726\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 6.7096171379089355 | KNN Loss: 5.661227703094482 | BCE Loss: 1.0483895540237427\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 6.6289215087890625 | KNN Loss: 5.602046012878418 | BCE Loss: 1.0268757343292236\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 6.679521560668945 | KNN Loss: 5.611362457275391 | BCE Loss: 1.0681592226028442\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 6.6325225830078125 | KNN Loss: 5.594064235687256 | BCE Loss: 1.0384583473205566\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 6.643624782562256 | KNN Loss: 5.626645565032959 | BCE Loss: 1.0169792175292969\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 6.696608066558838 | KNN Loss: 5.649499416351318 | BCE Loss: 1.0471086502075195\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 6.684086799621582 | KNN Loss: 5.613285064697266 | BCE Loss: 1.0708016157150269\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 6.698412895202637 | KNN Loss: 5.650229454040527 | BCE Loss: 1.048183560371399\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 6.754703521728516 | KNN Loss: 5.692196846008301 | BCE Loss: 1.0625066757202148\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 6.686426639556885 | KNN Loss: 5.640303611755371 | BCE Loss: 1.0461230278015137\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 6.684412479400635 | KNN Loss: 5.620611667633057 | BCE Loss: 1.0638006925582886\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 6.737398147583008 | KNN Loss: 5.677844524383545 | BCE Loss: 1.0595537424087524\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 6.646036624908447 | KNN Loss: 5.60527229309082 | BCE Loss: 1.040764331817627\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 6.7218475341796875 | KNN Loss: 5.675854682922363 | BCE Loss: 1.0459926128387451\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 6.692292213439941 | KNN Loss: 5.652029991149902 | BCE Loss: 1.0402624607086182\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 6.711393356323242 | KNN Loss: 5.651232719421387 | BCE Loss: 1.0601608753204346\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 6.716206073760986 | KNN Loss: 5.674015998840332 | BCE Loss: 1.0421901941299438\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 6.665284156799316 | KNN Loss: 5.613497257232666 | BCE Loss: 1.0517871379852295\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 6.6459503173828125 | KNN Loss: 5.605248928070068 | BCE Loss: 1.040701150894165\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 6.6455078125 | KNN Loss: 5.618052005767822 | BCE Loss: 1.0274558067321777\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 6.694234848022461 | KNN Loss: 5.613110542297363 | BCE Loss: 1.0811245441436768\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 6.698014259338379 | KNN Loss: 5.651839733123779 | BCE Loss: 1.0461747646331787\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 6.672279357910156 | KNN Loss: 5.598326683044434 | BCE Loss: 1.0739524364471436\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 6.705878734588623 | KNN Loss: 5.641719818115234 | BCE Loss: 1.0641590356826782\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 6.722357273101807 | KNN Loss: 5.6928324699401855 | BCE Loss: 1.029524803161621\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 6.664064407348633 | KNN Loss: 5.602230072021484 | BCE Loss: 1.0618345737457275\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 6.706195831298828 | KNN Loss: 5.666915416717529 | BCE Loss: 1.0392804145812988\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 6.644153118133545 | KNN Loss: 5.6205878257751465 | BCE Loss: 1.023565411567688\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 6.617671012878418 | KNN Loss: 5.605603218078613 | BCE Loss: 1.0120679140090942\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 6.670958042144775 | KNN Loss: 5.644439697265625 | BCE Loss: 1.0265183448791504\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 6.706472396850586 | KNN Loss: 5.651856422424316 | BCE Loss: 1.0546159744262695\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 6.655526161193848 | KNN Loss: 5.6198039054870605 | BCE Loss: 1.035722017288208\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 6.673149108886719 | KNN Loss: 5.647063732147217 | BCE Loss: 1.026085615158081\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 6.6647772789001465 | KNN Loss: 5.5922393798828125 | BCE Loss: 1.0725380182266235\n",
      "Epoch   438: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 6.664324760437012 | KNN Loss: 5.601932048797607 | BCE Loss: 1.0623925924301147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 6.674930095672607 | KNN Loss: 5.633243560791016 | BCE Loss: 1.0416865348815918\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 6.750466823577881 | KNN Loss: 5.698458194732666 | BCE Loss: 1.0520086288452148\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 6.657831192016602 | KNN Loss: 5.606848239898682 | BCE Loss: 1.0509827136993408\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 6.655025959014893 | KNN Loss: 5.593140602111816 | BCE Loss: 1.0618853569030762\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 6.741613388061523 | KNN Loss: 5.717184066772461 | BCE Loss: 1.0244293212890625\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 6.710047245025635 | KNN Loss: 5.659897327423096 | BCE Loss: 1.0501497983932495\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 6.647607803344727 | KNN Loss: 5.5958075523376465 | BCE Loss: 1.05180025100708\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 6.679595947265625 | KNN Loss: 5.613131523132324 | BCE Loss: 1.0664644241333008\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 6.671942234039307 | KNN Loss: 5.610418796539307 | BCE Loss: 1.0615234375\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 6.681973934173584 | KNN Loss: 5.649031639099121 | BCE Loss: 1.032942295074463\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 6.736327648162842 | KNN Loss: 5.67103910446167 | BCE Loss: 1.0652886629104614\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 6.664546012878418 | KNN Loss: 5.610240936279297 | BCE Loss: 1.054304838180542\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 6.6301703453063965 | KNN Loss: 5.601112365722656 | BCE Loss: 1.0290579795837402\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 6.704444885253906 | KNN Loss: 5.64511251449585 | BCE Loss: 1.0593324899673462\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 6.6394500732421875 | KNN Loss: 5.609961986541748 | BCE Loss: 1.0294878482818604\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 6.655588150024414 | KNN Loss: 5.603179454803467 | BCE Loss: 1.0524084568023682\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 6.722502708435059 | KNN Loss: 5.6777448654174805 | BCE Loss: 1.0447577238082886\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 6.6616411209106445 | KNN Loss: 5.6156086921691895 | BCE Loss: 1.0460323095321655\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 6.636240482330322 | KNN Loss: 5.613020420074463 | BCE Loss: 1.023220181465149\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 6.647219181060791 | KNN Loss: 5.606655597686768 | BCE Loss: 1.0405635833740234\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 6.7227678298950195 | KNN Loss: 5.6816887855529785 | BCE Loss: 1.041078805923462\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 6.681163787841797 | KNN Loss: 5.650951862335205 | BCE Loss: 1.030212163925171\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 6.650697231292725 | KNN Loss: 5.610794544219971 | BCE Loss: 1.039902687072754\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 6.669061660766602 | KNN Loss: 5.593954563140869 | BCE Loss: 1.0751068592071533\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 6.649984359741211 | KNN Loss: 5.594393253326416 | BCE Loss: 1.055591344833374\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 6.671951770782471 | KNN Loss: 5.597121715545654 | BCE Loss: 1.0748300552368164\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 6.639338493347168 | KNN Loss: 5.595219135284424 | BCE Loss: 1.0441194772720337\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 6.708185195922852 | KNN Loss: 5.65221643447876 | BCE Loss: 1.0559686422348022\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 6.677525043487549 | KNN Loss: 5.641906261444092 | BCE Loss: 1.035618782043457\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 6.704677581787109 | KNN Loss: 5.657745838165283 | BCE Loss: 1.046931505203247\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 6.637759685516357 | KNN Loss: 5.599200248718262 | BCE Loss: 1.0385595560073853\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 6.635262489318848 | KNN Loss: 5.602321147918701 | BCE Loss: 1.0329413414001465\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 6.689915657043457 | KNN Loss: 5.658923625946045 | BCE Loss: 1.0309922695159912\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 6.666656017303467 | KNN Loss: 5.6360182762146 | BCE Loss: 1.0306377410888672\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 6.66390323638916 | KNN Loss: 5.612285614013672 | BCE Loss: 1.0516176223754883\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 6.781052589416504 | KNN Loss: 5.736769676208496 | BCE Loss: 1.044283151626587\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 6.665622234344482 | KNN Loss: 5.592164993286133 | BCE Loss: 1.07345712184906\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 6.694637298583984 | KNN Loss: 5.627475261688232 | BCE Loss: 1.067162036895752\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 6.747345924377441 | KNN Loss: 5.709132194519043 | BCE Loss: 1.0382137298583984\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 6.671639442443848 | KNN Loss: 5.626149654388428 | BCE Loss: 1.0454895496368408\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 6.675058364868164 | KNN Loss: 5.655498504638672 | BCE Loss: 1.0195600986480713\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 6.697888374328613 | KNN Loss: 5.64654541015625 | BCE Loss: 1.0513427257537842\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 6.652350902557373 | KNN Loss: 5.601129055023193 | BCE Loss: 1.0512217283248901\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 6.62396240234375 | KNN Loss: 5.597971439361572 | BCE Loss: 1.0259907245635986\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 6.625646591186523 | KNN Loss: 5.606332302093506 | BCE Loss: 1.0193140506744385\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 6.671362400054932 | KNN Loss: 5.622431755065918 | BCE Loss: 1.0489306449890137\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 6.6377034187316895 | KNN Loss: 5.597439289093018 | BCE Loss: 1.0402642488479614\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 6.697915554046631 | KNN Loss: 5.646347999572754 | BCE Loss: 1.051567554473877\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 6.731446266174316 | KNN Loss: 5.685622692108154 | BCE Loss: 1.045823335647583\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 6.699596881866455 | KNN Loss: 5.677806377410889 | BCE Loss: 1.0217905044555664\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 6.649791717529297 | KNN Loss: 5.601504325866699 | BCE Loss: 1.0482876300811768\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 6.688404083251953 | KNN Loss: 5.640827655792236 | BCE Loss: 1.047576665878296\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 6.6749467849731445 | KNN Loss: 5.632040023803711 | BCE Loss: 1.0429068803787231\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 6.723240375518799 | KNN Loss: 5.671346187591553 | BCE Loss: 1.0518940687179565\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 6.6934661865234375 | KNN Loss: 5.645789623260498 | BCE Loss: 1.0476763248443604\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 6.649874687194824 | KNN Loss: 5.6029767990112305 | BCE Loss: 1.0468977689743042\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 6.660614967346191 | KNN Loss: 5.638337135314941 | BCE Loss: 1.022277593612671\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 6.620916366577148 | KNN Loss: 5.610855579376221 | BCE Loss: 1.0100609064102173\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 6.695725917816162 | KNN Loss: 5.644657611846924 | BCE Loss: 1.0510681867599487\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 6.6554083824157715 | KNN Loss: 5.605337619781494 | BCE Loss: 1.0500707626342773\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 6.679566383361816 | KNN Loss: 5.619411945343018 | BCE Loss: 1.0601543188095093\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 6.7036590576171875 | KNN Loss: 5.654351234436035 | BCE Loss: 1.0493075847625732\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 6.626678466796875 | KNN Loss: 5.60715913772583 | BCE Loss: 1.019519567489624\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 6.676717281341553 | KNN Loss: 5.6320672035217285 | BCE Loss: 1.0446499586105347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 6.635524749755859 | KNN Loss: 5.599367618560791 | BCE Loss: 1.0361573696136475\n",
      "Epoch   449: reducing learning rate of group 0 to 2.7058e-08.\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 6.635662078857422 | KNN Loss: 5.605652809143066 | BCE Loss: 1.0300092697143555\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 6.693984508514404 | KNN Loss: 5.647451400756836 | BCE Loss: 1.0465331077575684\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 6.695448398590088 | KNN Loss: 5.6347479820251465 | BCE Loss: 1.0607002973556519\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 6.683749198913574 | KNN Loss: 5.626462459564209 | BCE Loss: 1.0572868585586548\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 6.656353950500488 | KNN Loss: 5.63012170791626 | BCE Loss: 1.0262320041656494\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 6.671597480773926 | KNN Loss: 5.615428924560547 | BCE Loss: 1.056168794631958\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 6.753747940063477 | KNN Loss: 5.6805315017700195 | BCE Loss: 1.0732166767120361\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 6.679035186767578 | KNN Loss: 5.634798049926758 | BCE Loss: 1.0442373752593994\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 6.667383670806885 | KNN Loss: 5.601958274841309 | BCE Loss: 1.0654255151748657\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 6.657515525817871 | KNN Loss: 5.596179485321045 | BCE Loss: 1.061335802078247\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 6.630987644195557 | KNN Loss: 5.594229698181152 | BCE Loss: 1.0367579460144043\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 6.668030261993408 | KNN Loss: 5.619057655334473 | BCE Loss: 1.0489726066589355\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 6.694502353668213 | KNN Loss: 5.642724514007568 | BCE Loss: 1.0517778396606445\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 6.638016700744629 | KNN Loss: 5.591771602630615 | BCE Loss: 1.0462453365325928\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 6.6884684562683105 | KNN Loss: 5.640275478363037 | BCE Loss: 1.0481929779052734\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 6.633583068847656 | KNN Loss: 5.598020076751709 | BCE Loss: 1.0355632305145264\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 6.708685874938965 | KNN Loss: 5.684548377990723 | BCE Loss: 1.024137258529663\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 6.6543450355529785 | KNN Loss: 5.602300643920898 | BCE Loss: 1.0520445108413696\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 6.66910457611084 | KNN Loss: 5.609432697296143 | BCE Loss: 1.0596718788146973\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 6.659242630004883 | KNN Loss: 5.6050705909729 | BCE Loss: 1.0541718006134033\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 6.7390618324279785 | KNN Loss: 5.687307834625244 | BCE Loss: 1.0517539978027344\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 6.6271209716796875 | KNN Loss: 5.602560520172119 | BCE Loss: 1.024560570716858\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 6.65393590927124 | KNN Loss: 5.605825901031494 | BCE Loss: 1.0481098890304565\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 6.641953468322754 | KNN Loss: 5.618134498596191 | BCE Loss: 1.0238192081451416\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 6.713403701782227 | KNN Loss: 5.651811122894287 | BCE Loss: 1.06159245967865\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 6.675300598144531 | KNN Loss: 5.619686126708984 | BCE Loss: 1.0556144714355469\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 6.676263809204102 | KNN Loss: 5.641519069671631 | BCE Loss: 1.0347445011138916\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 6.67194128036499 | KNN Loss: 5.594198703765869 | BCE Loss: 1.0777424573898315\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 6.660752773284912 | KNN Loss: 5.611236572265625 | BCE Loss: 1.0495163202285767\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 6.605838298797607 | KNN Loss: 5.591887474060059 | BCE Loss: 1.0139509439468384\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 6.66822624206543 | KNN Loss: 5.620506763458252 | BCE Loss: 1.0477193593978882\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 6.713229179382324 | KNN Loss: 5.680694103240967 | BCE Loss: 1.0325350761413574\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 6.763430595397949 | KNN Loss: 5.691151142120361 | BCE Loss: 1.072279691696167\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 6.7931365966796875 | KNN Loss: 5.738675594329834 | BCE Loss: 1.0544610023498535\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 6.699496269226074 | KNN Loss: 5.622801303863525 | BCE Loss: 1.076695203781128\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 6.673422336578369 | KNN Loss: 5.626894474029541 | BCE Loss: 1.0465279817581177\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 6.64936637878418 | KNN Loss: 5.598402500152588 | BCE Loss: 1.0509638786315918\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 6.653083324432373 | KNN Loss: 5.593897342681885 | BCE Loss: 1.0591859817504883\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 6.689604759216309 | KNN Loss: 5.642900466918945 | BCE Loss: 1.0467045307159424\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 6.647733211517334 | KNN Loss: 5.618005752563477 | BCE Loss: 1.0297274589538574\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 6.634761333465576 | KNN Loss: 5.612663269042969 | BCE Loss: 1.0220980644226074\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 6.669425964355469 | KNN Loss: 5.616982460021973 | BCE Loss: 1.052443265914917\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 6.675492286682129 | KNN Loss: 5.603030681610107 | BCE Loss: 1.072461485862732\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 6.735866546630859 | KNN Loss: 5.660545349121094 | BCE Loss: 1.0753211975097656\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 6.640839099884033 | KNN Loss: 5.603065490722656 | BCE Loss: 1.037773609161377\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 6.656231880187988 | KNN Loss: 5.598397731781006 | BCE Loss: 1.0578339099884033\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 6.67064905166626 | KNN Loss: 5.605893135070801 | BCE Loss: 1.0647557973861694\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 6.648101806640625 | KNN Loss: 5.613983631134033 | BCE Loss: 1.0341182947158813\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 6.6750617027282715 | KNN Loss: 5.611250400543213 | BCE Loss: 1.0638113021850586\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 6.692632675170898 | KNN Loss: 5.646198749542236 | BCE Loss: 1.0464341640472412\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 6.6891584396362305 | KNN Loss: 5.62795877456665 | BCE Loss: 1.06119966506958\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 6.666659832000732 | KNN Loss: 5.625416278839111 | BCE Loss: 1.041243553161621\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 6.669291019439697 | KNN Loss: 5.622984886169434 | BCE Loss: 1.0463062524795532\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 6.783230781555176 | KNN Loss: 5.706819534301758 | BCE Loss: 1.076411247253418\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 6.710186958312988 | KNN Loss: 5.657215118408203 | BCE Loss: 1.0529718399047852\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 6.656790256500244 | KNN Loss: 5.609575271606445 | BCE Loss: 1.0472151041030884\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 6.657057762145996 | KNN Loss: 5.596718788146973 | BCE Loss: 1.060339093208313\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 6.693961143493652 | KNN Loss: 5.625394344329834 | BCE Loss: 1.0685665607452393\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 6.683270454406738 | KNN Loss: 5.629771709442139 | BCE Loss: 1.0534987449645996\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 6.689871788024902 | KNN Loss: 5.660414218902588 | BCE Loss: 1.0294573307037354\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 6.657163143157959 | KNN Loss: 5.601992607116699 | BCE Loss: 1.0551706552505493\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 6.711625099182129 | KNN Loss: 5.641224384307861 | BCE Loss: 1.0704004764556885\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 6.639462471008301 | KNN Loss: 5.614785671234131 | BCE Loss: 1.024677038192749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 6.671774387359619 | KNN Loss: 5.628738880157471 | BCE Loss: 1.0430353879928589\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 6.710239410400391 | KNN Loss: 5.652767658233643 | BCE Loss: 1.0574719905853271\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 6.665363788604736 | KNN Loss: 5.596570014953613 | BCE Loss: 1.0687936544418335\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 6.672429084777832 | KNN Loss: 5.633646488189697 | BCE Loss: 1.0387825965881348\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 6.689088821411133 | KNN Loss: 5.6199631690979 | BCE Loss: 1.0691254138946533\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 6.671342849731445 | KNN Loss: 5.636999607086182 | BCE Loss: 1.0343433618545532\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 6.694055557250977 | KNN Loss: 5.662044048309326 | BCE Loss: 1.0320112705230713\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 6.6909894943237305 | KNN Loss: 5.671223163604736 | BCE Loss: 1.0197665691375732\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 6.752148628234863 | KNN Loss: 5.7170329093933105 | BCE Loss: 1.0351157188415527\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 6.657623291015625 | KNN Loss: 5.610142707824707 | BCE Loss: 1.0474804639816284\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 6.672896862030029 | KNN Loss: 5.634165287017822 | BCE Loss: 1.0387316942214966\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 6.660297393798828 | KNN Loss: 5.614948749542236 | BCE Loss: 1.0453484058380127\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 6.688368320465088 | KNN Loss: 5.600039005279541 | BCE Loss: 1.0883293151855469\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 6.624085426330566 | KNN Loss: 5.611163139343262 | BCE Loss: 1.0129222869873047\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 6.626153945922852 | KNN Loss: 5.590205669403076 | BCE Loss: 1.0359481573104858\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 6.725050926208496 | KNN Loss: 5.690194129943848 | BCE Loss: 1.0348565578460693\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 6.668744087219238 | KNN Loss: 5.6275224685668945 | BCE Loss: 1.0412213802337646\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 6.697669506072998 | KNN Loss: 5.665945053100586 | BCE Loss: 1.031724452972412\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 6.678224563598633 | KNN Loss: 5.623978614807129 | BCE Loss: 1.054246187210083\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 6.657078742980957 | KNN Loss: 5.613868713378906 | BCE Loss: 1.0432100296020508\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 6.701128959655762 | KNN Loss: 5.657459735870361 | BCE Loss: 1.0436689853668213\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 6.674331188201904 | KNN Loss: 5.636129379272461 | BCE Loss: 1.0382018089294434\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 6.624727249145508 | KNN Loss: 5.6078619956970215 | BCE Loss: 1.0168654918670654\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 6.69556999206543 | KNN Loss: 5.61340856552124 | BCE Loss: 1.0821613073349\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 6.709390640258789 | KNN Loss: 5.644588470458984 | BCE Loss: 1.0648022890090942\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 6.656560897827148 | KNN Loss: 5.605220794677734 | BCE Loss: 1.051339864730835\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 6.656799793243408 | KNN Loss: 5.622086524963379 | BCE Loss: 1.0347132682800293\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 6.633047103881836 | KNN Loss: 5.592581748962402 | BCE Loss: 1.0404653549194336\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 6.676525115966797 | KNN Loss: 5.625193119049072 | BCE Loss: 1.051331877708435\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 6.659352779388428 | KNN Loss: 5.629467964172363 | BCE Loss: 1.029884934425354\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 6.654178142547607 | KNN Loss: 5.593454360961914 | BCE Loss: 1.0607237815856934\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 6.679012298583984 | KNN Loss: 5.594370365142822 | BCE Loss: 1.0846421718597412\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 6.645412445068359 | KNN Loss: 5.607624530792236 | BCE Loss: 1.0377880334854126\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 6.686596870422363 | KNN Loss: 5.6366424560546875 | BCE Loss: 1.0499546527862549\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 6.617124080657959 | KNN Loss: 5.6003642082214355 | BCE Loss: 1.016759991645813\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 6.662108421325684 | KNN Loss: 5.602300643920898 | BCE Loss: 1.0598077774047852\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 6.734788417816162 | KNN Loss: 5.665938377380371 | BCE Loss: 1.068850040435791\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 6.701748371124268 | KNN Loss: 5.656538963317871 | BCE Loss: 1.0452094078063965\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 6.7092604637146 | KNN Loss: 5.6456618309021 | BCE Loss: 1.0635987520217896\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 6.640887260437012 | KNN Loss: 5.6224751472473145 | BCE Loss: 1.0184122323989868\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 6.711459636688232 | KNN Loss: 5.667834281921387 | BCE Loss: 1.0436252355575562\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 6.642935276031494 | KNN Loss: 5.602584362030029 | BCE Loss: 1.0403509140014648\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 6.6544647216796875 | KNN Loss: 5.601287841796875 | BCE Loss: 1.0531771183013916\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 6.641184329986572 | KNN Loss: 5.588104248046875 | BCE Loss: 1.0530800819396973\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 6.6609039306640625 | KNN Loss: 5.616164684295654 | BCE Loss: 1.0447391271591187\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 6.732355117797852 | KNN Loss: 5.688989162445068 | BCE Loss: 1.0433658361434937\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 6.631955623626709 | KNN Loss: 5.6042633056640625 | BCE Loss: 1.0276923179626465\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 6.648116111755371 | KNN Loss: 5.598111629486084 | BCE Loss: 1.050004243850708\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 6.69062614440918 | KNN Loss: 5.610821723937988 | BCE Loss: 1.0798046588897705\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 6.676450252532959 | KNN Loss: 5.63607931137085 | BCE Loss: 1.0403708219528198\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 6.632672309875488 | KNN Loss: 5.610780715942383 | BCE Loss: 1.0218913555145264\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 6.645671844482422 | KNN Loss: 5.621728897094727 | BCE Loss: 1.0239429473876953\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 6.673868656158447 | KNN Loss: 5.63985538482666 | BCE Loss: 1.034013271331787\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 6.679337978363037 | KNN Loss: 5.601218223571777 | BCE Loss: 1.0781197547912598\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 6.648503303527832 | KNN Loss: 5.597591400146484 | BCE Loss: 1.0509119033813477\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 6.667553901672363 | KNN Loss: 5.594189643859863 | BCE Loss: 1.073364019393921\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 6.62551736831665 | KNN Loss: 5.60017728805542 | BCE Loss: 1.0253400802612305\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 6.688838005065918 | KNN Loss: 5.647130966186523 | BCE Loss: 1.041707158088684\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 6.672731399536133 | KNN Loss: 5.626274585723877 | BCE Loss: 1.0464569330215454\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 6.6701250076293945 | KNN Loss: 5.644514083862305 | BCE Loss: 1.0256106853485107\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 6.6512556076049805 | KNN Loss: 5.601715087890625 | BCE Loss: 1.0495407581329346\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 6.675486087799072 | KNN Loss: 5.643200874328613 | BCE Loss: 1.032285213470459\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 6.650226593017578 | KNN Loss: 5.599891662597656 | BCE Loss: 1.0503350496292114\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 6.688146114349365 | KNN Loss: 5.6194329261779785 | BCE Loss: 1.0687131881713867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 6.73414945602417 | KNN Loss: 5.693599700927734 | BCE Loss: 1.0405497550964355\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 6.708990573883057 | KNN Loss: 5.671413421630859 | BCE Loss: 1.0375771522521973\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 6.662715911865234 | KNN Loss: 5.614140510559082 | BCE Loss: 1.048575520515442\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 6.632256031036377 | KNN Loss: 5.601239204406738 | BCE Loss: 1.0310168266296387\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 6.696747303009033 | KNN Loss: 5.6356282234191895 | BCE Loss: 1.0611190795898438\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 6.6831254959106445 | KNN Loss: 5.640750885009766 | BCE Loss: 1.042374849319458\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 6.6779913902282715 | KNN Loss: 5.625983238220215 | BCE Loss: 1.052008032798767\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 6.68925666809082 | KNN Loss: 5.647762775421143 | BCE Loss: 1.0414941310882568\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 6.713127136230469 | KNN Loss: 5.690956115722656 | BCE Loss: 1.0221712589263916\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 6.665694236755371 | KNN Loss: 5.6265339851379395 | BCE Loss: 1.0391602516174316\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 6.75425910949707 | KNN Loss: 5.693195819854736 | BCE Loss: 1.061063289642334\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 6.6526408195495605 | KNN Loss: 5.595855712890625 | BCE Loss: 1.056785225868225\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 6.628515243530273 | KNN Loss: 5.60515022277832 | BCE Loss: 1.0233652591705322\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 6.691365718841553 | KNN Loss: 5.620080471038818 | BCE Loss: 1.0712851285934448\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 6.640643119812012 | KNN Loss: 5.605932235717773 | BCE Loss: 1.0347106456756592\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 6.666587829589844 | KNN Loss: 5.6120452880859375 | BCE Loss: 1.0545425415039062\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 6.640490531921387 | KNN Loss: 5.612369537353516 | BCE Loss: 1.0281212329864502\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 6.698766708374023 | KNN Loss: 5.643589019775391 | BCE Loss: 1.0551776885986328\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 6.727869033813477 | KNN Loss: 5.715514183044434 | BCE Loss: 1.012354850769043\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 6.727020263671875 | KNN Loss: 5.657820701599121 | BCE Loss: 1.0691993236541748\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 6.673820972442627 | KNN Loss: 5.636399269104004 | BCE Loss: 1.037421703338623\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 6.670656204223633 | KNN Loss: 5.605379104614258 | BCE Loss: 1.065276861190796\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 6.673402786254883 | KNN Loss: 5.644721031188965 | BCE Loss: 1.0286815166473389\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 6.615877151489258 | KNN Loss: 5.595998764038086 | BCE Loss: 1.019878625869751\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 6.65377140045166 | KNN Loss: 5.61009407043457 | BCE Loss: 1.0436770915985107\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 6.61964225769043 | KNN Loss: 5.589859962463379 | BCE Loss: 1.0297825336456299\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 6.671199798583984 | KNN Loss: 5.616375923156738 | BCE Loss: 1.0548241138458252\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 6.663862228393555 | KNN Loss: 5.612475872039795 | BCE Loss: 1.0513863563537598\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 6.7348952293396 | KNN Loss: 5.696952819824219 | BCE Loss: 1.0379424095153809\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 6.687621116638184 | KNN Loss: 5.663974761962891 | BCE Loss: 1.0236461162567139\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 6.7070770263671875 | KNN Loss: 5.628228187561035 | BCE Loss: 1.0788488388061523\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 6.624237060546875 | KNN Loss: 5.59895133972168 | BCE Loss: 1.0252854824066162\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 6.674413681030273 | KNN Loss: 5.623566627502441 | BCE Loss: 1.0508472919464111\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 6.673436164855957 | KNN Loss: 5.606017112731934 | BCE Loss: 1.0674192905426025\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 6.693157196044922 | KNN Loss: 5.644033432006836 | BCE Loss: 1.049123764038086\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 6.660000801086426 | KNN Loss: 5.6065850257873535 | BCE Loss: 1.0534155368804932\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 6.683574676513672 | KNN Loss: 5.630326271057129 | BCE Loss: 1.0532481670379639\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 6.696987152099609 | KNN Loss: 5.6360297203063965 | BCE Loss: 1.0609571933746338\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 6.66052770614624 | KNN Loss: 5.610945701599121 | BCE Loss: 1.0495821237564087\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 6.780918121337891 | KNN Loss: 5.731322288513184 | BCE Loss: 1.0495960712432861\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 6.672986030578613 | KNN Loss: 5.622878551483154 | BCE Loss: 1.050107717514038\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 6.692600727081299 | KNN Loss: 5.652897834777832 | BCE Loss: 1.0397028923034668\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 6.644209861755371 | KNN Loss: 5.592029094696045 | BCE Loss: 1.0521810054779053\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 6.669859886169434 | KNN Loss: 5.620104789733887 | BCE Loss: 1.0497550964355469\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 6.727059841156006 | KNN Loss: 5.65057373046875 | BCE Loss: 1.0764861106872559\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 6.731162071228027 | KNN Loss: 5.685530185699463 | BCE Loss: 1.0456318855285645\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 6.765413284301758 | KNN Loss: 5.728531360626221 | BCE Loss: 1.0368821620941162\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 6.700961112976074 | KNN Loss: 5.655492305755615 | BCE Loss: 1.045468807220459\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 6.692674160003662 | KNN Loss: 5.66456413269043 | BCE Loss: 1.0281100273132324\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 6.742493629455566 | KNN Loss: 5.705367088317871 | BCE Loss: 1.0371266603469849\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 6.651144504547119 | KNN Loss: 5.5900187492370605 | BCE Loss: 1.0611257553100586\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 6.687408924102783 | KNN Loss: 5.6161274909973145 | BCE Loss: 1.0712815523147583\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 6.664942741394043 | KNN Loss: 5.630167007446289 | BCE Loss: 1.0347754955291748\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 6.706809997558594 | KNN Loss: 5.667221546173096 | BCE Loss: 1.0395883321762085\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 6.719112873077393 | KNN Loss: 5.647934436798096 | BCE Loss: 1.0711785554885864\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 6.6222944259643555 | KNN Loss: 5.593985557556152 | BCE Loss: 1.0283088684082031\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 6.76102876663208 | KNN Loss: 5.706748008728027 | BCE Loss: 1.0542807579040527\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 6.6918768882751465 | KNN Loss: 5.604228496551514 | BCE Loss: 1.0876482725143433\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 6.640913963317871 | KNN Loss: 5.6110944747924805 | BCE Loss: 1.0298194885253906\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 6.74128532409668 | KNN Loss: 5.68566370010376 | BCE Loss: 1.0556217432022095\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 6.6600751876831055 | KNN Loss: 5.628566741943359 | BCE Loss: 1.031508445739746\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 6.643996238708496 | KNN Loss: 5.6073222160339355 | BCE Loss: 1.0366742610931396\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 6.702867031097412 | KNN Loss: 5.641635894775391 | BCE Loss: 1.061231017112732\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 6.640423774719238 | KNN Loss: 5.596042156219482 | BCE Loss: 1.0443813800811768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 6.637829303741455 | KNN Loss: 5.596517562866211 | BCE Loss: 1.0413117408752441\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 6.676870822906494 | KNN Loss: 5.602390289306641 | BCE Loss: 1.0744805335998535\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 6.654966831207275 | KNN Loss: 5.598052024841309 | BCE Loss: 1.0569148063659668\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 6.650898456573486 | KNN Loss: 5.617271900177002 | BCE Loss: 1.033626675605774\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 6.660549163818359 | KNN Loss: 5.61375617980957 | BCE Loss: 1.046792984008789\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 6.6108293533325195 | KNN Loss: 5.593349933624268 | BCE Loss: 1.017479658126831\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 6.678884506225586 | KNN Loss: 5.632691383361816 | BCE Loss: 1.0461933612823486\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 6.627462863922119 | KNN Loss: 5.601944923400879 | BCE Loss: 1.0255179405212402\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 6.686910152435303 | KNN Loss: 5.627257347106934 | BCE Loss: 1.0596528053283691\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 6.653560638427734 | KNN Loss: 5.601221561431885 | BCE Loss: 1.0523390769958496\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 6.717879295349121 | KNN Loss: 5.649173736572266 | BCE Loss: 1.0687055587768555\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 6.657750129699707 | KNN Loss: 5.642836093902588 | BCE Loss: 1.0149140357971191\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 6.642599582672119 | KNN Loss: 5.607827186584473 | BCE Loss: 1.0347723960876465\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 6.644579887390137 | KNN Loss: 5.598537445068359 | BCE Loss: 1.0460422039031982\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 6.7065110206604 | KNN Loss: 5.633479595184326 | BCE Loss: 1.0730313062667847\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 6.631509780883789 | KNN Loss: 5.600603103637695 | BCE Loss: 1.0309064388275146\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 6.649407386779785 | KNN Loss: 5.620271682739258 | BCE Loss: 1.0291354656219482\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 6.655218124389648 | KNN Loss: 5.601654052734375 | BCE Loss: 1.053564190864563\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 6.699135780334473 | KNN Loss: 5.650725841522217 | BCE Loss: 1.048410177230835\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 6.6668243408203125 | KNN Loss: 5.6137166023254395 | BCE Loss: 1.0531076192855835\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 6.640144348144531 | KNN Loss: 5.6147379875183105 | BCE Loss: 1.0254065990447998\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 6.715521812438965 | KNN Loss: 5.640613079071045 | BCE Loss: 1.0749086141586304\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 6.656554222106934 | KNN Loss: 5.623694896697998 | BCE Loss: 1.0328595638275146\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 6.65233850479126 | KNN Loss: 5.61354398727417 | BCE Loss: 1.0387943983078003\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 6.62727165222168 | KNN Loss: 5.601531028747559 | BCE Loss: 1.025740385055542\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 6.662363052368164 | KNN Loss: 5.614471912384033 | BCE Loss: 1.0478911399841309\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 6.666274547576904 | KNN Loss: 5.598230838775635 | BCE Loss: 1.06804358959198\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 6.717177391052246 | KNN Loss: 5.686766147613525 | BCE Loss: 1.0304110050201416\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 6.722360610961914 | KNN Loss: 5.654813766479492 | BCE Loss: 1.067547082901001\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 6.718212127685547 | KNN Loss: 5.651597023010254 | BCE Loss: 1.066615343093872\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 6.662272930145264 | KNN Loss: 5.621099948883057 | BCE Loss: 1.041172981262207\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 6.689404487609863 | KNN Loss: 5.628889083862305 | BCE Loss: 1.060515284538269\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 6.621281147003174 | KNN Loss: 5.592915058135986 | BCE Loss: 1.028366208076477\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 6.647954940795898 | KNN Loss: 5.6058783531188965 | BCE Loss: 1.0420763492584229\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 6.638957977294922 | KNN Loss: 5.590006351470947 | BCE Loss: 1.0489516258239746\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 6.610073089599609 | KNN Loss: 5.598189830780029 | BCE Loss: 1.0118831396102905\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 6.667953968048096 | KNN Loss: 5.615037441253662 | BCE Loss: 1.052916407585144\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 6.696908950805664 | KNN Loss: 5.62927770614624 | BCE Loss: 1.0676311254501343\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 6.670215606689453 | KNN Loss: 5.633310794830322 | BCE Loss: 1.0369045734405518\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 6.663183689117432 | KNN Loss: 5.61544132232666 | BCE Loss: 1.047742247581482\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 6.630199432373047 | KNN Loss: 5.594759464263916 | BCE Loss: 1.0354400873184204\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 6.7096266746521 | KNN Loss: 5.666536331176758 | BCE Loss: 1.0430903434753418\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 6.665780544281006 | KNN Loss: 5.623115062713623 | BCE Loss: 1.0426656007766724\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 6.668059825897217 | KNN Loss: 5.613572597503662 | BCE Loss: 1.0544871091842651\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 6.724259376525879 | KNN Loss: 5.641274452209473 | BCE Loss: 1.0829851627349854\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 6.6546196937561035 | KNN Loss: 5.601091384887695 | BCE Loss: 1.0535281896591187\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 6.663692474365234 | KNN Loss: 5.650815963745117 | BCE Loss: 1.0128763914108276\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 6.617616653442383 | KNN Loss: 5.6038007736206055 | BCE Loss: 1.0138161182403564\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 6.692159175872803 | KNN Loss: 5.624897480010986 | BCE Loss: 1.0672616958618164\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 6.6242828369140625 | KNN Loss: 5.6034650802612305 | BCE Loss: 1.020817756652832\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 6.745352745056152 | KNN Loss: 5.6640305519104 | BCE Loss: 1.0813219547271729\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 6.6098127365112305 | KNN Loss: 5.595585823059082 | BCE Loss: 1.0142266750335693\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 6.614205837249756 | KNN Loss: 5.603956699371338 | BCE Loss: 1.0102492570877075\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 6.716782569885254 | KNN Loss: 5.646261692047119 | BCE Loss: 1.0705211162567139\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 6.660917282104492 | KNN Loss: 5.617746353149414 | BCE Loss: 1.043170690536499\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 6.6908111572265625 | KNN Loss: 5.633902549743652 | BCE Loss: 1.0569088459014893\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 6.719539642333984 | KNN Loss: 5.660706996917725 | BCE Loss: 1.0588326454162598\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 6.727435111999512 | KNN Loss: 5.697198867797852 | BCE Loss: 1.0302362442016602\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 6.704747676849365 | KNN Loss: 5.668278694152832 | BCE Loss: 1.0364691019058228\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 6.671433448791504 | KNN Loss: 5.622239112854004 | BCE Loss: 1.0491943359375\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 6.645703315734863 | KNN Loss: 5.600691318511963 | BCE Loss: 1.0450122356414795\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 6.67612361907959 | KNN Loss: 5.656556129455566 | BCE Loss: 1.019567608833313\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 6.668328762054443 | KNN Loss: 5.623673915863037 | BCE Loss: 1.0446549654006958\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 6.643455505371094 | KNN Loss: 5.593846321105957 | BCE Loss: 1.0496091842651367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 6.679714202880859 | KNN Loss: 5.628486156463623 | BCE Loss: 1.0512281656265259\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 6.605490684509277 | KNN Loss: 5.593974590301514 | BCE Loss: 1.0115163326263428\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 6.646239757537842 | KNN Loss: 5.609392166137695 | BCE Loss: 1.0368475914001465\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 6.792389869689941 | KNN Loss: 5.7402024269104 | BCE Loss: 1.052187204360962\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 6.681451797485352 | KNN Loss: 5.624104022979736 | BCE Loss: 1.0573480129241943\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 6.635441780090332 | KNN Loss: 5.600375175476074 | BCE Loss: 1.0350664854049683\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 6.64028263092041 | KNN Loss: 5.606632232666016 | BCE Loss: 1.0336506366729736\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 6.675051689147949 | KNN Loss: 5.650125026702881 | BCE Loss: 1.024926781654358\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 6.72893762588501 | KNN Loss: 5.6793389320373535 | BCE Loss: 1.0495985746383667\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 6.735786437988281 | KNN Loss: 5.70761251449585 | BCE Loss: 1.0281739234924316\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 6.64148473739624 | KNN Loss: 5.591862678527832 | BCE Loss: 1.0496221780776978\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 6.664213180541992 | KNN Loss: 5.611552715301514 | BCE Loss: 1.0526604652404785\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 6.802501678466797 | KNN Loss: 5.7453484535217285 | BCE Loss: 1.0571532249450684\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 6.634798526763916 | KNN Loss: 5.59348726272583 | BCE Loss: 1.0413111448287964\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 6.642749786376953 | KNN Loss: 5.596075057983398 | BCE Loss: 1.0466748476028442\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 6.65333890914917 | KNN Loss: 5.602693557739258 | BCE Loss: 1.050645351409912\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 6.706055641174316 | KNN Loss: 5.6330389976501465 | BCE Loss: 1.07301664352417\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 6.701375961303711 | KNN Loss: 5.635009288787842 | BCE Loss: 1.0663666725158691\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 6.669085502624512 | KNN Loss: 5.633152008056641 | BCE Loss: 1.035933494567871\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 6.67299747467041 | KNN Loss: 5.614989757537842 | BCE Loss: 1.0580075979232788\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 6.7106428146362305 | KNN Loss: 5.647158145904541 | BCE Loss: 1.0634849071502686\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 6.6356940269470215 | KNN Loss: 5.591989994049072 | BCE Loss: 1.0437041521072388\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 6.648123264312744 | KNN Loss: 5.602875232696533 | BCE Loss: 1.0452481508255005\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 6.659712791442871 | KNN Loss: 5.606884002685547 | BCE Loss: 1.0528290271759033\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 6.700828552246094 | KNN Loss: 5.644547939300537 | BCE Loss: 1.0562806129455566\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 6.679758071899414 | KNN Loss: 5.634733200073242 | BCE Loss: 1.0450248718261719\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 6.648495197296143 | KNN Loss: 5.598011016845703 | BCE Loss: 1.0504841804504395\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 6.690381050109863 | KNN Loss: 5.6333513259887695 | BCE Loss: 1.0570294857025146\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 6.6561126708984375 | KNN Loss: 5.608158588409424 | BCE Loss: 1.0479542016983032\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 6.663885593414307 | KNN Loss: 5.6086273193359375 | BCE Loss: 1.0552581548690796\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 6.662093639373779 | KNN Loss: 5.62150764465332 | BCE Loss: 1.0405858755111694\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 6.777378082275391 | KNN Loss: 5.698663234710693 | BCE Loss: 1.0787148475646973\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 6.671642303466797 | KNN Loss: 5.6184868812561035 | BCE Loss: 1.053155541419983\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 6.78965425491333 | KNN Loss: 5.720019340515137 | BCE Loss: 1.0696349143981934\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 6.64028263092041 | KNN Loss: 5.595835208892822 | BCE Loss: 1.0444471836090088\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 6.6431121826171875 | KNN Loss: 5.612637996673584 | BCE Loss: 1.0304739475250244\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 6.716015815734863 | KNN Loss: 5.653799057006836 | BCE Loss: 1.0622169971466064\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 6.641294479370117 | KNN Loss: 5.611424446105957 | BCE Loss: 1.029869794845581\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 6.68833065032959 | KNN Loss: 5.631563663482666 | BCE Loss: 1.0567668676376343\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 6.6365814208984375 | KNN Loss: 5.597411632537842 | BCE Loss: 1.0391699075698853\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 6.688711643218994 | KNN Loss: 5.613560199737549 | BCE Loss: 1.0751514434814453\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 6.713099002838135 | KNN Loss: 5.675899505615234 | BCE Loss: 1.0371994972229004\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 6.686490058898926 | KNN Loss: 5.62043571472168 | BCE Loss: 1.0660542249679565\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 6.710953712463379 | KNN Loss: 5.636503219604492 | BCE Loss: 1.0744507312774658\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 6.653919219970703 | KNN Loss: 5.5970940589904785 | BCE Loss: 1.0568249225616455\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 6.682904243469238 | KNN Loss: 5.639444351196289 | BCE Loss: 1.0434601306915283\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 6.660362243652344 | KNN Loss: 5.605517387390137 | BCE Loss: 1.054844856262207\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 6.646269798278809 | KNN Loss: 5.610726356506348 | BCE Loss: 1.03554368019104\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 6.641678333282471 | KNN Loss: 5.6131978034973145 | BCE Loss: 1.0284805297851562\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 6.688453674316406 | KNN Loss: 5.626324653625488 | BCE Loss: 1.062129020690918\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 6.63626766204834 | KNN Loss: 5.594204425811768 | BCE Loss: 1.0420634746551514\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.8459,  3.5631,  2.5001,  3.1537,  3.3568,  0.6708,  2.3711,  2.0827,\n",
      "          2.1993,  1.9800,  2.0103,  1.9847,  0.8091,  1.7728,  1.2656,  1.4792,\n",
      "          2.4599,  2.8532,  2.6886,  2.2220,  1.6471,  2.9088,  2.0390,  2.3761,\n",
      "          2.4719,  1.5146,  1.8861,  1.3733,  1.5239,  0.2685, -0.1907,  0.9555,\n",
      "          0.2066,  0.9097,  1.4664,  1.3153,  0.9876,  3.2347,  0.8022,  1.3234,\n",
      "          0.9608, -0.6582, -0.2546,  2.2673,  1.9412,  0.6751, -0.1418,  0.0990,\n",
      "          1.4441,  2.3865,  1.7711,  0.0801,  1.4148,  0.4678, -0.5583,  1.0986,\n",
      "          1.4249,  1.3179,  1.2942,  1.7721,  0.5641,  0.8216,  0.1511,  1.6695,\n",
      "          1.2416,  1.6097, -1.7406,  0.2939,  2.2443,  2.0747,  2.4529,  0.4525,\n",
      "          1.3193,  2.3935,  1.7749,  1.2830,  0.2490,  0.7379,  0.2319,  1.5311,\n",
      "          0.0470,  0.3363,  1.7889, -0.3315,  0.2074, -1.0918, -2.2267, -0.2803,\n",
      "          0.5183, -1.7710,  0.4681, -0.1521, -0.5530, -0.8952,  0.5180,  1.2434,\n",
      "         -0.6065, -0.6744,  0.3626,  1.0738,  0.6741, -1.1953,  0.9330,  1.1014,\n",
      "         -1.1647, -1.0307, -0.1454,  0.0684, -1.0252, -1.5553, -0.4883, -2.5147,\n",
      "         -0.3727,  1.6394,  1.3009, -0.2656, -0.5810,  0.0257,  1.5339, -2.3078,\n",
      "          0.1515, -0.2317,  0.4348, -0.6715,  0.0654, -0.6897, -0.8584,  0.8531,\n",
      "          0.2332, -0.5335,  0.3199, -0.5919, -1.3292, -0.3618, -0.4231,  0.7680,\n",
      "         -0.4416,  0.0502, -1.8896, -0.9591, -1.3675,  0.5481, -1.7583, -0.9856,\n",
      "         -0.9093, -0.6267, -1.4643, -1.0943, -2.1532, -0.9135, -1.3037, -0.3505,\n",
      "         -1.6464,  0.4610, -1.4282, -0.5143, -2.7820,  0.1660, -0.0672, -0.7036,\n",
      "         -2.0556, -1.6149, -1.2393, -1.3177, -2.2648, -2.1993, -2.7936]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-2.7936, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(3.5631, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533b2ac962ea4df5ae24ebcde0b74979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].to('cpu') for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  5.63it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().to('cpu')\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d4d524a1d14ddb8ca58b07bc3deba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f5139121f5412db9e8a04a8ac13643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.02, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e47fe1458b410494a05244ab4a91ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "Epoch: 00 | Batch: 000 / 021 | Total loss: 9.760 | Reg loss: 0.007 | Tree loss: 9.760 | Accuracy: 0.000000 | 0.634 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 021 | Total loss: 9.752 | Reg loss: 0.007 | Tree loss: 9.752 | Accuracy: 0.000000 | 0.369 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 021 | Total loss: 9.742 | Reg loss: 0.007 | Tree loss: 9.742 | Accuracy: 0.000000 | 0.279 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 021 | Total loss: 9.728 | Reg loss: 0.006 | Tree loss: 9.728 | Accuracy: 0.000000 | 0.234 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 021 | Total loss: 9.717 | Reg loss: 0.006 | Tree loss: 9.717 | Accuracy: 0.000000 | 0.208 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 021 | Total loss: 9.703 | Reg loss: 0.006 | Tree loss: 9.703 | Accuracy: 0.000000 | 0.191 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 021 | Total loss: 9.693 | Reg loss: 0.006 | Tree loss: 9.693 | Accuracy: 0.000000 | 0.178 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 021 | Total loss: 9.677 | Reg loss: 0.006 | Tree loss: 9.677 | Accuracy: 0.000000 | 0.169 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 021 | Total loss: 9.667 | Reg loss: 0.007 | Tree loss: 9.667 | Accuracy: 0.000000 | 0.162 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 021 | Total loss: 9.656 | Reg loss: 0.007 | Tree loss: 9.656 | Accuracy: 0.000000 | 0.157 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 021 | Total loss: 9.641 | Reg loss: 0.007 | Tree loss: 9.641 | Accuracy: 0.000000 | 0.152 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 021 | Total loss: 9.631 | Reg loss: 0.007 | Tree loss: 9.631 | Accuracy: 0.000000 | 0.148 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 021 | Total loss: 9.616 | Reg loss: 0.007 | Tree loss: 9.616 | Accuracy: 0.000000 | 0.145 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 021 | Total loss: 9.609 | Reg loss: 0.008 | Tree loss: 9.609 | Accuracy: 0.000000 | 0.142 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 021 | Total loss: 9.594 | Reg loss: 0.008 | Tree loss: 9.594 | Accuracy: 0.000000 | 0.139 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 021 | Total loss: 9.584 | Reg loss: 0.008 | Tree loss: 9.584 | Accuracy: 0.000000 | 0.137 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 021 | Total loss: 9.569 | Reg loss: 0.008 | Tree loss: 9.569 | Accuracy: 0.000000 | 0.135 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 021 | Total loss: 9.557 | Reg loss: 0.009 | Tree loss: 9.557 | Accuracy: 0.000000 | 0.133 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 021 | Total loss: 9.548 | Reg loss: 0.009 | Tree loss: 9.548 | Accuracy: 0.000000 | 0.132 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 021 | Total loss: 9.532 | Reg loss: 0.009 | Tree loss: 9.532 | Accuracy: 0.000000 | 0.131 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 021 | Total loss: 9.521 | Reg loss: 0.010 | Tree loss: 9.521 | Accuracy: 0.000000 | 0.131 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 01 | Batch: 000 / 021 | Total loss: 9.637 | Reg loss: 0.003 | Tree loss: 9.637 | Accuracy: 0.000000 | 0.134 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 021 | Total loss: 9.622 | Reg loss: 0.003 | Tree loss: 9.622 | Accuracy: 0.000000 | 0.132 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 021 | Total loss: 9.613 | Reg loss: 0.003 | Tree loss: 9.613 | Accuracy: 0.000000 | 0.131 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 021 | Total loss: 9.603 | Reg loss: 0.004 | Tree loss: 9.603 | Accuracy: 0.000000 | 0.13 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 021 | Total loss: 9.591 | Reg loss: 0.004 | Tree loss: 9.591 | Accuracy: 0.000000 | 0.129 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 021 | Total loss: 9.575 | Reg loss: 0.004 | Tree loss: 9.575 | Accuracy: 0.000000 | 0.128 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 021 | Total loss: 9.565 | Reg loss: 0.005 | Tree loss: 9.565 | Accuracy: 0.000000 | 0.127 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 021 | Total loss: 9.555 | Reg loss: 0.005 | Tree loss: 9.555 | Accuracy: 0.000000 | 0.127 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 021 | Total loss: 9.544 | Reg loss: 0.005 | Tree loss: 9.544 | Accuracy: 0.000000 | 0.126 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 021 | Total loss: 9.529 | Reg loss: 0.006 | Tree loss: 9.529 | Accuracy: 0.000000 | 0.125 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 021 | Total loss: 9.518 | Reg loss: 0.006 | Tree loss: 9.518 | Accuracy: 0.000000 | 0.124 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 021 | Total loss: 9.504 | Reg loss: 0.006 | Tree loss: 9.504 | Accuracy: 0.000000 | 0.123 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 021 | Total loss: 9.496 | Reg loss: 0.007 | Tree loss: 9.496 | Accuracy: 0.000000 | 0.123 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 021 | Total loss: 9.481 | Reg loss: 0.007 | Tree loss: 9.481 | Accuracy: 0.000000 | 0.122 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 021 | Total loss: 9.468 | Reg loss: 0.007 | Tree loss: 9.468 | Accuracy: 0.000000 | 0.122 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 021 | Total loss: 9.454 | Reg loss: 0.008 | Tree loss: 9.454 | Accuracy: 0.000000 | 0.122 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 021 | Total loss: 9.441 | Reg loss: 0.008 | Tree loss: 9.441 | Accuracy: 0.000000 | 0.121 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 021 | Total loss: 9.429 | Reg loss: 0.009 | Tree loss: 9.429 | Accuracy: 0.000000 | 0.121 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 021 | Total loss: 9.418 | Reg loss: 0.009 | Tree loss: 9.418 | Accuracy: 0.000000 | 0.12 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 021 | Total loss: 9.405 | Reg loss: 0.009 | Tree loss: 9.405 | Accuracy: 0.000000 | 0.12 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 021 | Total loss: 9.397 | Reg loss: 0.010 | Tree loss: 9.397 | Accuracy: 0.000000 | 0.12 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 02 | Batch: 000 / 021 | Total loss: 9.520 | Reg loss: 0.005 | Tree loss: 9.520 | Accuracy: 0.000000 | 0.121 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 021 | Total loss: 9.507 | Reg loss: 0.005 | Tree loss: 9.507 | Accuracy: 0.000000 | 0.121 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 021 | Total loss: 9.495 | Reg loss: 0.005 | Tree loss: 9.495 | Accuracy: 0.000000 | 0.12 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 021 | Total loss: 9.483 | Reg loss: 0.005 | Tree loss: 9.483 | Accuracy: 0.000000 | 0.119 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 021 | Total loss: 9.472 | Reg loss: 0.005 | Tree loss: 9.472 | Accuracy: 0.000000 | 0.119 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 021 | Total loss: 9.460 | Reg loss: 0.006 | Tree loss: 9.460 | Accuracy: 0.000000 | 0.119 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 021 | Total loss: 9.448 | Reg loss: 0.006 | Tree loss: 9.448 | Accuracy: 0.000000 | 0.118 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 021 | Total loss: 9.436 | Reg loss: 0.006 | Tree loss: 9.436 | Accuracy: 0.000000 | 0.117 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 021 | Total loss: 9.422 | Reg loss: 0.006 | Tree loss: 9.422 | Accuracy: 0.000000 | 0.117 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 021 | Total loss: 9.411 | Reg loss: 0.007 | Tree loss: 9.411 | Accuracy: 0.001953 | 0.117 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 021 | Total loss: 9.399 | Reg loss: 0.007 | Tree loss: 9.399 | Accuracy: 0.000000 | 0.117 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 021 | Total loss: 9.382 | Reg loss: 0.007 | Tree loss: 9.382 | Accuracy: 0.001953 | 0.117 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 021 | Total loss: 9.375 | Reg loss: 0.008 | Tree loss: 9.375 | Accuracy: 0.000000 | 0.116 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 021 | Total loss: 9.359 | Reg loss: 0.008 | Tree loss: 9.359 | Accuracy: 0.001953 | 0.115 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 021 | Total loss: 9.350 | Reg loss: 0.008 | Tree loss: 9.350 | Accuracy: 0.003906 | 0.115 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 021 | Total loss: 9.334 | Reg loss: 0.009 | Tree loss: 9.334 | Accuracy: 0.009766 | 0.114 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 021 | Total loss: 9.322 | Reg loss: 0.009 | Tree loss: 9.322 | Accuracy: 0.015625 | 0.114 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 021 | Total loss: 9.314 | Reg loss: 0.010 | Tree loss: 9.314 | Accuracy: 0.013672 | 0.114 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 021 | Total loss: 9.298 | Reg loss: 0.010 | Tree loss: 9.298 | Accuracy: 0.025391 | 0.114 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 021 | Total loss: 9.285 | Reg loss: 0.010 | Tree loss: 9.285 | Accuracy: 0.052734 | 0.114 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 021 | Total loss: 9.273 | Reg loss: 0.011 | Tree loss: 9.273 | Accuracy: 0.068702 | 0.113 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Batch: 000 / 021 | Total loss: 9.402 | Reg loss: 0.006 | Tree loss: 9.402 | Accuracy: 0.000000 | 0.114 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 021 | Total loss: 9.391 | Reg loss: 0.006 | Tree loss: 9.391 | Accuracy: 0.000000 | 0.114 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 021 | Total loss: 9.380 | Reg loss: 0.007 | Tree loss: 9.380 | Accuracy: 0.001953 | 0.114 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 021 | Total loss: 9.366 | Reg loss: 0.007 | Tree loss: 9.366 | Accuracy: 0.000000 | 0.113 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 021 | Total loss: 9.349 | Reg loss: 0.007 | Tree loss: 9.349 | Accuracy: 0.003906 | 0.113 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 021 | Total loss: 9.341 | Reg loss: 0.007 | Tree loss: 9.341 | Accuracy: 0.011719 | 0.113 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 021 | Total loss: 9.331 | Reg loss: 0.007 | Tree loss: 9.331 | Accuracy: 0.011719 | 0.113 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 021 | Total loss: 9.315 | Reg loss: 0.008 | Tree loss: 9.315 | Accuracy: 0.023438 | 0.112 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 021 | Total loss: 9.303 | Reg loss: 0.008 | Tree loss: 9.303 | Accuracy: 0.037109 | 0.112 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 021 | Total loss: 9.291 | Reg loss: 0.008 | Tree loss: 9.291 | Accuracy: 0.064453 | 0.111 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 021 | Total loss: 9.284 | Reg loss: 0.008 | Tree loss: 9.284 | Accuracy: 0.105469 | 0.111 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 021 | Total loss: 9.263 | Reg loss: 0.009 | Tree loss: 9.263 | Accuracy: 0.193359 | 0.11 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 021 | Total loss: 9.252 | Reg loss: 0.009 | Tree loss: 9.252 | Accuracy: 0.265625 | 0.11 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 021 | Total loss: 9.242 | Reg loss: 0.009 | Tree loss: 9.242 | Accuracy: 0.302734 | 0.11 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 021 | Total loss: 9.229 | Reg loss: 0.010 | Tree loss: 9.229 | Accuracy: 0.335938 | 0.109 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 021 | Total loss: 9.216 | Reg loss: 0.010 | Tree loss: 9.216 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 021 | Total loss: 9.201 | Reg loss: 0.010 | Tree loss: 9.201 | Accuracy: 0.371094 | 0.108 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 021 | Total loss: 9.187 | Reg loss: 0.011 | Tree loss: 9.187 | Accuracy: 0.349609 | 0.108 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 021 | Total loss: 9.169 | Reg loss: 0.011 | Tree loss: 9.169 | Accuracy: 0.382812 | 0.108 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 021 | Total loss: 9.166 | Reg loss: 0.011 | Tree loss: 9.166 | Accuracy: 0.349609 | 0.108 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 021 | Total loss: 9.147 | Reg loss: 0.012 | Tree loss: 9.147 | Accuracy: 0.328244 | 0.107 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 04 | Batch: 000 / 021 | Total loss: 9.285 | Reg loss: 0.008 | Tree loss: 9.285 | Accuracy: 0.216797 | 0.108 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 021 | Total loss: 9.273 | Reg loss: 0.008 | Tree loss: 9.273 | Accuracy: 0.240234 | 0.107 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 021 | Total loss: 9.260 | Reg loss: 0.008 | Tree loss: 9.260 | Accuracy: 0.318359 | 0.107 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 021 | Total loss: 9.247 | Reg loss: 0.008 | Tree loss: 9.247 | Accuracy: 0.328125 | 0.107 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 021 | Total loss: 9.233 | Reg loss: 0.008 | Tree loss: 9.233 | Accuracy: 0.369141 | 0.107 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 021 | Total loss: 9.224 | Reg loss: 0.009 | Tree loss: 9.224 | Accuracy: 0.363281 | 0.106 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 021 | Total loss: 9.211 | Reg loss: 0.009 | Tree loss: 9.211 | Accuracy: 0.347656 | 0.106 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 021 | Total loss: 9.194 | Reg loss: 0.009 | Tree loss: 9.194 | Accuracy: 0.380859 | 0.106 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 021 | Total loss: 9.187 | Reg loss: 0.009 | Tree loss: 9.187 | Accuracy: 0.335938 | 0.105 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 021 | Total loss: 9.171 | Reg loss: 0.009 | Tree loss: 9.171 | Accuracy: 0.349609 | 0.105 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 021 | Total loss: 9.155 | Reg loss: 0.010 | Tree loss: 9.155 | Accuracy: 0.355469 | 0.105 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 021 | Total loss: 9.148 | Reg loss: 0.010 | Tree loss: 9.148 | Accuracy: 0.337891 | 0.105 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 021 | Total loss: 9.135 | Reg loss: 0.010 | Tree loss: 9.135 | Accuracy: 0.337891 | 0.104 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 021 | Total loss: 9.118 | Reg loss: 0.011 | Tree loss: 9.118 | Accuracy: 0.394531 | 0.104 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 021 | Total loss: 9.103 | Reg loss: 0.011 | Tree loss: 9.103 | Accuracy: 0.359375 | 0.104 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 021 | Total loss: 9.092 | Reg loss: 0.011 | Tree loss: 9.092 | Accuracy: 0.357422 | 0.104 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 021 | Total loss: 9.074 | Reg loss: 0.012 | Tree loss: 9.074 | Accuracy: 0.394531 | 0.104 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 021 | Total loss: 9.062 | Reg loss: 0.012 | Tree loss: 9.062 | Accuracy: 0.369141 | 0.104 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 021 | Total loss: 9.055 | Reg loss: 0.012 | Tree loss: 9.055 | Accuracy: 0.378906 | 0.104 sec/iter\n",
      "Epoch: 04 | Batch: 019 / 021 | Total loss: 9.033 | Reg loss: 0.013 | Tree loss: 9.033 | Accuracy: 0.373047 | 0.104 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 021 | Total loss: 9.032 | Reg loss: 0.013 | Tree loss: 9.032 | Accuracy: 0.396947 | 0.104 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 05 | Batch: 000 / 021 | Total loss: 9.170 | Reg loss: 0.009 | Tree loss: 9.170 | Accuracy: 0.324219 | 0.104 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 021 | Total loss: 9.153 | Reg loss: 0.009 | Tree loss: 9.153 | Accuracy: 0.330078 | 0.104 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 021 | Total loss: 9.139 | Reg loss: 0.009 | Tree loss: 9.139 | Accuracy: 0.375000 | 0.104 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 021 | Total loss: 9.131 | Reg loss: 0.010 | Tree loss: 9.131 | Accuracy: 0.341797 | 0.104 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 021 | Total loss: 9.113 | Reg loss: 0.010 | Tree loss: 9.113 | Accuracy: 0.375000 | 0.104 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 021 | Total loss: 9.108 | Reg loss: 0.010 | Tree loss: 9.108 | Accuracy: 0.341797 | 0.104 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 021 | Total loss: 9.088 | Reg loss: 0.010 | Tree loss: 9.088 | Accuracy: 0.349609 | 0.104 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 021 | Total loss: 9.079 | Reg loss: 0.010 | Tree loss: 9.079 | Accuracy: 0.376953 | 0.103 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 021 | Total loss: 9.058 | Reg loss: 0.011 | Tree loss: 9.058 | Accuracy: 0.376953 | 0.103 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 021 | Total loss: 9.049 | Reg loss: 0.011 | Tree loss: 9.049 | Accuracy: 0.373047 | 0.103 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 021 | Total loss: 9.039 | Reg loss: 0.011 | Tree loss: 9.039 | Accuracy: 0.349609 | 0.103 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 021 | Total loss: 9.025 | Reg loss: 0.011 | Tree loss: 9.025 | Accuracy: 0.355469 | 0.103 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 021 | Total loss: 9.005 | Reg loss: 0.012 | Tree loss: 9.005 | Accuracy: 0.382812 | 0.103 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 021 | Total loss: 8.996 | Reg loss: 0.012 | Tree loss: 8.996 | Accuracy: 0.373047 | 0.104 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 021 | Total loss: 8.979 | Reg loss: 0.012 | Tree loss: 8.979 | Accuracy: 0.392578 | 0.104 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 021 | Total loss: 8.962 | Reg loss: 0.013 | Tree loss: 8.962 | Accuracy: 0.417969 | 0.103 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 021 | Total loss: 8.953 | Reg loss: 0.013 | Tree loss: 8.953 | Accuracy: 0.341797 | 0.104 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 021 | Total loss: 8.940 | Reg loss: 0.013 | Tree loss: 8.940 | Accuracy: 0.353516 | 0.104 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 021 | Total loss: 8.923 | Reg loss: 0.014 | Tree loss: 8.923 | Accuracy: 0.371094 | 0.104 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 021 | Total loss: 8.909 | Reg loss: 0.014 | Tree loss: 8.909 | Accuracy: 0.402344 | 0.104 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 021 | Total loss: 8.911 | Reg loss: 0.014 | Tree loss: 8.911 | Accuracy: 0.274809 | 0.104 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Batch: 000 / 021 | Total loss: 9.043 | Reg loss: 0.011 | Tree loss: 9.043 | Accuracy: 0.376953 | 0.104 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 021 | Total loss: 9.036 | Reg loss: 0.011 | Tree loss: 9.036 | Accuracy: 0.345703 | 0.104 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 021 | Total loss: 9.022 | Reg loss: 0.011 | Tree loss: 9.022 | Accuracy: 0.349609 | 0.104 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 021 | Total loss: 8.997 | Reg loss: 0.011 | Tree loss: 8.997 | Accuracy: 0.414062 | 0.104 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 021 | Total loss: 8.988 | Reg loss: 0.011 | Tree loss: 8.988 | Accuracy: 0.371094 | 0.104 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 021 | Total loss: 8.987 | Reg loss: 0.011 | Tree loss: 8.987 | Accuracy: 0.330078 | 0.104 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 021 | Total loss: 8.973 | Reg loss: 0.011 | Tree loss: 8.973 | Accuracy: 0.335938 | 0.104 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 021 | Total loss: 8.956 | Reg loss: 0.012 | Tree loss: 8.956 | Accuracy: 0.353516 | 0.104 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 021 | Total loss: 8.935 | Reg loss: 0.012 | Tree loss: 8.935 | Accuracy: 0.404297 | 0.104 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 021 | Total loss: 8.929 | Reg loss: 0.012 | Tree loss: 8.929 | Accuracy: 0.343750 | 0.104 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 021 | Total loss: 8.908 | Reg loss: 0.012 | Tree loss: 8.908 | Accuracy: 0.390625 | 0.103 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 021 | Total loss: 8.903 | Reg loss: 0.013 | Tree loss: 8.903 | Accuracy: 0.332031 | 0.103 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 021 | Total loss: 8.886 | Reg loss: 0.013 | Tree loss: 8.886 | Accuracy: 0.388672 | 0.103 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 021 | Total loss: 8.870 | Reg loss: 0.013 | Tree loss: 8.870 | Accuracy: 0.359375 | 0.103 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 021 | Total loss: 8.856 | Reg loss: 0.013 | Tree loss: 8.856 | Accuracy: 0.371094 | 0.103 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 021 | Total loss: 8.841 | Reg loss: 0.014 | Tree loss: 8.841 | Accuracy: 0.376953 | 0.103 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 021 | Total loss: 8.829 | Reg loss: 0.014 | Tree loss: 8.829 | Accuracy: 0.384766 | 0.103 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 021 | Total loss: 8.811 | Reg loss: 0.014 | Tree loss: 8.811 | Accuracy: 0.347656 | 0.103 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 021 | Total loss: 8.792 | Reg loss: 0.015 | Tree loss: 8.792 | Accuracy: 0.394531 | 0.103 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 021 | Total loss: 8.775 | Reg loss: 0.015 | Tree loss: 8.775 | Accuracy: 0.394531 | 0.103 sec/iter\n",
      "Epoch: 06 | Batch: 020 / 021 | Total loss: 8.756 | Reg loss: 0.015 | Tree loss: 8.756 | Accuracy: 0.351145 | 0.103 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 07 | Batch: 000 / 021 | Total loss: 8.920 | Reg loss: 0.012 | Tree loss: 8.920 | Accuracy: 0.367188 | 0.104 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 021 | Total loss: 8.911 | Reg loss: 0.012 | Tree loss: 8.911 | Accuracy: 0.392578 | 0.104 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 021 | Total loss: 8.898 | Reg loss: 0.012 | Tree loss: 8.898 | Accuracy: 0.378906 | 0.104 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 021 | Total loss: 8.883 | Reg loss: 0.012 | Tree loss: 8.883 | Accuracy: 0.357422 | 0.104 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 021 | Total loss: 8.869 | Reg loss: 0.012 | Tree loss: 8.869 | Accuracy: 0.359375 | 0.104 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 021 | Total loss: 8.853 | Reg loss: 0.013 | Tree loss: 8.853 | Accuracy: 0.376953 | 0.103 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 021 | Total loss: 8.842 | Reg loss: 0.013 | Tree loss: 8.842 | Accuracy: 0.365234 | 0.103 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 021 | Total loss: 8.821 | Reg loss: 0.013 | Tree loss: 8.821 | Accuracy: 0.371094 | 0.103 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 021 | Total loss: 8.814 | Reg loss: 0.013 | Tree loss: 8.814 | Accuracy: 0.373047 | 0.103 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 021 | Total loss: 8.796 | Reg loss: 0.013 | Tree loss: 8.796 | Accuracy: 0.388672 | 0.103 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 021 | Total loss: 8.792 | Reg loss: 0.014 | Tree loss: 8.792 | Accuracy: 0.351562 | 0.103 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 021 | Total loss: 8.763 | Reg loss: 0.014 | Tree loss: 8.763 | Accuracy: 0.402344 | 0.103 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 021 | Total loss: 8.762 | Reg loss: 0.014 | Tree loss: 8.762 | Accuracy: 0.332031 | 0.103 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 021 | Total loss: 8.740 | Reg loss: 0.014 | Tree loss: 8.740 | Accuracy: 0.378906 | 0.103 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 021 | Total loss: 8.723 | Reg loss: 0.015 | Tree loss: 8.723 | Accuracy: 0.367188 | 0.103 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 021 | Total loss: 8.715 | Reg loss: 0.015 | Tree loss: 8.715 | Accuracy: 0.353516 | 0.103 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 021 | Total loss: 8.697 | Reg loss: 0.015 | Tree loss: 8.697 | Accuracy: 0.361328 | 0.103 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 021 | Total loss: 8.685 | Reg loss: 0.016 | Tree loss: 8.685 | Accuracy: 0.363281 | 0.103 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 021 | Total loss: 8.673 | Reg loss: 0.016 | Tree loss: 8.673 | Accuracy: 0.357422 | 0.103 sec/iter\n",
      "Epoch: 07 | Batch: 019 / 021 | Total loss: 8.644 | Reg loss: 0.016 | Tree loss: 8.644 | Accuracy: 0.388672 | 0.103 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 021 | Total loss: 8.647 | Reg loss: 0.017 | Tree loss: 8.647 | Accuracy: 0.366412 | 0.103 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 08 | Batch: 000 / 021 | Total loss: 8.808 | Reg loss: 0.013 | Tree loss: 8.808 | Accuracy: 0.326172 | 0.104 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 021 | Total loss: 8.781 | Reg loss: 0.013 | Tree loss: 8.781 | Accuracy: 0.396484 | 0.104 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 021 | Total loss: 8.777 | Reg loss: 0.014 | Tree loss: 8.777 | Accuracy: 0.351562 | 0.104 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 021 | Total loss: 8.756 | Reg loss: 0.014 | Tree loss: 8.756 | Accuracy: 0.390625 | 0.103 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 021 | Total loss: 8.742 | Reg loss: 0.014 | Tree loss: 8.742 | Accuracy: 0.375000 | 0.103 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 021 | Total loss: 8.713 | Reg loss: 0.014 | Tree loss: 8.713 | Accuracy: 0.398438 | 0.103 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 021 | Total loss: 8.721 | Reg loss: 0.014 | Tree loss: 8.721 | Accuracy: 0.341797 | 0.103 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 021 | Total loss: 8.696 | Reg loss: 0.014 | Tree loss: 8.696 | Accuracy: 0.375000 | 0.103 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 021 | Total loss: 8.683 | Reg loss: 0.014 | Tree loss: 8.683 | Accuracy: 0.363281 | 0.103 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 021 | Total loss: 8.661 | Reg loss: 0.015 | Tree loss: 8.661 | Accuracy: 0.373047 | 0.103 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 021 | Total loss: 8.654 | Reg loss: 0.015 | Tree loss: 8.654 | Accuracy: 0.361328 | 0.103 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 021 | Total loss: 8.633 | Reg loss: 0.015 | Tree loss: 8.633 | Accuracy: 0.361328 | 0.103 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 021 | Total loss: 8.626 | Reg loss: 0.015 | Tree loss: 8.626 | Accuracy: 0.388672 | 0.103 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 021 | Total loss: 8.609 | Reg loss: 0.016 | Tree loss: 8.609 | Accuracy: 0.392578 | 0.103 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 021 | Total loss: 8.601 | Reg loss: 0.016 | Tree loss: 8.601 | Accuracy: 0.373047 | 0.103 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 021 | Total loss: 8.587 | Reg loss: 0.016 | Tree loss: 8.587 | Accuracy: 0.341797 | 0.103 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 021 | Total loss: 8.554 | Reg loss: 0.016 | Tree loss: 8.554 | Accuracy: 0.400391 | 0.103 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 021 | Total loss: 8.559 | Reg loss: 0.017 | Tree loss: 8.559 | Accuracy: 0.349609 | 0.103 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 021 | Total loss: 8.539 | Reg loss: 0.017 | Tree loss: 8.539 | Accuracy: 0.375000 | 0.102 sec/iter\n",
      "Epoch: 08 | Batch: 019 / 021 | Total loss: 8.529 | Reg loss: 0.017 | Tree loss: 8.529 | Accuracy: 0.369141 | 0.102 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 021 | Total loss: 8.489 | Reg loss: 0.018 | Tree loss: 8.489 | Accuracy: 0.442748 | 0.102 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 09 | Batch: 000 / 021 | Total loss: 8.676 | Reg loss: 0.015 | Tree loss: 8.676 | Accuracy: 0.361328 | 0.103 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Batch: 001 / 021 | Total loss: 8.653 | Reg loss: 0.015 | Tree loss: 8.653 | Accuracy: 0.386719 | 0.103 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 021 | Total loss: 8.640 | Reg loss: 0.015 | Tree loss: 8.640 | Accuracy: 0.367188 | 0.103 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 021 | Total loss: 8.628 | Reg loss: 0.015 | Tree loss: 8.628 | Accuracy: 0.341797 | 0.103 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 021 | Total loss: 8.610 | Reg loss: 0.015 | Tree loss: 8.610 | Accuracy: 0.392578 | 0.103 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 021 | Total loss: 8.588 | Reg loss: 0.015 | Tree loss: 8.588 | Accuracy: 0.380859 | 0.103 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 021 | Total loss: 8.590 | Reg loss: 0.015 | Tree loss: 8.590 | Accuracy: 0.355469 | 0.103 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 021 | Total loss: 8.575 | Reg loss: 0.015 | Tree loss: 8.575 | Accuracy: 0.341797 | 0.103 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 021 | Total loss: 8.558 | Reg loss: 0.016 | Tree loss: 8.558 | Accuracy: 0.365234 | 0.103 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 021 | Total loss: 8.528 | Reg loss: 0.016 | Tree loss: 8.528 | Accuracy: 0.384766 | 0.102 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 021 | Total loss: 8.520 | Reg loss: 0.016 | Tree loss: 8.520 | Accuracy: 0.369141 | 0.102 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 021 | Total loss: 8.506 | Reg loss: 0.016 | Tree loss: 8.506 | Accuracy: 0.392578 | 0.102 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 021 | Total loss: 8.481 | Reg loss: 0.016 | Tree loss: 8.481 | Accuracy: 0.398438 | 0.102 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 021 | Total loss: 8.472 | Reg loss: 0.017 | Tree loss: 8.472 | Accuracy: 0.388672 | 0.102 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 021 | Total loss: 8.473 | Reg loss: 0.017 | Tree loss: 8.473 | Accuracy: 0.337891 | 0.102 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 021 | Total loss: 8.452 | Reg loss: 0.017 | Tree loss: 8.452 | Accuracy: 0.369141 | 0.102 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 021 | Total loss: 8.434 | Reg loss: 0.017 | Tree loss: 8.434 | Accuracy: 0.380859 | 0.102 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 021 | Total loss: 8.412 | Reg loss: 0.018 | Tree loss: 8.412 | Accuracy: 0.394531 | 0.102 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 021 | Total loss: 8.408 | Reg loss: 0.018 | Tree loss: 8.408 | Accuracy: 0.353516 | 0.101 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 021 | Total loss: 8.379 | Reg loss: 0.018 | Tree loss: 8.379 | Accuracy: 0.378906 | 0.101 sec/iter\n",
      "Epoch: 09 | Batch: 020 / 021 | Total loss: 8.385 | Reg loss: 0.019 | Tree loss: 8.385 | Accuracy: 0.328244 | 0.101 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 10 | Batch: 000 / 021 | Total loss: 8.542 | Reg loss: 0.016 | Tree loss: 8.542 | Accuracy: 0.376953 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 021 | Total loss: 8.522 | Reg loss: 0.016 | Tree loss: 8.522 | Accuracy: 0.408203 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 021 | Total loss: 8.518 | Reg loss: 0.016 | Tree loss: 8.518 | Accuracy: 0.380859 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 021 | Total loss: 8.495 | Reg loss: 0.016 | Tree loss: 8.495 | Accuracy: 0.367188 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 021 | Total loss: 8.492 | Reg loss: 0.016 | Tree loss: 8.492 | Accuracy: 0.326172 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 021 | Total loss: 8.461 | Reg loss: 0.016 | Tree loss: 8.461 | Accuracy: 0.378906 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 021 | Total loss: 8.442 | Reg loss: 0.016 | Tree loss: 8.442 | Accuracy: 0.369141 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 021 | Total loss: 8.439 | Reg loss: 0.016 | Tree loss: 8.439 | Accuracy: 0.363281 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 021 | Total loss: 8.413 | Reg loss: 0.017 | Tree loss: 8.413 | Accuracy: 0.390625 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 021 | Total loss: 8.397 | Reg loss: 0.017 | Tree loss: 8.397 | Accuracy: 0.388672 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 021 | Total loss: 8.387 | Reg loss: 0.017 | Tree loss: 8.387 | Accuracy: 0.361328 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 021 | Total loss: 8.381 | Reg loss: 0.017 | Tree loss: 8.381 | Accuracy: 0.357422 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 021 | Total loss: 8.349 | Reg loss: 0.017 | Tree loss: 8.349 | Accuracy: 0.367188 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 021 | Total loss: 8.340 | Reg loss: 0.018 | Tree loss: 8.340 | Accuracy: 0.386719 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 021 | Total loss: 8.328 | Reg loss: 0.018 | Tree loss: 8.328 | Accuracy: 0.361328 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 021 | Total loss: 8.321 | Reg loss: 0.018 | Tree loss: 8.321 | Accuracy: 0.335938 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 021 | Total loss: 8.291 | Reg loss: 0.018 | Tree loss: 8.291 | Accuracy: 0.373047 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 021 | Total loss: 8.270 | Reg loss: 0.019 | Tree loss: 8.270 | Accuracy: 0.406250 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 021 | Total loss: 8.265 | Reg loss: 0.019 | Tree loss: 8.265 | Accuracy: 0.384766 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 019 / 021 | Total loss: 8.255 | Reg loss: 0.019 | Tree loss: 8.255 | Accuracy: 0.353516 | 0.102 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 021 | Total loss: 8.218 | Reg loss: 0.020 | Tree loss: 8.218 | Accuracy: 0.374046 | 0.102 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 11 | Batch: 000 / 021 | Total loss: 8.406 | Reg loss: 0.017 | Tree loss: 8.406 | Accuracy: 0.406250 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 021 | Total loss: 8.378 | Reg loss: 0.017 | Tree loss: 8.378 | Accuracy: 0.445312 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 021 | Total loss: 8.385 | Reg loss: 0.017 | Tree loss: 8.385 | Accuracy: 0.351562 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 021 | Total loss: 8.373 | Reg loss: 0.017 | Tree loss: 8.373 | Accuracy: 0.355469 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 021 | Total loss: 8.347 | Reg loss: 0.017 | Tree loss: 8.347 | Accuracy: 0.384766 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 021 | Total loss: 8.328 | Reg loss: 0.017 | Tree loss: 8.328 | Accuracy: 0.367188 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 021 | Total loss: 8.314 | Reg loss: 0.017 | Tree loss: 8.314 | Accuracy: 0.332031 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 021 | Total loss: 8.293 | Reg loss: 0.017 | Tree loss: 8.293 | Accuracy: 0.382812 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 021 | Total loss: 8.288 | Reg loss: 0.018 | Tree loss: 8.288 | Accuracy: 0.353516 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 021 | Total loss: 8.265 | Reg loss: 0.018 | Tree loss: 8.265 | Accuracy: 0.386719 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 021 | Total loss: 8.257 | Reg loss: 0.018 | Tree loss: 8.257 | Accuracy: 0.375000 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 021 | Total loss: 8.233 | Reg loss: 0.018 | Tree loss: 8.233 | Accuracy: 0.373047 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 021 | Total loss: 8.216 | Reg loss: 0.018 | Tree loss: 8.216 | Accuracy: 0.378906 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 021 | Total loss: 8.214 | Reg loss: 0.019 | Tree loss: 8.214 | Accuracy: 0.324219 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 021 | Total loss: 8.168 | Reg loss: 0.019 | Tree loss: 8.168 | Accuracy: 0.414062 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 021 | Total loss: 8.174 | Reg loss: 0.019 | Tree loss: 8.174 | Accuracy: 0.349609 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 021 | Total loss: 8.147 | Reg loss: 0.020 | Tree loss: 8.147 | Accuracy: 0.369141 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 021 | Total loss: 8.147 | Reg loss: 0.020 | Tree loss: 8.147 | Accuracy: 0.351562 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 021 | Total loss: 8.119 | Reg loss: 0.020 | Tree loss: 8.119 | Accuracy: 0.367188 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 021 | Total loss: 8.097 | Reg loss: 0.020 | Tree loss: 8.097 | Accuracy: 0.380859 | 0.102 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 021 | Total loss: 8.116 | Reg loss: 0.021 | Tree loss: 8.116 | Accuracy: 0.320611 | 0.102 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Batch: 000 / 021 | Total loss: 8.283 | Reg loss: 0.018 | Tree loss: 8.283 | Accuracy: 0.396484 | 0.103 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 021 | Total loss: 8.252 | Reg loss: 0.018 | Tree loss: 8.252 | Accuracy: 0.398438 | 0.103 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 021 | Total loss: 8.250 | Reg loss: 0.018 | Tree loss: 8.250 | Accuracy: 0.375000 | 0.103 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 021 | Total loss: 8.225 | Reg loss: 0.018 | Tree loss: 8.225 | Accuracy: 0.359375 | 0.102 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 021 | Total loss: 8.202 | Reg loss: 0.018 | Tree loss: 8.202 | Accuracy: 0.373047 | 0.102 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 021 | Total loss: 8.204 | Reg loss: 0.018 | Tree loss: 8.204 | Accuracy: 0.353516 | 0.102 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 021 | Total loss: 8.177 | Reg loss: 0.018 | Tree loss: 8.177 | Accuracy: 0.369141 | 0.102 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 021 | Total loss: 8.175 | Reg loss: 0.018 | Tree loss: 8.175 | Accuracy: 0.333984 | 0.102 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 021 | Total loss: 8.145 | Reg loss: 0.019 | Tree loss: 8.145 | Accuracy: 0.386719 | 0.102 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 021 | Total loss: 8.141 | Reg loss: 0.019 | Tree loss: 8.141 | Accuracy: 0.322266 | 0.102 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 021 | Total loss: 8.108 | Reg loss: 0.019 | Tree loss: 8.108 | Accuracy: 0.380859 | 0.102 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 021 | Total loss: 8.090 | Reg loss: 0.019 | Tree loss: 8.090 | Accuracy: 0.355469 | 0.102 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 021 | Total loss: 8.069 | Reg loss: 0.020 | Tree loss: 8.069 | Accuracy: 0.384766 | 0.102 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 021 | Total loss: 8.053 | Reg loss: 0.020 | Tree loss: 8.053 | Accuracy: 0.378906 | 0.102 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 021 | Total loss: 8.041 | Reg loss: 0.020 | Tree loss: 8.041 | Accuracy: 0.367188 | 0.101 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 021 | Total loss: 8.028 | Reg loss: 0.020 | Tree loss: 8.028 | Accuracy: 0.386719 | 0.101 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 021 | Total loss: 8.009 | Reg loss: 0.021 | Tree loss: 8.009 | Accuracy: 0.376953 | 0.101 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 021 | Total loss: 7.978 | Reg loss: 0.021 | Tree loss: 7.978 | Accuracy: 0.369141 | 0.101 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 021 | Total loss: 7.979 | Reg loss: 0.021 | Tree loss: 7.979 | Accuracy: 0.375000 | 0.101 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 021 | Total loss: 7.926 | Reg loss: 0.022 | Tree loss: 7.926 | Accuracy: 0.392578 | 0.101 sec/iter\n",
      "Epoch: 12 | Batch: 020 / 021 | Total loss: 7.942 | Reg loss: 0.022 | Tree loss: 7.942 | Accuracy: 0.351145 | 0.101 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 13 | Batch: 000 / 021 | Total loss: 8.138 | Reg loss: 0.019 | Tree loss: 8.138 | Accuracy: 0.351562 | 0.101 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 021 | Total loss: 8.106 | Reg loss: 0.019 | Tree loss: 8.106 | Accuracy: 0.421875 | 0.101 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 021 | Total loss: 8.102 | Reg loss: 0.019 | Tree loss: 8.102 | Accuracy: 0.400391 | 0.101 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 021 | Total loss: 8.085 | Reg loss: 0.019 | Tree loss: 8.085 | Accuracy: 0.376953 | 0.101 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 021 | Total loss: 8.072 | Reg loss: 0.019 | Tree loss: 8.072 | Accuracy: 0.361328 | 0.101 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 021 | Total loss: 8.063 | Reg loss: 0.019 | Tree loss: 8.063 | Accuracy: 0.371094 | 0.101 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 021 | Total loss: 8.032 | Reg loss: 0.019 | Tree loss: 8.032 | Accuracy: 0.373047 | 0.101 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 021 | Total loss: 8.006 | Reg loss: 0.020 | Tree loss: 8.006 | Accuracy: 0.380859 | 0.101 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 021 | Total loss: 8.000 | Reg loss: 0.020 | Tree loss: 8.000 | Accuracy: 0.369141 | 0.101 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 021 | Total loss: 7.972 | Reg loss: 0.020 | Tree loss: 7.972 | Accuracy: 0.363281 | 0.101 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 021 | Total loss: 7.968 | Reg loss: 0.020 | Tree loss: 7.968 | Accuracy: 0.363281 | 0.101 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 021 | Total loss: 7.948 | Reg loss: 0.021 | Tree loss: 7.948 | Accuracy: 0.376953 | 0.101 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 021 | Total loss: 7.943 | Reg loss: 0.021 | Tree loss: 7.943 | Accuracy: 0.363281 | 0.101 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 021 | Total loss: 7.914 | Reg loss: 0.021 | Tree loss: 7.914 | Accuracy: 0.376953 | 0.102 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 021 | Total loss: 7.909 | Reg loss: 0.022 | Tree loss: 7.909 | Accuracy: 0.341797 | 0.102 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 021 | Total loss: 7.894 | Reg loss: 0.022 | Tree loss: 7.894 | Accuracy: 0.357422 | 0.102 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 021 | Total loss: 7.834 | Reg loss: 0.022 | Tree loss: 7.834 | Accuracy: 0.404297 | 0.102 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 021 | Total loss: 7.846 | Reg loss: 0.022 | Tree loss: 7.846 | Accuracy: 0.341797 | 0.102 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 021 | Total loss: 7.828 | Reg loss: 0.023 | Tree loss: 7.828 | Accuracy: 0.357422 | 0.102 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 021 | Total loss: 7.789 | Reg loss: 0.023 | Tree loss: 7.789 | Accuracy: 0.375000 | 0.102 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 021 | Total loss: 7.772 | Reg loss: 0.023 | Tree loss: 7.772 | Accuracy: 0.389313 | 0.102 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 14 | Batch: 000 / 021 | Total loss: 8.009 | Reg loss: 0.020 | Tree loss: 8.009 | Accuracy: 0.369141 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 021 | Total loss: 7.984 | Reg loss: 0.020 | Tree loss: 7.984 | Accuracy: 0.396484 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 021 | Total loss: 7.975 | Reg loss: 0.020 | Tree loss: 7.975 | Accuracy: 0.347656 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 021 | Total loss: 7.944 | Reg loss: 0.020 | Tree loss: 7.944 | Accuracy: 0.396484 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 021 | Total loss: 7.931 | Reg loss: 0.020 | Tree loss: 7.931 | Accuracy: 0.359375 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 021 | Total loss: 7.917 | Reg loss: 0.020 | Tree loss: 7.917 | Accuracy: 0.382812 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 021 | Total loss: 7.896 | Reg loss: 0.021 | Tree loss: 7.896 | Accuracy: 0.375000 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 021 | Total loss: 7.873 | Reg loss: 0.021 | Tree loss: 7.873 | Accuracy: 0.347656 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 021 | Total loss: 7.863 | Reg loss: 0.021 | Tree loss: 7.863 | Accuracy: 0.357422 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 021 | Total loss: 7.843 | Reg loss: 0.021 | Tree loss: 7.843 | Accuracy: 0.341797 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 021 | Total loss: 7.824 | Reg loss: 0.022 | Tree loss: 7.824 | Accuracy: 0.359375 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 021 | Total loss: 7.787 | Reg loss: 0.022 | Tree loss: 7.787 | Accuracy: 0.371094 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 021 | Total loss: 7.774 | Reg loss: 0.022 | Tree loss: 7.774 | Accuracy: 0.373047 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 021 | Total loss: 7.761 | Reg loss: 0.023 | Tree loss: 7.761 | Accuracy: 0.384766 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 021 | Total loss: 7.741 | Reg loss: 0.023 | Tree loss: 7.741 | Accuracy: 0.343750 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 021 | Total loss: 7.688 | Reg loss: 0.023 | Tree loss: 7.688 | Accuracy: 0.445312 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 021 | Total loss: 7.695 | Reg loss: 0.023 | Tree loss: 7.695 | Accuracy: 0.384766 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 021 | Total loss: 7.684 | Reg loss: 0.024 | Tree loss: 7.684 | Accuracy: 0.365234 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 021 | Total loss: 7.681 | Reg loss: 0.024 | Tree loss: 7.681 | Accuracy: 0.355469 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 021 | Total loss: 7.656 | Reg loss: 0.024 | Tree loss: 7.656 | Accuracy: 0.365234 | 0.102 sec/iter\n",
      "Epoch: 14 | Batch: 020 / 021 | Total loss: 7.601 | Reg loss: 0.025 | Tree loss: 7.601 | Accuracy: 0.412214 | 0.102 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Batch: 000 / 021 | Total loss: 7.873 | Reg loss: 0.021 | Tree loss: 7.873 | Accuracy: 0.335938 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 021 | Total loss: 7.853 | Reg loss: 0.021 | Tree loss: 7.853 | Accuracy: 0.363281 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 021 | Total loss: 7.821 | Reg loss: 0.021 | Tree loss: 7.821 | Accuracy: 0.382812 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 021 | Total loss: 7.824 | Reg loss: 0.021 | Tree loss: 7.824 | Accuracy: 0.367188 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 021 | Total loss: 7.790 | Reg loss: 0.022 | Tree loss: 7.790 | Accuracy: 0.375000 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 021 | Total loss: 7.776 | Reg loss: 0.022 | Tree loss: 7.776 | Accuracy: 0.376953 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 021 | Total loss: 7.751 | Reg loss: 0.022 | Tree loss: 7.751 | Accuracy: 0.349609 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 021 | Total loss: 7.759 | Reg loss: 0.022 | Tree loss: 7.759 | Accuracy: 0.349609 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 021 | Total loss: 7.695 | Reg loss: 0.022 | Tree loss: 7.695 | Accuracy: 0.388672 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 021 | Total loss: 7.699 | Reg loss: 0.023 | Tree loss: 7.699 | Accuracy: 0.382812 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 021 | Total loss: 7.664 | Reg loss: 0.023 | Tree loss: 7.664 | Accuracy: 0.392578 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 021 | Total loss: 7.642 | Reg loss: 0.023 | Tree loss: 7.642 | Accuracy: 0.386719 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 021 | Total loss: 7.629 | Reg loss: 0.023 | Tree loss: 7.629 | Accuracy: 0.369141 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 021 | Total loss: 7.602 | Reg loss: 0.024 | Tree loss: 7.602 | Accuracy: 0.386719 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 021 | Total loss: 7.596 | Reg loss: 0.024 | Tree loss: 7.596 | Accuracy: 0.375000 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 021 | Total loss: 7.591 | Reg loss: 0.024 | Tree loss: 7.591 | Accuracy: 0.330078 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 021 | Total loss: 7.521 | Reg loss: 0.025 | Tree loss: 7.521 | Accuracy: 0.417969 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 021 | Total loss: 7.529 | Reg loss: 0.025 | Tree loss: 7.529 | Accuracy: 0.384766 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 021 | Total loss: 7.510 | Reg loss: 0.025 | Tree loss: 7.510 | Accuracy: 0.375000 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 021 | Total loss: 7.512 | Reg loss: 0.026 | Tree loss: 7.512 | Accuracy: 0.347656 | 0.103 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 021 | Total loss: 7.443 | Reg loss: 0.026 | Tree loss: 7.443 | Accuracy: 0.351145 | 0.103 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 16 | Batch: 000 / 021 | Total loss: 7.744 | Reg loss: 0.022 | Tree loss: 7.744 | Accuracy: 0.367188 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 021 | Total loss: 7.734 | Reg loss: 0.023 | Tree loss: 7.734 | Accuracy: 0.367188 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 021 | Total loss: 7.689 | Reg loss: 0.023 | Tree loss: 7.689 | Accuracy: 0.373047 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 021 | Total loss: 7.665 | Reg loss: 0.023 | Tree loss: 7.665 | Accuracy: 0.390625 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 021 | Total loss: 7.655 | Reg loss: 0.023 | Tree loss: 7.655 | Accuracy: 0.371094 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 021 | Total loss: 7.633 | Reg loss: 0.023 | Tree loss: 7.633 | Accuracy: 0.357422 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 021 | Total loss: 7.575 | Reg loss: 0.023 | Tree loss: 7.575 | Accuracy: 0.404297 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 021 | Total loss: 7.615 | Reg loss: 0.023 | Tree loss: 7.615 | Accuracy: 0.341797 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 021 | Total loss: 7.568 | Reg loss: 0.024 | Tree loss: 7.568 | Accuracy: 0.382812 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 021 | Total loss: 7.541 | Reg loss: 0.024 | Tree loss: 7.541 | Accuracy: 0.382812 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 021 | Total loss: 7.527 | Reg loss: 0.024 | Tree loss: 7.527 | Accuracy: 0.378906 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 021 | Total loss: 7.513 | Reg loss: 0.024 | Tree loss: 7.513 | Accuracy: 0.359375 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 021 | Total loss: 7.478 | Reg loss: 0.025 | Tree loss: 7.478 | Accuracy: 0.365234 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 021 | Total loss: 7.501 | Reg loss: 0.025 | Tree loss: 7.501 | Accuracy: 0.351562 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 021 | Total loss: 7.438 | Reg loss: 0.025 | Tree loss: 7.438 | Accuracy: 0.376953 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 021 | Total loss: 7.400 | Reg loss: 0.025 | Tree loss: 7.400 | Accuracy: 0.406250 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 021 | Total loss: 7.410 | Reg loss: 0.026 | Tree loss: 7.410 | Accuracy: 0.382812 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 021 | Total loss: 7.402 | Reg loss: 0.026 | Tree loss: 7.402 | Accuracy: 0.359375 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 021 | Total loss: 7.351 | Reg loss: 0.026 | Tree loss: 7.351 | Accuracy: 0.367188 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 021 | Total loss: 7.348 | Reg loss: 0.027 | Tree loss: 7.348 | Accuracy: 0.343750 | 0.103 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 021 | Total loss: 7.332 | Reg loss: 0.027 | Tree loss: 7.332 | Accuracy: 0.389313 | 0.103 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 17 | Batch: 000 / 021 | Total loss: 7.598 | Reg loss: 0.024 | Tree loss: 7.598 | Accuracy: 0.378906 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 021 | Total loss: 7.564 | Reg loss: 0.024 | Tree loss: 7.564 | Accuracy: 0.380859 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 021 | Total loss: 7.534 | Reg loss: 0.024 | Tree loss: 7.534 | Accuracy: 0.396484 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 021 | Total loss: 7.527 | Reg loss: 0.024 | Tree loss: 7.527 | Accuracy: 0.375000 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 021 | Total loss: 7.505 | Reg loss: 0.024 | Tree loss: 7.505 | Accuracy: 0.382812 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 021 | Total loss: 7.500 | Reg loss: 0.024 | Tree loss: 7.500 | Accuracy: 0.365234 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 021 | Total loss: 7.491 | Reg loss: 0.024 | Tree loss: 7.491 | Accuracy: 0.347656 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 021 | Total loss: 7.443 | Reg loss: 0.025 | Tree loss: 7.443 | Accuracy: 0.396484 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 021 | Total loss: 7.439 | Reg loss: 0.025 | Tree loss: 7.439 | Accuracy: 0.345703 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 021 | Total loss: 7.418 | Reg loss: 0.025 | Tree loss: 7.418 | Accuracy: 0.363281 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 021 | Total loss: 7.396 | Reg loss: 0.025 | Tree loss: 7.396 | Accuracy: 0.369141 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 021 | Total loss: 7.353 | Reg loss: 0.025 | Tree loss: 7.353 | Accuracy: 0.396484 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 021 | Total loss: 7.360 | Reg loss: 0.026 | Tree loss: 7.360 | Accuracy: 0.347656 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 021 | Total loss: 7.327 | Reg loss: 0.026 | Tree loss: 7.327 | Accuracy: 0.408203 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 021 | Total loss: 7.298 | Reg loss: 0.026 | Tree loss: 7.298 | Accuracy: 0.357422 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 021 | Total loss: 7.284 | Reg loss: 0.026 | Tree loss: 7.284 | Accuracy: 0.378906 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 021 | Total loss: 7.261 | Reg loss: 0.027 | Tree loss: 7.261 | Accuracy: 0.361328 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 021 | Total loss: 7.258 | Reg loss: 0.027 | Tree loss: 7.258 | Accuracy: 0.343750 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 021 | Total loss: 7.222 | Reg loss: 0.027 | Tree loss: 7.222 | Accuracy: 0.365234 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 021 | Total loss: 7.209 | Reg loss: 0.028 | Tree loss: 7.209 | Accuracy: 0.361328 | 0.104 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 021 | Total loss: 7.215 | Reg loss: 0.028 | Tree loss: 7.215 | Accuracy: 0.412214 | 0.104 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 18 | Batch: 000 / 021 | Total loss: 7.474 | Reg loss: 0.025 | Tree loss: 7.474 | Accuracy: 0.375000 | 0.104 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Batch: 001 / 021 | Total loss: 7.468 | Reg loss: 0.025 | Tree loss: 7.468 | Accuracy: 0.375000 | 0.104 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 021 | Total loss: 7.427 | Reg loss: 0.025 | Tree loss: 7.427 | Accuracy: 0.355469 | 0.104 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 021 | Total loss: 7.436 | Reg loss: 0.025 | Tree loss: 7.436 | Accuracy: 0.349609 | 0.104 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 021 | Total loss: 7.377 | Reg loss: 0.025 | Tree loss: 7.377 | Accuracy: 0.378906 | 0.104 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 021 | Total loss: 7.369 | Reg loss: 0.025 | Tree loss: 7.369 | Accuracy: 0.355469 | 0.104 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 021 | Total loss: 7.331 | Reg loss: 0.025 | Tree loss: 7.331 | Accuracy: 0.371094 | 0.104 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 021 | Total loss: 7.321 | Reg loss: 0.026 | Tree loss: 7.321 | Accuracy: 0.390625 | 0.104 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 021 | Total loss: 7.291 | Reg loss: 0.026 | Tree loss: 7.291 | Accuracy: 0.335938 | 0.104 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 021 | Total loss: 7.277 | Reg loss: 0.026 | Tree loss: 7.277 | Accuracy: 0.380859 | 0.104 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 021 | Total loss: 7.276 | Reg loss: 0.026 | Tree loss: 7.276 | Accuracy: 0.351562 | 0.104 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 021 | Total loss: 7.227 | Reg loss: 0.026 | Tree loss: 7.227 | Accuracy: 0.380859 | 0.104 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 021 | Total loss: 7.185 | Reg loss: 0.027 | Tree loss: 7.185 | Accuracy: 0.386719 | 0.104 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 021 | Total loss: 7.184 | Reg loss: 0.027 | Tree loss: 7.184 | Accuracy: 0.355469 | 0.104 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 021 | Total loss: 7.163 | Reg loss: 0.027 | Tree loss: 7.163 | Accuracy: 0.376953 | 0.104 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 021 | Total loss: 7.138 | Reg loss: 0.027 | Tree loss: 7.138 | Accuracy: 0.390625 | 0.105 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 021 | Total loss: 7.093 | Reg loss: 0.028 | Tree loss: 7.093 | Accuracy: 0.386719 | 0.105 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 021 | Total loss: 7.063 | Reg loss: 0.028 | Tree loss: 7.063 | Accuracy: 0.402344 | 0.105 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 021 | Total loss: 7.063 | Reg loss: 0.028 | Tree loss: 7.063 | Accuracy: 0.386719 | 0.105 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 021 | Total loss: 7.082 | Reg loss: 0.029 | Tree loss: 7.082 | Accuracy: 0.357422 | 0.105 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 021 | Total loss: 7.077 | Reg loss: 0.029 | Tree loss: 7.077 | Accuracy: 0.328244 | 0.105 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 19 | Batch: 000 / 021 | Total loss: 7.321 | Reg loss: 0.026 | Tree loss: 7.321 | Accuracy: 0.386719 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 021 | Total loss: 7.305 | Reg loss: 0.026 | Tree loss: 7.305 | Accuracy: 0.369141 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 021 | Total loss: 7.297 | Reg loss: 0.026 | Tree loss: 7.297 | Accuracy: 0.378906 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 021 | Total loss: 7.242 | Reg loss: 0.026 | Tree loss: 7.242 | Accuracy: 0.408203 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 021 | Total loss: 7.279 | Reg loss: 0.026 | Tree loss: 7.279 | Accuracy: 0.357422 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 021 | Total loss: 7.218 | Reg loss: 0.026 | Tree loss: 7.218 | Accuracy: 0.402344 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 021 | Total loss: 7.207 | Reg loss: 0.026 | Tree loss: 7.207 | Accuracy: 0.394531 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 021 | Total loss: 7.209 | Reg loss: 0.027 | Tree loss: 7.209 | Accuracy: 0.339844 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 021 | Total loss: 7.160 | Reg loss: 0.027 | Tree loss: 7.160 | Accuracy: 0.382812 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 021 | Total loss: 7.161 | Reg loss: 0.027 | Tree loss: 7.161 | Accuracy: 0.365234 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 021 | Total loss: 7.118 | Reg loss: 0.027 | Tree loss: 7.118 | Accuracy: 0.357422 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 021 | Total loss: 7.101 | Reg loss: 0.027 | Tree loss: 7.101 | Accuracy: 0.355469 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 021 | Total loss: 7.060 | Reg loss: 0.028 | Tree loss: 7.060 | Accuracy: 0.365234 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 021 | Total loss: 7.050 | Reg loss: 0.028 | Tree loss: 7.050 | Accuracy: 0.351562 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 021 | Total loss: 7.031 | Reg loss: 0.028 | Tree loss: 7.031 | Accuracy: 0.351562 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 021 | Total loss: 6.988 | Reg loss: 0.028 | Tree loss: 6.988 | Accuracy: 0.376953 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 021 | Total loss: 6.982 | Reg loss: 0.029 | Tree loss: 6.982 | Accuracy: 0.369141 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 021 | Total loss: 6.963 | Reg loss: 0.029 | Tree loss: 6.963 | Accuracy: 0.369141 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 021 | Total loss: 6.969 | Reg loss: 0.029 | Tree loss: 6.969 | Accuracy: 0.355469 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 021 | Total loss: 6.912 | Reg loss: 0.029 | Tree loss: 6.912 | Accuracy: 0.382812 | 0.105 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 021 | Total loss: 6.865 | Reg loss: 0.030 | Tree loss: 6.865 | Accuracy: 0.427481 | 0.105 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 20 | Batch: 000 / 021 | Total loss: 7.184 | Reg loss: 0.027 | Tree loss: 7.184 | Accuracy: 0.400391 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 021 | Total loss: 7.189 | Reg loss: 0.027 | Tree loss: 7.189 | Accuracy: 0.378906 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 021 | Total loss: 7.159 | Reg loss: 0.027 | Tree loss: 7.159 | Accuracy: 0.353516 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 021 | Total loss: 7.163 | Reg loss: 0.027 | Tree loss: 7.163 | Accuracy: 0.359375 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 021 | Total loss: 7.117 | Reg loss: 0.027 | Tree loss: 7.117 | Accuracy: 0.378906 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 021 | Total loss: 7.095 | Reg loss: 0.027 | Tree loss: 7.095 | Accuracy: 0.394531 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 021 | Total loss: 7.054 | Reg loss: 0.027 | Tree loss: 7.054 | Accuracy: 0.390625 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 021 | Total loss: 7.051 | Reg loss: 0.028 | Tree loss: 7.051 | Accuracy: 0.376953 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 021 | Total loss: 7.001 | Reg loss: 0.028 | Tree loss: 7.001 | Accuracy: 0.359375 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 021 | Total loss: 7.011 | Reg loss: 0.028 | Tree loss: 7.011 | Accuracy: 0.349609 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 021 | Total loss: 6.979 | Reg loss: 0.028 | Tree loss: 6.979 | Accuracy: 0.363281 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 021 | Total loss: 6.988 | Reg loss: 0.028 | Tree loss: 6.988 | Accuracy: 0.376953 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 021 | Total loss: 6.942 | Reg loss: 0.028 | Tree loss: 6.942 | Accuracy: 0.382812 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 021 | Total loss: 6.905 | Reg loss: 0.029 | Tree loss: 6.905 | Accuracy: 0.394531 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 021 | Total loss: 6.910 | Reg loss: 0.029 | Tree loss: 6.910 | Accuracy: 0.375000 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 021 | Total loss: 6.855 | Reg loss: 0.029 | Tree loss: 6.855 | Accuracy: 0.371094 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 021 | Total loss: 6.865 | Reg loss: 0.029 | Tree loss: 6.865 | Accuracy: 0.335938 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 021 | Total loss: 6.845 | Reg loss: 0.030 | Tree loss: 6.845 | Accuracy: 0.351562 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 021 | Total loss: 6.829 | Reg loss: 0.030 | Tree loss: 6.829 | Accuracy: 0.347656 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 019 / 021 | Total loss: 6.766 | Reg loss: 0.030 | Tree loss: 6.766 | Accuracy: 0.386719 | 0.105 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 021 | Total loss: 6.796 | Reg loss: 0.030 | Tree loss: 6.796 | Accuracy: 0.389313 | 0.105 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 21 | Batch: 000 / 021 | Total loss: 7.076 | Reg loss: 0.028 | Tree loss: 7.076 | Accuracy: 0.355469 | 0.105 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Batch: 001 / 021 | Total loss: 7.033 | Reg loss: 0.028 | Tree loss: 7.033 | Accuracy: 0.419922 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 021 | Total loss: 7.028 | Reg loss: 0.028 | Tree loss: 7.028 | Accuracy: 0.365234 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 021 | Total loss: 7.005 | Reg loss: 0.028 | Tree loss: 7.005 | Accuracy: 0.367188 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 021 | Total loss: 7.035 | Reg loss: 0.028 | Tree loss: 7.035 | Accuracy: 0.324219 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 021 | Total loss: 6.990 | Reg loss: 0.028 | Tree loss: 6.990 | Accuracy: 0.343750 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 021 | Total loss: 6.963 | Reg loss: 0.028 | Tree loss: 6.963 | Accuracy: 0.363281 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 021 | Total loss: 6.919 | Reg loss: 0.028 | Tree loss: 6.919 | Accuracy: 0.337891 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 021 | Total loss: 6.922 | Reg loss: 0.029 | Tree loss: 6.922 | Accuracy: 0.376953 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 021 | Total loss: 6.890 | Reg loss: 0.029 | Tree loss: 6.890 | Accuracy: 0.333984 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 021 | Total loss: 6.823 | Reg loss: 0.029 | Tree loss: 6.823 | Accuracy: 0.408203 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 021 | Total loss: 6.820 | Reg loss: 0.029 | Tree loss: 6.820 | Accuracy: 0.386719 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 021 | Total loss: 6.780 | Reg loss: 0.029 | Tree loss: 6.780 | Accuracy: 0.388672 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 021 | Total loss: 6.766 | Reg loss: 0.030 | Tree loss: 6.766 | Accuracy: 0.392578 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 021 | Total loss: 6.785 | Reg loss: 0.030 | Tree loss: 6.785 | Accuracy: 0.371094 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 021 | Total loss: 6.745 | Reg loss: 0.030 | Tree loss: 6.745 | Accuracy: 0.382812 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 021 | Total loss: 6.725 | Reg loss: 0.030 | Tree loss: 6.725 | Accuracy: 0.367188 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 021 | Total loss: 6.678 | Reg loss: 0.030 | Tree loss: 6.678 | Accuracy: 0.386719 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 021 | Total loss: 6.675 | Reg loss: 0.031 | Tree loss: 6.675 | Accuracy: 0.375000 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 021 | Total loss: 6.668 | Reg loss: 0.031 | Tree loss: 6.668 | Accuracy: 0.373047 | 0.105 sec/iter\n",
      "Epoch: 21 | Batch: 020 / 021 | Total loss: 6.599 | Reg loss: 0.031 | Tree loss: 6.599 | Accuracy: 0.419847 | 0.105 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 22 | Batch: 000 / 021 | Total loss: 6.963 | Reg loss: 0.029 | Tree loss: 6.963 | Accuracy: 0.363281 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 021 | Total loss: 6.939 | Reg loss: 0.029 | Tree loss: 6.939 | Accuracy: 0.369141 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 021 | Total loss: 6.909 | Reg loss: 0.029 | Tree loss: 6.909 | Accuracy: 0.402344 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 021 | Total loss: 6.883 | Reg loss: 0.029 | Tree loss: 6.883 | Accuracy: 0.361328 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 021 | Total loss: 6.842 | Reg loss: 0.029 | Tree loss: 6.842 | Accuracy: 0.406250 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 021 | Total loss: 6.832 | Reg loss: 0.029 | Tree loss: 6.832 | Accuracy: 0.376953 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 021 | Total loss: 6.845 | Reg loss: 0.029 | Tree loss: 6.845 | Accuracy: 0.351562 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 021 | Total loss: 6.810 | Reg loss: 0.029 | Tree loss: 6.810 | Accuracy: 0.373047 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 021 | Total loss: 6.774 | Reg loss: 0.029 | Tree loss: 6.774 | Accuracy: 0.396484 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 021 | Total loss: 6.714 | Reg loss: 0.030 | Tree loss: 6.714 | Accuracy: 0.380859 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 021 | Total loss: 6.742 | Reg loss: 0.030 | Tree loss: 6.742 | Accuracy: 0.345703 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 021 | Total loss: 6.688 | Reg loss: 0.030 | Tree loss: 6.688 | Accuracy: 0.378906 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 021 | Total loss: 6.686 | Reg loss: 0.030 | Tree loss: 6.686 | Accuracy: 0.355469 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 021 | Total loss: 6.645 | Reg loss: 0.030 | Tree loss: 6.645 | Accuracy: 0.357422 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 021 | Total loss: 6.660 | Reg loss: 0.031 | Tree loss: 6.660 | Accuracy: 0.347656 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 021 | Total loss: 6.621 | Reg loss: 0.031 | Tree loss: 6.621 | Accuracy: 0.373047 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 021 | Total loss: 6.555 | Reg loss: 0.031 | Tree loss: 6.555 | Accuracy: 0.400391 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 021 | Total loss: 6.537 | Reg loss: 0.031 | Tree loss: 6.537 | Accuracy: 0.361328 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 021 | Total loss: 6.547 | Reg loss: 0.031 | Tree loss: 6.547 | Accuracy: 0.365234 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 021 | Total loss: 6.531 | Reg loss: 0.032 | Tree loss: 6.531 | Accuracy: 0.363281 | 0.105 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 021 | Total loss: 6.554 | Reg loss: 0.032 | Tree loss: 6.554 | Accuracy: 0.381679 | 0.105 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 23 | Batch: 000 / 021 | Total loss: 6.842 | Reg loss: 0.029 | Tree loss: 6.842 | Accuracy: 0.373047 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 021 | Total loss: 6.804 | Reg loss: 0.029 | Tree loss: 6.804 | Accuracy: 0.380859 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 021 | Total loss: 6.764 | Reg loss: 0.030 | Tree loss: 6.764 | Accuracy: 0.369141 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 021 | Total loss: 6.766 | Reg loss: 0.030 | Tree loss: 6.766 | Accuracy: 0.369141 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 021 | Total loss: 6.755 | Reg loss: 0.030 | Tree loss: 6.755 | Accuracy: 0.371094 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 021 | Total loss: 6.691 | Reg loss: 0.030 | Tree loss: 6.691 | Accuracy: 0.408203 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 021 | Total loss: 6.684 | Reg loss: 0.030 | Tree loss: 6.684 | Accuracy: 0.357422 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 021 | Total loss: 6.655 | Reg loss: 0.030 | Tree loss: 6.655 | Accuracy: 0.363281 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 021 | Total loss: 6.657 | Reg loss: 0.030 | Tree loss: 6.657 | Accuracy: 0.361328 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 021 | Total loss: 6.602 | Reg loss: 0.030 | Tree loss: 6.602 | Accuracy: 0.378906 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 021 | Total loss: 6.603 | Reg loss: 0.030 | Tree loss: 6.603 | Accuracy: 0.361328 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 021 | Total loss: 6.571 | Reg loss: 0.031 | Tree loss: 6.571 | Accuracy: 0.390625 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 021 | Total loss: 6.564 | Reg loss: 0.031 | Tree loss: 6.564 | Accuracy: 0.376953 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 021 | Total loss: 6.502 | Reg loss: 0.031 | Tree loss: 6.502 | Accuracy: 0.392578 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 021 | Total loss: 6.504 | Reg loss: 0.031 | Tree loss: 6.504 | Accuracy: 0.398438 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 021 | Total loss: 6.502 | Reg loss: 0.031 | Tree loss: 6.502 | Accuracy: 0.375000 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 021 | Total loss: 6.475 | Reg loss: 0.032 | Tree loss: 6.475 | Accuracy: 0.324219 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 021 | Total loss: 6.444 | Reg loss: 0.032 | Tree loss: 6.444 | Accuracy: 0.322266 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 021 | Total loss: 6.431 | Reg loss: 0.032 | Tree loss: 6.431 | Accuracy: 0.384766 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 021 | Total loss: 6.383 | Reg loss: 0.032 | Tree loss: 6.383 | Accuracy: 0.380859 | 0.105 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 021 | Total loss: 6.411 | Reg loss: 0.033 | Tree loss: 6.411 | Accuracy: 0.343511 | 0.105 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Batch: 000 / 021 | Total loss: 6.694 | Reg loss: 0.030 | Tree loss: 6.694 | Accuracy: 0.353516 | 0.105 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 021 | Total loss: 6.699 | Reg loss: 0.030 | Tree loss: 6.699 | Accuracy: 0.388672 | 0.105 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 021 | Total loss: 6.617 | Reg loss: 0.030 | Tree loss: 6.617 | Accuracy: 0.425781 | 0.105 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 021 | Total loss: 6.634 | Reg loss: 0.030 | Tree loss: 6.634 | Accuracy: 0.361328 | 0.105 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 021 | Total loss: 6.627 | Reg loss: 0.030 | Tree loss: 6.627 | Accuracy: 0.392578 | 0.105 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 021 | Total loss: 6.614 | Reg loss: 0.031 | Tree loss: 6.614 | Accuracy: 0.347656 | 0.105 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 021 | Total loss: 6.567 | Reg loss: 0.031 | Tree loss: 6.567 | Accuracy: 0.394531 | 0.105 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 021 | Total loss: 6.545 | Reg loss: 0.031 | Tree loss: 6.545 | Accuracy: 0.339844 | 0.105 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 021 | Total loss: 6.502 | Reg loss: 0.031 | Tree loss: 6.502 | Accuracy: 0.369141 | 0.105 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 021 | Total loss: 6.481 | Reg loss: 0.031 | Tree loss: 6.481 | Accuracy: 0.410156 | 0.105 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 021 | Total loss: 6.479 | Reg loss: 0.031 | Tree loss: 6.479 | Accuracy: 0.388672 | 0.105 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 021 | Total loss: 6.466 | Reg loss: 0.031 | Tree loss: 6.466 | Accuracy: 0.380859 | 0.105 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 021 | Total loss: 6.461 | Reg loss: 0.032 | Tree loss: 6.461 | Accuracy: 0.347656 | 0.105 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 021 | Total loss: 6.421 | Reg loss: 0.032 | Tree loss: 6.421 | Accuracy: 0.347656 | 0.105 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 021 | Total loss: 6.389 | Reg loss: 0.032 | Tree loss: 6.389 | Accuracy: 0.369141 | 0.105 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 021 | Total loss: 6.333 | Reg loss: 0.032 | Tree loss: 6.333 | Accuracy: 0.384766 | 0.106 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 021 | Total loss: 6.348 | Reg loss: 0.032 | Tree loss: 6.348 | Accuracy: 0.332031 | 0.106 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 021 | Total loss: 6.297 | Reg loss: 0.033 | Tree loss: 6.297 | Accuracy: 0.361328 | 0.106 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 021 | Total loss: 6.293 | Reg loss: 0.033 | Tree loss: 6.293 | Accuracy: 0.365234 | 0.106 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 021 | Total loss: 6.254 | Reg loss: 0.033 | Tree loss: 6.254 | Accuracy: 0.373047 | 0.106 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 021 | Total loss: 6.219 | Reg loss: 0.033 | Tree loss: 6.219 | Accuracy: 0.366412 | 0.106 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 25 | Batch: 000 / 021 | Total loss: 6.578 | Reg loss: 0.031 | Tree loss: 6.578 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 021 | Total loss: 6.575 | Reg loss: 0.031 | Tree loss: 6.575 | Accuracy: 0.361328 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 021 | Total loss: 6.543 | Reg loss: 0.031 | Tree loss: 6.543 | Accuracy: 0.373047 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 021 | Total loss: 6.512 | Reg loss: 0.031 | Tree loss: 6.512 | Accuracy: 0.361328 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 021 | Total loss: 6.464 | Reg loss: 0.031 | Tree loss: 6.464 | Accuracy: 0.365234 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 021 | Total loss: 6.492 | Reg loss: 0.031 | Tree loss: 6.492 | Accuracy: 0.363281 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 021 | Total loss: 6.409 | Reg loss: 0.031 | Tree loss: 6.409 | Accuracy: 0.378906 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 021 | Total loss: 6.420 | Reg loss: 0.031 | Tree loss: 6.420 | Accuracy: 0.386719 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 021 | Total loss: 6.378 | Reg loss: 0.032 | Tree loss: 6.378 | Accuracy: 0.378906 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 021 | Total loss: 6.357 | Reg loss: 0.032 | Tree loss: 6.357 | Accuracy: 0.392578 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 021 | Total loss: 6.372 | Reg loss: 0.032 | Tree loss: 6.372 | Accuracy: 0.371094 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 021 | Total loss: 6.344 | Reg loss: 0.032 | Tree loss: 6.344 | Accuracy: 0.361328 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 021 | Total loss: 6.301 | Reg loss: 0.032 | Tree loss: 6.301 | Accuracy: 0.351562 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 021 | Total loss: 6.281 | Reg loss: 0.032 | Tree loss: 6.281 | Accuracy: 0.378906 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 021 | Total loss: 6.271 | Reg loss: 0.033 | Tree loss: 6.271 | Accuracy: 0.353516 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 021 | Total loss: 6.255 | Reg loss: 0.033 | Tree loss: 6.255 | Accuracy: 0.363281 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 021 | Total loss: 6.201 | Reg loss: 0.033 | Tree loss: 6.201 | Accuracy: 0.371094 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 021 | Total loss: 6.173 | Reg loss: 0.033 | Tree loss: 6.173 | Accuracy: 0.376953 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 021 | Total loss: 6.184 | Reg loss: 0.033 | Tree loss: 6.184 | Accuracy: 0.371094 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 021 | Total loss: 6.125 | Reg loss: 0.033 | Tree loss: 6.125 | Accuracy: 0.404297 | 0.106 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 021 | Total loss: 6.125 | Reg loss: 0.034 | Tree loss: 6.125 | Accuracy: 0.366412 | 0.106 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 26 | Batch: 000 / 021 | Total loss: 6.410 | Reg loss: 0.032 | Tree loss: 6.410 | Accuracy: 0.404297 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 021 | Total loss: 6.425 | Reg loss: 0.032 | Tree loss: 6.425 | Accuracy: 0.357422 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 021 | Total loss: 6.417 | Reg loss: 0.032 | Tree loss: 6.417 | Accuracy: 0.400391 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 021 | Total loss: 6.389 | Reg loss: 0.032 | Tree loss: 6.389 | Accuracy: 0.408203 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 021 | Total loss: 6.371 | Reg loss: 0.032 | Tree loss: 6.371 | Accuracy: 0.384766 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 021 | Total loss: 6.341 | Reg loss: 0.032 | Tree loss: 6.341 | Accuracy: 0.367188 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 021 | Total loss: 6.335 | Reg loss: 0.032 | Tree loss: 6.335 | Accuracy: 0.367188 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 021 | Total loss: 6.297 | Reg loss: 0.032 | Tree loss: 6.297 | Accuracy: 0.394531 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 021 | Total loss: 6.294 | Reg loss: 0.032 | Tree loss: 6.294 | Accuracy: 0.335938 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 021 | Total loss: 6.233 | Reg loss: 0.032 | Tree loss: 6.233 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 021 | Total loss: 6.236 | Reg loss: 0.032 | Tree loss: 6.236 | Accuracy: 0.376953 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 021 | Total loss: 6.159 | Reg loss: 0.033 | Tree loss: 6.159 | Accuracy: 0.361328 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 021 | Total loss: 6.213 | Reg loss: 0.033 | Tree loss: 6.213 | Accuracy: 0.353516 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 021 | Total loss: 6.172 | Reg loss: 0.033 | Tree loss: 6.172 | Accuracy: 0.339844 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 021 | Total loss: 6.134 | Reg loss: 0.033 | Tree loss: 6.134 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 021 | Total loss: 6.108 | Reg loss: 0.033 | Tree loss: 6.108 | Accuracy: 0.349609 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 021 | Total loss: 6.103 | Reg loss: 0.033 | Tree loss: 6.103 | Accuracy: 0.345703 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 021 | Total loss: 6.059 | Reg loss: 0.034 | Tree loss: 6.059 | Accuracy: 0.388672 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 021 | Total loss: 6.075 | Reg loss: 0.034 | Tree loss: 6.075 | Accuracy: 0.343750 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 021 | Total loss: 6.024 | Reg loss: 0.034 | Tree loss: 6.024 | Accuracy: 0.400391 | 0.106 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 021 | Total loss: 5.958 | Reg loss: 0.034 | Tree loss: 5.958 | Accuracy: 0.427481 | 0.106 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Batch: 000 / 021 | Total loss: 6.337 | Reg loss: 0.032 | Tree loss: 6.337 | Accuracy: 0.376953 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 021 | Total loss: 6.324 | Reg loss: 0.032 | Tree loss: 6.324 | Accuracy: 0.365234 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 021 | Total loss: 6.308 | Reg loss: 0.032 | Tree loss: 6.308 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 021 | Total loss: 6.274 | Reg loss: 0.032 | Tree loss: 6.274 | Accuracy: 0.353516 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 021 | Total loss: 6.232 | Reg loss: 0.032 | Tree loss: 6.232 | Accuracy: 0.394531 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 021 | Total loss: 6.254 | Reg loss: 0.032 | Tree loss: 6.254 | Accuracy: 0.351562 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 021 | Total loss: 6.199 | Reg loss: 0.032 | Tree loss: 6.199 | Accuracy: 0.363281 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 021 | Total loss: 6.178 | Reg loss: 0.033 | Tree loss: 6.178 | Accuracy: 0.373047 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 021 | Total loss: 6.133 | Reg loss: 0.033 | Tree loss: 6.133 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 021 | Total loss: 6.109 | Reg loss: 0.033 | Tree loss: 6.109 | Accuracy: 0.394531 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 021 | Total loss: 6.114 | Reg loss: 0.033 | Tree loss: 6.114 | Accuracy: 0.384766 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 021 | Total loss: 6.082 | Reg loss: 0.033 | Tree loss: 6.082 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 021 | Total loss: 6.105 | Reg loss: 0.033 | Tree loss: 6.105 | Accuracy: 0.332031 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 021 | Total loss: 6.010 | Reg loss: 0.033 | Tree loss: 6.010 | Accuracy: 0.382812 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 021 | Total loss: 6.023 | Reg loss: 0.034 | Tree loss: 6.023 | Accuracy: 0.363281 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 021 | Total loss: 5.965 | Reg loss: 0.034 | Tree loss: 5.965 | Accuracy: 0.414062 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 021 | Total loss: 5.993 | Reg loss: 0.034 | Tree loss: 5.993 | Accuracy: 0.339844 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 021 | Total loss: 5.929 | Reg loss: 0.034 | Tree loss: 5.929 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 021 | Total loss: 5.941 | Reg loss: 0.034 | Tree loss: 5.941 | Accuracy: 0.347656 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 021 | Total loss: 5.875 | Reg loss: 0.034 | Tree loss: 5.875 | Accuracy: 0.410156 | 0.106 sec/iter\n",
      "Epoch: 27 | Batch: 020 / 021 | Total loss: 5.820 | Reg loss: 0.035 | Tree loss: 5.820 | Accuracy: 0.404580 | 0.106 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 28 | Batch: 000 / 021 | Total loss: 6.184 | Reg loss: 0.033 | Tree loss: 6.184 | Accuracy: 0.404297 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 021 | Total loss: 6.166 | Reg loss: 0.033 | Tree loss: 6.166 | Accuracy: 0.392578 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 021 | Total loss: 6.206 | Reg loss: 0.033 | Tree loss: 6.206 | Accuracy: 0.333984 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 021 | Total loss: 6.147 | Reg loss: 0.033 | Tree loss: 6.147 | Accuracy: 0.355469 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 021 | Total loss: 6.120 | Reg loss: 0.033 | Tree loss: 6.120 | Accuracy: 0.363281 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 021 | Total loss: 6.108 | Reg loss: 0.033 | Tree loss: 6.108 | Accuracy: 0.339844 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 021 | Total loss: 6.078 | Reg loss: 0.033 | Tree loss: 6.078 | Accuracy: 0.416016 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 021 | Total loss: 6.053 | Reg loss: 0.033 | Tree loss: 6.053 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 021 | Total loss: 6.018 | Reg loss: 0.033 | Tree loss: 6.018 | Accuracy: 0.396484 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 021 | Total loss: 6.024 | Reg loss: 0.033 | Tree loss: 6.024 | Accuracy: 0.357422 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 021 | Total loss: 6.009 | Reg loss: 0.033 | Tree loss: 6.009 | Accuracy: 0.343750 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 021 | Total loss: 5.954 | Reg loss: 0.034 | Tree loss: 5.954 | Accuracy: 0.373047 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 021 | Total loss: 5.954 | Reg loss: 0.034 | Tree loss: 5.954 | Accuracy: 0.386719 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 021 | Total loss: 5.910 | Reg loss: 0.034 | Tree loss: 5.910 | Accuracy: 0.386719 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 021 | Total loss: 5.871 | Reg loss: 0.034 | Tree loss: 5.871 | Accuracy: 0.380859 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 021 | Total loss: 5.885 | Reg loss: 0.034 | Tree loss: 5.885 | Accuracy: 0.347656 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 021 | Total loss: 5.877 | Reg loss: 0.034 | Tree loss: 5.877 | Accuracy: 0.351562 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 021 | Total loss: 5.813 | Reg loss: 0.035 | Tree loss: 5.813 | Accuracy: 0.371094 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 021 | Total loss: 5.824 | Reg loss: 0.035 | Tree loss: 5.824 | Accuracy: 0.357422 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 021 | Total loss: 5.747 | Reg loss: 0.035 | Tree loss: 5.747 | Accuracy: 0.396484 | 0.106 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 021 | Total loss: 5.806 | Reg loss: 0.035 | Tree loss: 5.806 | Accuracy: 0.404580 | 0.106 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 29 | Batch: 000 / 021 | Total loss: 6.088 | Reg loss: 0.033 | Tree loss: 6.088 | Accuracy: 0.378906 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 021 | Total loss: 6.091 | Reg loss: 0.033 | Tree loss: 6.091 | Accuracy: 0.355469 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 021 | Total loss: 6.070 | Reg loss: 0.033 | Tree loss: 6.070 | Accuracy: 0.388672 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 021 | Total loss: 6.000 | Reg loss: 0.033 | Tree loss: 6.000 | Accuracy: 0.394531 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 021 | Total loss: 6.007 | Reg loss: 0.033 | Tree loss: 6.007 | Accuracy: 0.386719 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 021 | Total loss: 5.999 | Reg loss: 0.033 | Tree loss: 5.999 | Accuracy: 0.353516 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 021 | Total loss: 5.958 | Reg loss: 0.034 | Tree loss: 5.958 | Accuracy: 0.365234 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 021 | Total loss: 5.922 | Reg loss: 0.034 | Tree loss: 5.922 | Accuracy: 0.425781 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 021 | Total loss: 5.915 | Reg loss: 0.034 | Tree loss: 5.915 | Accuracy: 0.375000 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 021 | Total loss: 5.886 | Reg loss: 0.034 | Tree loss: 5.886 | Accuracy: 0.363281 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 021 | Total loss: 5.899 | Reg loss: 0.034 | Tree loss: 5.899 | Accuracy: 0.373047 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 021 | Total loss: 5.824 | Reg loss: 0.034 | Tree loss: 5.824 | Accuracy: 0.357422 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 021 | Total loss: 5.835 | Reg loss: 0.034 | Tree loss: 5.835 | Accuracy: 0.384766 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 021 | Total loss: 5.771 | Reg loss: 0.034 | Tree loss: 5.771 | Accuracy: 0.361328 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 021 | Total loss: 5.741 | Reg loss: 0.035 | Tree loss: 5.741 | Accuracy: 0.396484 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 021 | Total loss: 5.787 | Reg loss: 0.035 | Tree loss: 5.787 | Accuracy: 0.341797 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 021 | Total loss: 5.740 | Reg loss: 0.035 | Tree loss: 5.740 | Accuracy: 0.335938 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 021 | Total loss: 5.719 | Reg loss: 0.035 | Tree loss: 5.719 | Accuracy: 0.330078 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 021 | Total loss: 5.659 | Reg loss: 0.035 | Tree loss: 5.659 | Accuracy: 0.382812 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 021 | Total loss: 5.655 | Reg loss: 0.035 | Tree loss: 5.655 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 021 | Total loss: 5.590 | Reg loss: 0.036 | Tree loss: 5.590 | Accuracy: 0.419847 | 0.106 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 30 | Batch: 000 / 021 | Total loss: 5.961 | Reg loss: 0.034 | Tree loss: 5.961 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 021 | Total loss: 5.967 | Reg loss: 0.034 | Tree loss: 5.967 | Accuracy: 0.378906 | 0.106 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Batch: 002 / 021 | Total loss: 5.925 | Reg loss: 0.034 | Tree loss: 5.925 | Accuracy: 0.371094 | 0.106 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 021 | Total loss: 5.896 | Reg loss: 0.034 | Tree loss: 5.896 | Accuracy: 0.375000 | 0.106 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 021 | Total loss: 5.903 | Reg loss: 0.034 | Tree loss: 5.903 | Accuracy: 0.361328 | 0.106 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 021 | Total loss: 5.876 | Reg loss: 0.034 | Tree loss: 5.876 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 021 | Total loss: 5.846 | Reg loss: 0.034 | Tree loss: 5.846 | Accuracy: 0.359375 | 0.105 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 021 | Total loss: 5.815 | Reg loss: 0.034 | Tree loss: 5.815 | Accuracy: 0.396484 | 0.105 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 021 | Total loss: 5.791 | Reg loss: 0.034 | Tree loss: 5.791 | Accuracy: 0.355469 | 0.105 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 021 | Total loss: 5.790 | Reg loss: 0.034 | Tree loss: 5.790 | Accuracy: 0.382812 | 0.105 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 021 | Total loss: 5.758 | Reg loss: 0.034 | Tree loss: 5.758 | Accuracy: 0.392578 | 0.105 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 021 | Total loss: 5.690 | Reg loss: 0.035 | Tree loss: 5.690 | Accuracy: 0.382812 | 0.105 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 021 | Total loss: 5.673 | Reg loss: 0.035 | Tree loss: 5.673 | Accuracy: 0.396484 | 0.105 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 021 | Total loss: 5.677 | Reg loss: 0.035 | Tree loss: 5.677 | Accuracy: 0.349609 | 0.105 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 021 | Total loss: 5.717 | Reg loss: 0.035 | Tree loss: 5.717 | Accuracy: 0.332031 | 0.105 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 021 | Total loss: 5.659 | Reg loss: 0.035 | Tree loss: 5.659 | Accuracy: 0.353516 | 0.105 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 021 | Total loss: 5.586 | Reg loss: 0.035 | Tree loss: 5.586 | Accuracy: 0.384766 | 0.105 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 021 | Total loss: 5.549 | Reg loss: 0.035 | Tree loss: 5.549 | Accuracy: 0.390625 | 0.105 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 021 | Total loss: 5.571 | Reg loss: 0.036 | Tree loss: 5.571 | Accuracy: 0.363281 | 0.105 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 021 | Total loss: 5.529 | Reg loss: 0.036 | Tree loss: 5.529 | Accuracy: 0.376953 | 0.105 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 021 | Total loss: 5.512 | Reg loss: 0.036 | Tree loss: 5.512 | Accuracy: 0.335878 | 0.105 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 31 | Batch: 000 / 021 | Total loss: 5.839 | Reg loss: 0.034 | Tree loss: 5.839 | Accuracy: 0.406250 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 021 | Total loss: 5.860 | Reg loss: 0.034 | Tree loss: 5.860 | Accuracy: 0.343750 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 021 | Total loss: 5.829 | Reg loss: 0.034 | Tree loss: 5.829 | Accuracy: 0.386719 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 021 | Total loss: 5.788 | Reg loss: 0.034 | Tree loss: 5.788 | Accuracy: 0.363281 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 021 | Total loss: 5.756 | Reg loss: 0.034 | Tree loss: 5.756 | Accuracy: 0.398438 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 021 | Total loss: 5.752 | Reg loss: 0.034 | Tree loss: 5.752 | Accuracy: 0.355469 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 021 | Total loss: 5.693 | Reg loss: 0.034 | Tree loss: 5.693 | Accuracy: 0.378906 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 021 | Total loss: 5.704 | Reg loss: 0.035 | Tree loss: 5.704 | Accuracy: 0.337891 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 021 | Total loss: 5.683 | Reg loss: 0.035 | Tree loss: 5.683 | Accuracy: 0.376953 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 021 | Total loss: 5.687 | Reg loss: 0.035 | Tree loss: 5.687 | Accuracy: 0.388672 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 021 | Total loss: 5.618 | Reg loss: 0.035 | Tree loss: 5.618 | Accuracy: 0.371094 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 021 | Total loss: 5.611 | Reg loss: 0.035 | Tree loss: 5.611 | Accuracy: 0.341797 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 021 | Total loss: 5.566 | Reg loss: 0.035 | Tree loss: 5.566 | Accuracy: 0.373047 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 021 | Total loss: 5.540 | Reg loss: 0.035 | Tree loss: 5.540 | Accuracy: 0.384766 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 021 | Total loss: 5.533 | Reg loss: 0.035 | Tree loss: 5.533 | Accuracy: 0.359375 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 021 | Total loss: 5.528 | Reg loss: 0.036 | Tree loss: 5.528 | Accuracy: 0.361328 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 021 | Total loss: 5.469 | Reg loss: 0.036 | Tree loss: 5.469 | Accuracy: 0.380859 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 021 | Total loss: 5.462 | Reg loss: 0.036 | Tree loss: 5.462 | Accuracy: 0.351562 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 021 | Total loss: 5.418 | Reg loss: 0.036 | Tree loss: 5.418 | Accuracy: 0.361328 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 021 | Total loss: 5.412 | Reg loss: 0.036 | Tree loss: 5.412 | Accuracy: 0.404297 | 0.105 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 021 | Total loss: 5.400 | Reg loss: 0.036 | Tree loss: 5.400 | Accuracy: 0.396947 | 0.105 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 32 | Batch: 000 / 021 | Total loss: 5.734 | Reg loss: 0.035 | Tree loss: 5.734 | Accuracy: 0.375000 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 021 | Total loss: 5.677 | Reg loss: 0.035 | Tree loss: 5.677 | Accuracy: 0.376953 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 021 | Total loss: 5.670 | Reg loss: 0.035 | Tree loss: 5.670 | Accuracy: 0.414062 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 021 | Total loss: 5.658 | Reg loss: 0.035 | Tree loss: 5.658 | Accuracy: 0.398438 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 021 | Total loss: 5.665 | Reg loss: 0.035 | Tree loss: 5.665 | Accuracy: 0.349609 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 021 | Total loss: 5.647 | Reg loss: 0.035 | Tree loss: 5.647 | Accuracy: 0.353516 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 021 | Total loss: 5.601 | Reg loss: 0.035 | Tree loss: 5.601 | Accuracy: 0.369141 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 021 | Total loss: 5.591 | Reg loss: 0.035 | Tree loss: 5.591 | Accuracy: 0.371094 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 021 | Total loss: 5.516 | Reg loss: 0.035 | Tree loss: 5.516 | Accuracy: 0.369141 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 021 | Total loss: 5.536 | Reg loss: 0.035 | Tree loss: 5.536 | Accuracy: 0.353516 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 021 | Total loss: 5.467 | Reg loss: 0.035 | Tree loss: 5.467 | Accuracy: 0.416016 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 021 | Total loss: 5.463 | Reg loss: 0.036 | Tree loss: 5.463 | Accuracy: 0.365234 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 021 | Total loss: 5.487 | Reg loss: 0.036 | Tree loss: 5.487 | Accuracy: 0.333984 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 021 | Total loss: 5.435 | Reg loss: 0.036 | Tree loss: 5.435 | Accuracy: 0.351562 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 021 | Total loss: 5.365 | Reg loss: 0.036 | Tree loss: 5.365 | Accuracy: 0.390625 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 021 | Total loss: 5.415 | Reg loss: 0.036 | Tree loss: 5.415 | Accuracy: 0.349609 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 021 | Total loss: 5.392 | Reg loss: 0.036 | Tree loss: 5.392 | Accuracy: 0.382812 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 021 | Total loss: 5.339 | Reg loss: 0.036 | Tree loss: 5.339 | Accuracy: 0.353516 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 021 | Total loss: 5.319 | Reg loss: 0.037 | Tree loss: 5.319 | Accuracy: 0.382812 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 021 | Total loss: 5.299 | Reg loss: 0.037 | Tree loss: 5.299 | Accuracy: 0.365234 | 0.105 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 021 | Total loss: 5.212 | Reg loss: 0.037 | Tree loss: 5.212 | Accuracy: 0.412214 | 0.105 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 33 | Batch: 000 / 021 | Total loss: 5.640 | Reg loss: 0.035 | Tree loss: 5.640 | Accuracy: 0.363281 | 0.105 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | Batch: 001 / 021 | Total loss: 5.587 | Reg loss: 0.035 | Tree loss: 5.587 | Accuracy: 0.369141 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 021 | Total loss: 5.526 | Reg loss: 0.035 | Tree loss: 5.526 | Accuracy: 0.363281 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 021 | Total loss: 5.519 | Reg loss: 0.035 | Tree loss: 5.519 | Accuracy: 0.365234 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 021 | Total loss: 5.508 | Reg loss: 0.035 | Tree loss: 5.508 | Accuracy: 0.378906 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 021 | Total loss: 5.477 | Reg loss: 0.035 | Tree loss: 5.477 | Accuracy: 0.384766 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 021 | Total loss: 5.491 | Reg loss: 0.035 | Tree loss: 5.491 | Accuracy: 0.378906 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 021 | Total loss: 5.425 | Reg loss: 0.036 | Tree loss: 5.425 | Accuracy: 0.367188 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 021 | Total loss: 5.454 | Reg loss: 0.036 | Tree loss: 5.454 | Accuracy: 0.363281 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 021 | Total loss: 5.372 | Reg loss: 0.036 | Tree loss: 5.372 | Accuracy: 0.419922 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 021 | Total loss: 5.369 | Reg loss: 0.036 | Tree loss: 5.369 | Accuracy: 0.388672 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 021 | Total loss: 5.362 | Reg loss: 0.036 | Tree loss: 5.362 | Accuracy: 0.343750 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 021 | Total loss: 5.327 | Reg loss: 0.036 | Tree loss: 5.327 | Accuracy: 0.349609 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 021 | Total loss: 5.310 | Reg loss: 0.036 | Tree loss: 5.310 | Accuracy: 0.376953 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 021 | Total loss: 5.297 | Reg loss: 0.036 | Tree loss: 5.297 | Accuracy: 0.365234 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 021 | Total loss: 5.228 | Reg loss: 0.037 | Tree loss: 5.228 | Accuracy: 0.388672 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 021 | Total loss: 5.219 | Reg loss: 0.037 | Tree loss: 5.219 | Accuracy: 0.386719 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 021 | Total loss: 5.219 | Reg loss: 0.037 | Tree loss: 5.219 | Accuracy: 0.386719 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 021 | Total loss: 5.224 | Reg loss: 0.037 | Tree loss: 5.224 | Accuracy: 0.320312 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 021 | Total loss: 5.181 | Reg loss: 0.037 | Tree loss: 5.181 | Accuracy: 0.369141 | 0.105 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 021 | Total loss: 5.183 | Reg loss: 0.037 | Tree loss: 5.183 | Accuracy: 0.381679 | 0.105 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 34 | Batch: 000 / 021 | Total loss: 5.521 | Reg loss: 0.036 | Tree loss: 5.521 | Accuracy: 0.347656 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 021 | Total loss: 5.445 | Reg loss: 0.036 | Tree loss: 5.445 | Accuracy: 0.392578 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 021 | Total loss: 5.429 | Reg loss: 0.036 | Tree loss: 5.429 | Accuracy: 0.392578 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 021 | Total loss: 5.401 | Reg loss: 0.036 | Tree loss: 5.401 | Accuracy: 0.390625 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 021 | Total loss: 5.340 | Reg loss: 0.036 | Tree loss: 5.340 | Accuracy: 0.402344 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 021 | Total loss: 5.336 | Reg loss: 0.036 | Tree loss: 5.336 | Accuracy: 0.398438 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 021 | Total loss: 5.303 | Reg loss: 0.036 | Tree loss: 5.303 | Accuracy: 0.402344 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 021 | Total loss: 5.364 | Reg loss: 0.036 | Tree loss: 5.364 | Accuracy: 0.339844 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 021 | Total loss: 5.284 | Reg loss: 0.036 | Tree loss: 5.284 | Accuracy: 0.369141 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 021 | Total loss: 5.298 | Reg loss: 0.036 | Tree loss: 5.298 | Accuracy: 0.335938 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 021 | Total loss: 5.201 | Reg loss: 0.036 | Tree loss: 5.201 | Accuracy: 0.394531 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 021 | Total loss: 5.237 | Reg loss: 0.036 | Tree loss: 5.237 | Accuracy: 0.369141 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 021 | Total loss: 5.224 | Reg loss: 0.036 | Tree loss: 5.224 | Accuracy: 0.351562 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 021 | Total loss: 5.181 | Reg loss: 0.037 | Tree loss: 5.181 | Accuracy: 0.375000 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 021 | Total loss: 5.151 | Reg loss: 0.037 | Tree loss: 5.151 | Accuracy: 0.384766 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 021 | Total loss: 5.105 | Reg loss: 0.037 | Tree loss: 5.105 | Accuracy: 0.353516 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 021 | Total loss: 5.095 | Reg loss: 0.037 | Tree loss: 5.095 | Accuracy: 0.378906 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 021 | Total loss: 5.112 | Reg loss: 0.037 | Tree loss: 5.112 | Accuracy: 0.355469 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 018 / 021 | Total loss: 5.097 | Reg loss: 0.037 | Tree loss: 5.097 | Accuracy: 0.337891 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 021 | Total loss: 5.088 | Reg loss: 0.037 | Tree loss: 5.088 | Accuracy: 0.347656 | 0.105 sec/iter\n",
      "Epoch: 34 | Batch: 020 / 021 | Total loss: 4.961 | Reg loss: 0.038 | Tree loss: 4.961 | Accuracy: 0.419847 | 0.105 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 35 | Batch: 000 / 021 | Total loss: 5.339 | Reg loss: 0.036 | Tree loss: 5.339 | Accuracy: 0.378906 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 021 | Total loss: 5.323 | Reg loss: 0.036 | Tree loss: 5.323 | Accuracy: 0.367188 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 021 | Total loss: 5.344 | Reg loss: 0.036 | Tree loss: 5.344 | Accuracy: 0.355469 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 021 | Total loss: 5.282 | Reg loss: 0.036 | Tree loss: 5.282 | Accuracy: 0.332031 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 021 | Total loss: 5.244 | Reg loss: 0.036 | Tree loss: 5.244 | Accuracy: 0.363281 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 021 | Total loss: 5.256 | Reg loss: 0.036 | Tree loss: 5.256 | Accuracy: 0.373047 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 021 | Total loss: 5.153 | Reg loss: 0.036 | Tree loss: 5.153 | Accuracy: 0.421875 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 021 | Total loss: 5.246 | Reg loss: 0.036 | Tree loss: 5.246 | Accuracy: 0.347656 | 0.105 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 021 | Total loss: 5.127 | Reg loss: 0.036 | Tree loss: 5.127 | Accuracy: 0.390625 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 021 | Total loss: 5.150 | Reg loss: 0.036 | Tree loss: 5.150 | Accuracy: 0.380859 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 021 | Total loss: 5.140 | Reg loss: 0.037 | Tree loss: 5.140 | Accuracy: 0.375000 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 021 | Total loss: 5.101 | Reg loss: 0.037 | Tree loss: 5.101 | Accuracy: 0.332031 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 021 | Total loss: 5.103 | Reg loss: 0.037 | Tree loss: 5.103 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 021 | Total loss: 5.008 | Reg loss: 0.037 | Tree loss: 5.008 | Accuracy: 0.380859 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 021 | Total loss: 5.045 | Reg loss: 0.037 | Tree loss: 5.045 | Accuracy: 0.357422 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 021 | Total loss: 4.966 | Reg loss: 0.037 | Tree loss: 4.966 | Accuracy: 0.414062 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 021 | Total loss: 5.005 | Reg loss: 0.037 | Tree loss: 5.005 | Accuracy: 0.371094 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 021 | Total loss: 4.982 | Reg loss: 0.037 | Tree loss: 4.982 | Accuracy: 0.359375 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 018 / 021 | Total loss: 4.918 | Reg loss: 0.037 | Tree loss: 4.918 | Accuracy: 0.392578 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 019 / 021 | Total loss: 4.907 | Reg loss: 0.038 | Tree loss: 4.907 | Accuracy: 0.392578 | 0.106 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 021 | Total loss: 4.943 | Reg loss: 0.038 | Tree loss: 4.943 | Accuracy: 0.282443 | 0.106 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 36 | Batch: 000 / 021 | Total loss: 5.192 | Reg loss: 0.036 | Tree loss: 5.192 | Accuracy: 0.402344 | 0.106 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Batch: 001 / 021 | Total loss: 5.200 | Reg loss: 0.036 | Tree loss: 5.200 | Accuracy: 0.400391 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 021 | Total loss: 5.135 | Reg loss: 0.036 | Tree loss: 5.135 | Accuracy: 0.367188 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 021 | Total loss: 5.153 | Reg loss: 0.036 | Tree loss: 5.153 | Accuracy: 0.388672 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 021 | Total loss: 5.148 | Reg loss: 0.036 | Tree loss: 5.148 | Accuracy: 0.349609 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 021 | Total loss: 5.078 | Reg loss: 0.036 | Tree loss: 5.078 | Accuracy: 0.394531 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 021 | Total loss: 5.080 | Reg loss: 0.036 | Tree loss: 5.080 | Accuracy: 0.378906 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 021 | Total loss: 5.009 | Reg loss: 0.036 | Tree loss: 5.009 | Accuracy: 0.371094 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 021 | Total loss: 5.021 | Reg loss: 0.037 | Tree loss: 5.021 | Accuracy: 0.386719 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 021 | Total loss: 5.047 | Reg loss: 0.037 | Tree loss: 5.047 | Accuracy: 0.337891 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 021 | Total loss: 5.009 | Reg loss: 0.037 | Tree loss: 5.009 | Accuracy: 0.353516 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 021 | Total loss: 4.959 | Reg loss: 0.037 | Tree loss: 4.959 | Accuracy: 0.394531 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 021 | Total loss: 4.955 | Reg loss: 0.037 | Tree loss: 4.955 | Accuracy: 0.357422 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 021 | Total loss: 4.942 | Reg loss: 0.037 | Tree loss: 4.942 | Accuracy: 0.337891 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 021 | Total loss: 4.913 | Reg loss: 0.037 | Tree loss: 4.913 | Accuracy: 0.359375 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 021 | Total loss: 4.919 | Reg loss: 0.037 | Tree loss: 4.919 | Accuracy: 0.373047 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 021 | Total loss: 4.849 | Reg loss: 0.037 | Tree loss: 4.849 | Accuracy: 0.394531 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 021 | Total loss: 4.873 | Reg loss: 0.038 | Tree loss: 4.873 | Accuracy: 0.355469 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 021 | Total loss: 4.821 | Reg loss: 0.038 | Tree loss: 4.821 | Accuracy: 0.371094 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 021 | Total loss: 4.799 | Reg loss: 0.038 | Tree loss: 4.799 | Accuracy: 0.367188 | 0.106 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 021 | Total loss: 4.744 | Reg loss: 0.038 | Tree loss: 4.744 | Accuracy: 0.335878 | 0.106 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 37 | Batch: 000 / 021 | Total loss: 5.098 | Reg loss: 0.036 | Tree loss: 5.098 | Accuracy: 0.384766 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 021 | Total loss: 5.096 | Reg loss: 0.036 | Tree loss: 5.096 | Accuracy: 0.357422 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 021 | Total loss: 5.058 | Reg loss: 0.036 | Tree loss: 5.058 | Accuracy: 0.392578 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 021 | Total loss: 5.027 | Reg loss: 0.036 | Tree loss: 5.027 | Accuracy: 0.359375 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 021 | Total loss: 5.013 | Reg loss: 0.036 | Tree loss: 5.013 | Accuracy: 0.378906 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 021 | Total loss: 4.975 | Reg loss: 0.037 | Tree loss: 4.975 | Accuracy: 0.386719 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 021 | Total loss: 4.954 | Reg loss: 0.037 | Tree loss: 4.954 | Accuracy: 0.355469 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 021 | Total loss: 4.929 | Reg loss: 0.037 | Tree loss: 4.929 | Accuracy: 0.373047 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 021 | Total loss: 4.926 | Reg loss: 0.037 | Tree loss: 4.926 | Accuracy: 0.339844 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 021 | Total loss: 4.876 | Reg loss: 0.037 | Tree loss: 4.876 | Accuracy: 0.375000 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 021 | Total loss: 4.888 | Reg loss: 0.037 | Tree loss: 4.888 | Accuracy: 0.330078 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 021 | Total loss: 4.858 | Reg loss: 0.037 | Tree loss: 4.858 | Accuracy: 0.351562 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 021 | Total loss: 4.761 | Reg loss: 0.037 | Tree loss: 4.761 | Accuracy: 0.392578 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 021 | Total loss: 4.821 | Reg loss: 0.037 | Tree loss: 4.821 | Accuracy: 0.365234 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 021 | Total loss: 4.783 | Reg loss: 0.037 | Tree loss: 4.783 | Accuracy: 0.376953 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 021 | Total loss: 4.725 | Reg loss: 0.037 | Tree loss: 4.725 | Accuracy: 0.396484 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 021 | Total loss: 4.723 | Reg loss: 0.038 | Tree loss: 4.723 | Accuracy: 0.375000 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 021 | Total loss: 4.687 | Reg loss: 0.038 | Tree loss: 4.687 | Accuracy: 0.384766 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 021 | Total loss: 4.654 | Reg loss: 0.038 | Tree loss: 4.654 | Accuracy: 0.373047 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 021 | Total loss: 4.613 | Reg loss: 0.038 | Tree loss: 4.613 | Accuracy: 0.414062 | 0.106 sec/iter\n",
      "Epoch: 37 | Batch: 020 / 021 | Total loss: 4.807 | Reg loss: 0.038 | Tree loss: 4.807 | Accuracy: 0.251908 | 0.106 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 38 | Batch: 000 / 021 | Total loss: 4.934 | Reg loss: 0.036 | Tree loss: 4.934 | Accuracy: 0.382812 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 021 | Total loss: 4.933 | Reg loss: 0.036 | Tree loss: 4.933 | Accuracy: 0.353516 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 021 | Total loss: 4.926 | Reg loss: 0.037 | Tree loss: 4.926 | Accuracy: 0.375000 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 021 | Total loss: 4.884 | Reg loss: 0.037 | Tree loss: 4.884 | Accuracy: 0.396484 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 021 | Total loss: 4.894 | Reg loss: 0.037 | Tree loss: 4.894 | Accuracy: 0.375000 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 021 | Total loss: 4.903 | Reg loss: 0.037 | Tree loss: 4.903 | Accuracy: 0.355469 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 021 | Total loss: 4.833 | Reg loss: 0.037 | Tree loss: 4.833 | Accuracy: 0.349609 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 021 | Total loss: 4.806 | Reg loss: 0.037 | Tree loss: 4.806 | Accuracy: 0.400391 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 021 | Total loss: 4.801 | Reg loss: 0.037 | Tree loss: 4.801 | Accuracy: 0.355469 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 021 | Total loss: 4.749 | Reg loss: 0.037 | Tree loss: 4.749 | Accuracy: 0.361328 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 021 | Total loss: 4.731 | Reg loss: 0.037 | Tree loss: 4.731 | Accuracy: 0.339844 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 021 | Total loss: 4.733 | Reg loss: 0.037 | Tree loss: 4.733 | Accuracy: 0.351562 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 021 | Total loss: 4.678 | Reg loss: 0.037 | Tree loss: 4.678 | Accuracy: 0.382812 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 021 | Total loss: 4.659 | Reg loss: 0.037 | Tree loss: 4.659 | Accuracy: 0.363281 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 021 | Total loss: 4.651 | Reg loss: 0.037 | Tree loss: 4.651 | Accuracy: 0.388672 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 021 | Total loss: 4.563 | Reg loss: 0.037 | Tree loss: 4.563 | Accuracy: 0.382812 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 021 | Total loss: 4.605 | Reg loss: 0.038 | Tree loss: 4.605 | Accuracy: 0.373047 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 021 | Total loss: 4.560 | Reg loss: 0.038 | Tree loss: 4.560 | Accuracy: 0.392578 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 021 | Total loss: 4.500 | Reg loss: 0.038 | Tree loss: 4.500 | Accuracy: 0.425781 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 021 | Total loss: 4.572 | Reg loss: 0.038 | Tree loss: 4.572 | Accuracy: 0.316406 | 0.106 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 021 | Total loss: 4.530 | Reg loss: 0.038 | Tree loss: 4.530 | Accuracy: 0.412214 | 0.106 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Batch: 000 / 021 | Total loss: 4.881 | Reg loss: 0.036 | Tree loss: 4.881 | Accuracy: 0.380859 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 021 | Total loss: 4.811 | Reg loss: 0.036 | Tree loss: 4.811 | Accuracy: 0.376953 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 021 | Total loss: 4.783 | Reg loss: 0.036 | Tree loss: 4.783 | Accuracy: 0.373047 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 021 | Total loss: 4.752 | Reg loss: 0.037 | Tree loss: 4.752 | Accuracy: 0.429688 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 021 | Total loss: 4.770 | Reg loss: 0.037 | Tree loss: 4.770 | Accuracy: 0.355469 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 021 | Total loss: 4.752 | Reg loss: 0.037 | Tree loss: 4.752 | Accuracy: 0.375000 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 021 | Total loss: 4.706 | Reg loss: 0.037 | Tree loss: 4.706 | Accuracy: 0.365234 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 021 | Total loss: 4.712 | Reg loss: 0.037 | Tree loss: 4.712 | Accuracy: 0.332031 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 021 | Total loss: 4.636 | Reg loss: 0.037 | Tree loss: 4.636 | Accuracy: 0.394531 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 021 | Total loss: 4.594 | Reg loss: 0.037 | Tree loss: 4.594 | Accuracy: 0.355469 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 021 | Total loss: 4.590 | Reg loss: 0.037 | Tree loss: 4.590 | Accuracy: 0.355469 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 021 | Total loss: 4.520 | Reg loss: 0.037 | Tree loss: 4.520 | Accuracy: 0.390625 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 021 | Total loss: 4.605 | Reg loss: 0.037 | Tree loss: 4.605 | Accuracy: 0.330078 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 021 | Total loss: 4.534 | Reg loss: 0.037 | Tree loss: 4.534 | Accuracy: 0.396484 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 021 | Total loss: 4.518 | Reg loss: 0.037 | Tree loss: 4.518 | Accuracy: 0.353516 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 021 | Total loss: 4.506 | Reg loss: 0.037 | Tree loss: 4.506 | Accuracy: 0.355469 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 021 | Total loss: 4.422 | Reg loss: 0.038 | Tree loss: 4.422 | Accuracy: 0.412109 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 021 | Total loss: 4.430 | Reg loss: 0.038 | Tree loss: 4.430 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 021 | Total loss: 4.442 | Reg loss: 0.038 | Tree loss: 4.442 | Accuracy: 0.367188 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 021 | Total loss: 4.347 | Reg loss: 0.038 | Tree loss: 4.347 | Accuracy: 0.384766 | 0.106 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 021 | Total loss: 4.423 | Reg loss: 0.038 | Tree loss: 4.423 | Accuracy: 0.290076 | 0.106 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 40 | Batch: 000 / 021 | Total loss: 4.697 | Reg loss: 0.036 | Tree loss: 4.697 | Accuracy: 0.378906 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 021 | Total loss: 4.730 | Reg loss: 0.036 | Tree loss: 4.730 | Accuracy: 0.371094 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 021 | Total loss: 4.707 | Reg loss: 0.036 | Tree loss: 4.707 | Accuracy: 0.359375 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 021 | Total loss: 4.641 | Reg loss: 0.036 | Tree loss: 4.641 | Accuracy: 0.380859 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 021 | Total loss: 4.618 | Reg loss: 0.036 | Tree loss: 4.618 | Accuracy: 0.382812 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 021 | Total loss: 4.603 | Reg loss: 0.036 | Tree loss: 4.603 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 021 | Total loss: 4.544 | Reg loss: 0.037 | Tree loss: 4.544 | Accuracy: 0.404297 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 021 | Total loss: 4.545 | Reg loss: 0.037 | Tree loss: 4.545 | Accuracy: 0.378906 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 021 | Total loss: 4.502 | Reg loss: 0.037 | Tree loss: 4.502 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 021 | Total loss: 4.471 | Reg loss: 0.037 | Tree loss: 4.471 | Accuracy: 0.388672 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 021 | Total loss: 4.481 | Reg loss: 0.037 | Tree loss: 4.481 | Accuracy: 0.343750 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 021 | Total loss: 4.463 | Reg loss: 0.037 | Tree loss: 4.463 | Accuracy: 0.371094 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 021 | Total loss: 4.417 | Reg loss: 0.037 | Tree loss: 4.417 | Accuracy: 0.355469 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 021 | Total loss: 4.387 | Reg loss: 0.037 | Tree loss: 4.387 | Accuracy: 0.363281 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 021 | Total loss: 4.392 | Reg loss: 0.037 | Tree loss: 4.392 | Accuracy: 0.371094 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 021 | Total loss: 4.313 | Reg loss: 0.037 | Tree loss: 4.313 | Accuracy: 0.384766 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 021 | Total loss: 4.321 | Reg loss: 0.037 | Tree loss: 4.321 | Accuracy: 0.394531 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 021 | Total loss: 4.321 | Reg loss: 0.038 | Tree loss: 4.321 | Accuracy: 0.347656 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 021 | Total loss: 4.304 | Reg loss: 0.038 | Tree loss: 4.304 | Accuracy: 0.349609 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 021 | Total loss: 4.256 | Reg loss: 0.038 | Tree loss: 4.256 | Accuracy: 0.382812 | 0.106 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 021 | Total loss: 4.288 | Reg loss: 0.038 | Tree loss: 4.288 | Accuracy: 0.312977 | 0.106 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 41 | Batch: 000 / 021 | Total loss: 4.594 | Reg loss: 0.036 | Tree loss: 4.594 | Accuracy: 0.376953 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 021 | Total loss: 4.560 | Reg loss: 0.036 | Tree loss: 4.560 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 021 | Total loss: 4.552 | Reg loss: 0.036 | Tree loss: 4.552 | Accuracy: 0.361328 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 021 | Total loss: 4.571 | Reg loss: 0.036 | Tree loss: 4.571 | Accuracy: 0.333984 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 021 | Total loss: 4.490 | Reg loss: 0.036 | Tree loss: 4.490 | Accuracy: 0.353516 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 021 | Total loss: 4.477 | Reg loss: 0.036 | Tree loss: 4.477 | Accuracy: 0.355469 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 021 | Total loss: 4.431 | Reg loss: 0.036 | Tree loss: 4.431 | Accuracy: 0.376953 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 021 | Total loss: 4.424 | Reg loss: 0.036 | Tree loss: 4.424 | Accuracy: 0.371094 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 021 | Total loss: 4.374 | Reg loss: 0.036 | Tree loss: 4.374 | Accuracy: 0.388672 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 021 | Total loss: 4.359 | Reg loss: 0.037 | Tree loss: 4.359 | Accuracy: 0.380859 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 021 | Total loss: 4.338 | Reg loss: 0.037 | Tree loss: 4.338 | Accuracy: 0.367188 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 021 | Total loss: 4.274 | Reg loss: 0.037 | Tree loss: 4.274 | Accuracy: 0.384766 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 021 | Total loss: 4.225 | Reg loss: 0.037 | Tree loss: 4.225 | Accuracy: 0.396484 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 021 | Total loss: 4.263 | Reg loss: 0.037 | Tree loss: 4.263 | Accuracy: 0.390625 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 021 | Total loss: 4.233 | Reg loss: 0.037 | Tree loss: 4.233 | Accuracy: 0.392578 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 021 | Total loss: 4.239 | Reg loss: 0.037 | Tree loss: 4.239 | Accuracy: 0.353516 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 021 | Total loss: 4.195 | Reg loss: 0.037 | Tree loss: 4.195 | Accuracy: 0.367188 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 021 | Total loss: 4.146 | Reg loss: 0.037 | Tree loss: 4.146 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 021 | Total loss: 4.137 | Reg loss: 0.038 | Tree loss: 4.137 | Accuracy: 0.369141 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 021 | Total loss: 4.112 | Reg loss: 0.038 | Tree loss: 4.112 | Accuracy: 0.375000 | 0.106 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 021 | Total loss: 4.128 | Reg loss: 0.038 | Tree loss: 4.128 | Accuracy: 0.366412 | 0.106 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 42 | Batch: 000 / 021 | Total loss: 4.432 | Reg loss: 0.036 | Tree loss: 4.432 | Accuracy: 0.369141 | 0.106 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 | Batch: 001 / 021 | Total loss: 4.396 | Reg loss: 0.036 | Tree loss: 4.396 | Accuracy: 0.396484 | 0.106 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 021 | Total loss: 4.422 | Reg loss: 0.036 | Tree loss: 4.422 | Accuracy: 0.384766 | 0.106 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 021 | Total loss: 4.419 | Reg loss: 0.036 | Tree loss: 4.419 | Accuracy: 0.339844 | 0.106 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 021 | Total loss: 4.374 | Reg loss: 0.036 | Tree loss: 4.374 | Accuracy: 0.373047 | 0.106 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 021 | Total loss: 4.347 | Reg loss: 0.036 | Tree loss: 4.347 | Accuracy: 0.355469 | 0.106 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 021 | Total loss: 4.304 | Reg loss: 0.036 | Tree loss: 4.304 | Accuracy: 0.351562 | 0.106 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 021 | Total loss: 4.252 | Reg loss: 0.036 | Tree loss: 4.252 | Accuracy: 0.398438 | 0.107 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 021 | Total loss: 4.281 | Reg loss: 0.036 | Tree loss: 4.281 | Accuracy: 0.355469 | 0.107 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 021 | Total loss: 4.247 | Reg loss: 0.036 | Tree loss: 4.247 | Accuracy: 0.347656 | 0.107 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 021 | Total loss: 4.131 | Reg loss: 0.036 | Tree loss: 4.131 | Accuracy: 0.419922 | 0.107 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 021 | Total loss: 4.205 | Reg loss: 0.037 | Tree loss: 4.205 | Accuracy: 0.341797 | 0.107 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 021 | Total loss: 4.193 | Reg loss: 0.037 | Tree loss: 4.193 | Accuracy: 0.353516 | 0.107 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 021 | Total loss: 4.070 | Reg loss: 0.037 | Tree loss: 4.070 | Accuracy: 0.406250 | 0.107 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 021 | Total loss: 4.091 | Reg loss: 0.037 | Tree loss: 4.091 | Accuracy: 0.378906 | 0.107 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 021 | Total loss: 4.118 | Reg loss: 0.037 | Tree loss: 4.118 | Accuracy: 0.351562 | 0.107 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 021 | Total loss: 4.070 | Reg loss: 0.037 | Tree loss: 4.070 | Accuracy: 0.363281 | 0.107 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 021 | Total loss: 4.031 | Reg loss: 0.037 | Tree loss: 4.031 | Accuracy: 0.384766 | 0.107 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 021 | Total loss: 4.023 | Reg loss: 0.037 | Tree loss: 4.023 | Accuracy: 0.363281 | 0.107 sec/iter\n",
      "Epoch: 42 | Batch: 019 / 021 | Total loss: 3.920 | Reg loss: 0.038 | Tree loss: 3.920 | Accuracy: 0.410156 | 0.107 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 021 | Total loss: 4.026 | Reg loss: 0.038 | Tree loss: 4.026 | Accuracy: 0.320611 | 0.107 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 43 | Batch: 000 / 021 | Total loss: 4.317 | Reg loss: 0.036 | Tree loss: 4.317 | Accuracy: 0.382812 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 021 | Total loss: 4.300 | Reg loss: 0.036 | Tree loss: 4.300 | Accuracy: 0.404297 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 021 | Total loss: 4.306 | Reg loss: 0.036 | Tree loss: 4.306 | Accuracy: 0.369141 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 021 | Total loss: 4.280 | Reg loss: 0.036 | Tree loss: 4.280 | Accuracy: 0.386719 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 021 | Total loss: 4.262 | Reg loss: 0.036 | Tree loss: 4.262 | Accuracy: 0.373047 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 021 | Total loss: 4.202 | Reg loss: 0.036 | Tree loss: 4.202 | Accuracy: 0.404297 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 021 | Total loss: 4.159 | Reg loss: 0.036 | Tree loss: 4.159 | Accuracy: 0.398438 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 021 | Total loss: 4.150 | Reg loss: 0.036 | Tree loss: 4.150 | Accuracy: 0.396484 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 021 | Total loss: 4.112 | Reg loss: 0.036 | Tree loss: 4.112 | Accuracy: 0.371094 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 021 | Total loss: 4.085 | Reg loss: 0.036 | Tree loss: 4.085 | Accuracy: 0.345703 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 021 | Total loss: 4.070 | Reg loss: 0.036 | Tree loss: 4.070 | Accuracy: 0.369141 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 021 | Total loss: 4.069 | Reg loss: 0.036 | Tree loss: 4.069 | Accuracy: 0.375000 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 021 | Total loss: 4.003 | Reg loss: 0.036 | Tree loss: 4.003 | Accuracy: 0.341797 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 021 | Total loss: 3.951 | Reg loss: 0.037 | Tree loss: 3.951 | Accuracy: 0.359375 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 021 | Total loss: 3.946 | Reg loss: 0.037 | Tree loss: 3.946 | Accuracy: 0.373047 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 021 | Total loss: 3.910 | Reg loss: 0.037 | Tree loss: 3.910 | Accuracy: 0.378906 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 021 | Total loss: 3.886 | Reg loss: 0.037 | Tree loss: 3.886 | Accuracy: 0.365234 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 021 | Total loss: 3.973 | Reg loss: 0.037 | Tree loss: 3.973 | Accuracy: 0.324219 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 021 | Total loss: 3.889 | Reg loss: 0.037 | Tree loss: 3.889 | Accuracy: 0.351562 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 021 | Total loss: 3.827 | Reg loss: 0.037 | Tree loss: 3.827 | Accuracy: 0.371094 | 0.107 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 021 | Total loss: 3.913 | Reg loss: 0.037 | Tree loss: 3.913 | Accuracy: 0.335878 | 0.107 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 44 | Batch: 000 / 021 | Total loss: 4.211 | Reg loss: 0.036 | Tree loss: 4.211 | Accuracy: 0.384766 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 021 | Total loss: 4.203 | Reg loss: 0.036 | Tree loss: 4.203 | Accuracy: 0.361328 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 021 | Total loss: 4.137 | Reg loss: 0.036 | Tree loss: 4.137 | Accuracy: 0.390625 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 021 | Total loss: 4.115 | Reg loss: 0.036 | Tree loss: 4.115 | Accuracy: 0.363281 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 021 | Total loss: 4.144 | Reg loss: 0.036 | Tree loss: 4.144 | Accuracy: 0.310547 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 021 | Total loss: 4.071 | Reg loss: 0.036 | Tree loss: 4.071 | Accuracy: 0.371094 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 021 | Total loss: 4.017 | Reg loss: 0.036 | Tree loss: 4.017 | Accuracy: 0.373047 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 021 | Total loss: 4.027 | Reg loss: 0.036 | Tree loss: 4.027 | Accuracy: 0.373047 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 021 | Total loss: 3.923 | Reg loss: 0.036 | Tree loss: 3.923 | Accuracy: 0.410156 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 021 | Total loss: 3.948 | Reg loss: 0.036 | Tree loss: 3.948 | Accuracy: 0.349609 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 021 | Total loss: 3.978 | Reg loss: 0.036 | Tree loss: 3.978 | Accuracy: 0.355469 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 021 | Total loss: 3.938 | Reg loss: 0.036 | Tree loss: 3.938 | Accuracy: 0.353516 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 021 | Total loss: 3.848 | Reg loss: 0.036 | Tree loss: 3.848 | Accuracy: 0.394531 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 021 | Total loss: 3.842 | Reg loss: 0.036 | Tree loss: 3.842 | Accuracy: 0.375000 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 021 | Total loss: 3.871 | Reg loss: 0.036 | Tree loss: 3.871 | Accuracy: 0.386719 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 021 | Total loss: 3.790 | Reg loss: 0.036 | Tree loss: 3.790 | Accuracy: 0.365234 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 021 | Total loss: 3.744 | Reg loss: 0.037 | Tree loss: 3.744 | Accuracy: 0.388672 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 021 | Total loss: 3.741 | Reg loss: 0.037 | Tree loss: 3.741 | Accuracy: 0.384766 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 021 | Total loss: 3.747 | Reg loss: 0.037 | Tree loss: 3.747 | Accuracy: 0.367188 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 019 / 021 | Total loss: 3.734 | Reg loss: 0.037 | Tree loss: 3.734 | Accuracy: 0.345703 | 0.107 sec/iter\n",
      "Epoch: 44 | Batch: 020 / 021 | Total loss: 3.562 | Reg loss: 0.037 | Tree loss: 3.562 | Accuracy: 0.480916 | 0.107 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 | Batch: 000 / 021 | Total loss: 4.126 | Reg loss: 0.035 | Tree loss: 4.126 | Accuracy: 0.310547 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 021 | Total loss: 4.098 | Reg loss: 0.035 | Tree loss: 4.098 | Accuracy: 0.341797 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 021 | Total loss: 4.008 | Reg loss: 0.035 | Tree loss: 4.008 | Accuracy: 0.404297 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 021 | Total loss: 4.026 | Reg loss: 0.035 | Tree loss: 4.026 | Accuracy: 0.380859 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 021 | Total loss: 4.028 | Reg loss: 0.035 | Tree loss: 4.028 | Accuracy: 0.355469 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 021 | Total loss: 3.953 | Reg loss: 0.035 | Tree loss: 3.953 | Accuracy: 0.351562 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 021 | Total loss: 3.876 | Reg loss: 0.035 | Tree loss: 3.876 | Accuracy: 0.380859 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 021 | Total loss: 3.871 | Reg loss: 0.035 | Tree loss: 3.871 | Accuracy: 0.371094 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 021 | Total loss: 3.875 | Reg loss: 0.035 | Tree loss: 3.875 | Accuracy: 0.357422 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 021 | Total loss: 3.822 | Reg loss: 0.035 | Tree loss: 3.822 | Accuracy: 0.375000 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 021 | Total loss: 3.777 | Reg loss: 0.036 | Tree loss: 3.777 | Accuracy: 0.384766 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 021 | Total loss: 3.735 | Reg loss: 0.036 | Tree loss: 3.735 | Accuracy: 0.400391 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 021 | Total loss: 3.792 | Reg loss: 0.036 | Tree loss: 3.792 | Accuracy: 0.390625 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 021 | Total loss: 3.694 | Reg loss: 0.036 | Tree loss: 3.694 | Accuracy: 0.378906 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 021 | Total loss: 3.664 | Reg loss: 0.036 | Tree loss: 3.664 | Accuracy: 0.414062 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 021 | Total loss: 3.682 | Reg loss: 0.036 | Tree loss: 3.682 | Accuracy: 0.388672 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 021 | Total loss: 3.642 | Reg loss: 0.036 | Tree loss: 3.642 | Accuracy: 0.367188 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 021 | Total loss: 3.633 | Reg loss: 0.036 | Tree loss: 3.633 | Accuracy: 0.371094 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 021 | Total loss: 3.628 | Reg loss: 0.036 | Tree loss: 3.628 | Accuracy: 0.335938 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 021 | Total loss: 3.580 | Reg loss: 0.037 | Tree loss: 3.580 | Accuracy: 0.378906 | 0.107 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 021 | Total loss: 3.491 | Reg loss: 0.037 | Tree loss: 3.491 | Accuracy: 0.343511 | 0.107 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 46 | Batch: 000 / 021 | Total loss: 3.945 | Reg loss: 0.035 | Tree loss: 3.945 | Accuracy: 0.367188 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 021 | Total loss: 3.853 | Reg loss: 0.035 | Tree loss: 3.853 | Accuracy: 0.419922 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 021 | Total loss: 3.919 | Reg loss: 0.035 | Tree loss: 3.919 | Accuracy: 0.378906 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 021 | Total loss: 3.896 | Reg loss: 0.035 | Tree loss: 3.896 | Accuracy: 0.380859 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 021 | Total loss: 3.830 | Reg loss: 0.035 | Tree loss: 3.830 | Accuracy: 0.406250 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 021 | Total loss: 3.852 | Reg loss: 0.035 | Tree loss: 3.852 | Accuracy: 0.369141 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 021 | Total loss: 3.789 | Reg loss: 0.035 | Tree loss: 3.789 | Accuracy: 0.380859 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 021 | Total loss: 3.781 | Reg loss: 0.035 | Tree loss: 3.781 | Accuracy: 0.369141 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 021 | Total loss: 3.783 | Reg loss: 0.035 | Tree loss: 3.783 | Accuracy: 0.333984 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 021 | Total loss: 3.727 | Reg loss: 0.035 | Tree loss: 3.727 | Accuracy: 0.378906 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 021 | Total loss: 3.736 | Reg loss: 0.035 | Tree loss: 3.736 | Accuracy: 0.351562 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 021 | Total loss: 3.623 | Reg loss: 0.035 | Tree loss: 3.623 | Accuracy: 0.406250 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 021 | Total loss: 3.682 | Reg loss: 0.035 | Tree loss: 3.682 | Accuracy: 0.345703 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 021 | Total loss: 3.562 | Reg loss: 0.035 | Tree loss: 3.562 | Accuracy: 0.371094 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 021 | Total loss: 3.567 | Reg loss: 0.035 | Tree loss: 3.567 | Accuracy: 0.378906 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 021 | Total loss: 3.568 | Reg loss: 0.036 | Tree loss: 3.568 | Accuracy: 0.380859 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 021 | Total loss: 3.548 | Reg loss: 0.036 | Tree loss: 3.548 | Accuracy: 0.359375 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 021 | Total loss: 3.526 | Reg loss: 0.036 | Tree loss: 3.526 | Accuracy: 0.332031 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 021 | Total loss: 3.482 | Reg loss: 0.036 | Tree loss: 3.482 | Accuracy: 0.367188 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 021 | Total loss: 3.417 | Reg loss: 0.036 | Tree loss: 3.417 | Accuracy: 0.371094 | 0.107 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 021 | Total loss: 3.532 | Reg loss: 0.036 | Tree loss: 3.532 | Accuracy: 0.305344 | 0.107 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 47 | Batch: 000 / 021 | Total loss: 3.817 | Reg loss: 0.034 | Tree loss: 3.817 | Accuracy: 0.369141 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 021 | Total loss: 3.805 | Reg loss: 0.034 | Tree loss: 3.805 | Accuracy: 0.382812 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 021 | Total loss: 3.787 | Reg loss: 0.034 | Tree loss: 3.787 | Accuracy: 0.375000 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 021 | Total loss: 3.777 | Reg loss: 0.034 | Tree loss: 3.777 | Accuracy: 0.357422 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 021 | Total loss: 3.749 | Reg loss: 0.034 | Tree loss: 3.749 | Accuracy: 0.363281 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 021 | Total loss: 3.764 | Reg loss: 0.034 | Tree loss: 3.764 | Accuracy: 0.337891 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 021 | Total loss: 3.768 | Reg loss: 0.034 | Tree loss: 3.768 | Accuracy: 0.314453 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 021 | Total loss: 3.658 | Reg loss: 0.034 | Tree loss: 3.658 | Accuracy: 0.380859 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 021 | Total loss: 3.594 | Reg loss: 0.034 | Tree loss: 3.594 | Accuracy: 0.406250 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 021 | Total loss: 3.607 | Reg loss: 0.034 | Tree loss: 3.607 | Accuracy: 0.361328 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 021 | Total loss: 3.567 | Reg loss: 0.034 | Tree loss: 3.567 | Accuracy: 0.349609 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 021 | Total loss: 3.590 | Reg loss: 0.034 | Tree loss: 3.590 | Accuracy: 0.355469 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 021 | Total loss: 3.502 | Reg loss: 0.035 | Tree loss: 3.502 | Accuracy: 0.351562 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 021 | Total loss: 3.477 | Reg loss: 0.035 | Tree loss: 3.477 | Accuracy: 0.400391 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 021 | Total loss: 3.463 | Reg loss: 0.035 | Tree loss: 3.463 | Accuracy: 0.386719 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 021 | Total loss: 3.395 | Reg loss: 0.035 | Tree loss: 3.395 | Accuracy: 0.404297 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 021 | Total loss: 3.399 | Reg loss: 0.035 | Tree loss: 3.399 | Accuracy: 0.400391 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 021 | Total loss: 3.350 | Reg loss: 0.035 | Tree loss: 3.350 | Accuracy: 0.380859 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 021 | Total loss: 3.315 | Reg loss: 0.035 | Tree loss: 3.315 | Accuracy: 0.386719 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 021 | Total loss: 3.366 | Reg loss: 0.035 | Tree loss: 3.366 | Accuracy: 0.353516 | 0.107 sec/iter\n",
      "Epoch: 47 | Batch: 020 / 021 | Total loss: 3.371 | Reg loss: 0.036 | Tree loss: 3.371 | Accuracy: 0.427481 | 0.107 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 48 | Batch: 000 / 021 | Total loss: 3.759 | Reg loss: 0.033 | Tree loss: 3.759 | Accuracy: 0.392578 | 0.107 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 | Batch: 001 / 021 | Total loss: 3.698 | Reg loss: 0.033 | Tree loss: 3.698 | Accuracy: 0.400391 | 0.107 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 021 | Total loss: 3.659 | Reg loss: 0.033 | Tree loss: 3.659 | Accuracy: 0.380859 | 0.107 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 021 | Total loss: 3.671 | Reg loss: 0.033 | Tree loss: 3.671 | Accuracy: 0.341797 | 0.107 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 021 | Total loss: 3.621 | Reg loss: 0.033 | Tree loss: 3.621 | Accuracy: 0.369141 | 0.107 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 021 | Total loss: 3.618 | Reg loss: 0.033 | Tree loss: 3.618 | Accuracy: 0.369141 | 0.107 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 021 | Total loss: 3.614 | Reg loss: 0.033 | Tree loss: 3.614 | Accuracy: 0.386719 | 0.107 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 021 | Total loss: 3.544 | Reg loss: 0.033 | Tree loss: 3.544 | Accuracy: 0.388672 | 0.107 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 021 | Total loss: 3.472 | Reg loss: 0.034 | Tree loss: 3.472 | Accuracy: 0.363281 | 0.107 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 021 | Total loss: 3.494 | Reg loss: 0.034 | Tree loss: 3.494 | Accuracy: 0.365234 | 0.107 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 021 | Total loss: 3.422 | Reg loss: 0.034 | Tree loss: 3.422 | Accuracy: 0.394531 | 0.107 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 021 | Total loss: 3.464 | Reg loss: 0.034 | Tree loss: 3.464 | Accuracy: 0.353516 | 0.107 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 021 | Total loss: 3.396 | Reg loss: 0.034 | Tree loss: 3.396 | Accuracy: 0.367188 | 0.107 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 021 | Total loss: 3.362 | Reg loss: 0.034 | Tree loss: 3.362 | Accuracy: 0.365234 | 0.108 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 021 | Total loss: 3.326 | Reg loss: 0.034 | Tree loss: 3.326 | Accuracy: 0.384766 | 0.108 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 021 | Total loss: 3.260 | Reg loss: 0.034 | Tree loss: 3.260 | Accuracy: 0.406250 | 0.108 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 021 | Total loss: 3.288 | Reg loss: 0.034 | Tree loss: 3.288 | Accuracy: 0.353516 | 0.108 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 021 | Total loss: 3.296 | Reg loss: 0.035 | Tree loss: 3.296 | Accuracy: 0.349609 | 0.108 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 021 | Total loss: 3.255 | Reg loss: 0.035 | Tree loss: 3.255 | Accuracy: 0.361328 | 0.108 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 021 | Total loss: 3.227 | Reg loss: 0.035 | Tree loss: 3.227 | Accuracy: 0.355469 | 0.108 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 021 | Total loss: 3.219 | Reg loss: 0.035 | Tree loss: 3.219 | Accuracy: 0.305344 | 0.108 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 49 | Batch: 000 / 021 | Total loss: 3.656 | Reg loss: 0.033 | Tree loss: 3.656 | Accuracy: 0.384766 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 021 | Total loss: 3.615 | Reg loss: 0.033 | Tree loss: 3.615 | Accuracy: 0.367188 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 021 | Total loss: 3.634 | Reg loss: 0.033 | Tree loss: 3.634 | Accuracy: 0.353516 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 021 | Total loss: 3.574 | Reg loss: 0.033 | Tree loss: 3.574 | Accuracy: 0.349609 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 021 | Total loss: 3.467 | Reg loss: 0.033 | Tree loss: 3.467 | Accuracy: 0.402344 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 021 | Total loss: 3.470 | Reg loss: 0.033 | Tree loss: 3.470 | Accuracy: 0.390625 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 021 | Total loss: 3.486 | Reg loss: 0.033 | Tree loss: 3.486 | Accuracy: 0.371094 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 021 | Total loss: 3.456 | Reg loss: 0.033 | Tree loss: 3.456 | Accuracy: 0.357422 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 021 | Total loss: 3.410 | Reg loss: 0.033 | Tree loss: 3.410 | Accuracy: 0.371094 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 021 | Total loss: 3.352 | Reg loss: 0.033 | Tree loss: 3.352 | Accuracy: 0.371094 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 021 | Total loss: 3.343 | Reg loss: 0.033 | Tree loss: 3.343 | Accuracy: 0.376953 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 021 | Total loss: 3.300 | Reg loss: 0.033 | Tree loss: 3.300 | Accuracy: 0.394531 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 021 | Total loss: 3.256 | Reg loss: 0.033 | Tree loss: 3.256 | Accuracy: 0.373047 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 021 | Total loss: 3.260 | Reg loss: 0.033 | Tree loss: 3.260 | Accuracy: 0.363281 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 021 | Total loss: 3.217 | Reg loss: 0.034 | Tree loss: 3.217 | Accuracy: 0.369141 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 021 | Total loss: 3.174 | Reg loss: 0.034 | Tree loss: 3.174 | Accuracy: 0.371094 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 021 | Total loss: 3.174 | Reg loss: 0.034 | Tree loss: 3.174 | Accuracy: 0.363281 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 021 | Total loss: 3.171 | Reg loss: 0.034 | Tree loss: 3.171 | Accuracy: 0.367188 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 021 | Total loss: 3.145 | Reg loss: 0.034 | Tree loss: 3.145 | Accuracy: 0.357422 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 021 | Total loss: 3.043 | Reg loss: 0.034 | Tree loss: 3.043 | Accuracy: 0.390625 | 0.108 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 021 | Total loss: 3.211 | Reg loss: 0.035 | Tree loss: 3.211 | Accuracy: 0.320611 | 0.108 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 50 | Batch: 000 / 021 | Total loss: 3.571 | Reg loss: 0.032 | Tree loss: 3.571 | Accuracy: 0.349609 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 021 | Total loss: 3.497 | Reg loss: 0.032 | Tree loss: 3.497 | Accuracy: 0.335938 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 021 | Total loss: 3.529 | Reg loss: 0.032 | Tree loss: 3.529 | Accuracy: 0.351562 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 021 | Total loss: 3.466 | Reg loss: 0.032 | Tree loss: 3.466 | Accuracy: 0.382812 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 021 | Total loss: 3.435 | Reg loss: 0.032 | Tree loss: 3.435 | Accuracy: 0.371094 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 021 | Total loss: 3.388 | Reg loss: 0.032 | Tree loss: 3.388 | Accuracy: 0.384766 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 021 | Total loss: 3.362 | Reg loss: 0.032 | Tree loss: 3.362 | Accuracy: 0.414062 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 021 | Total loss: 3.262 | Reg loss: 0.032 | Tree loss: 3.262 | Accuracy: 0.404297 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 021 | Total loss: 3.261 | Reg loss: 0.032 | Tree loss: 3.261 | Accuracy: 0.406250 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 021 | Total loss: 3.273 | Reg loss: 0.032 | Tree loss: 3.273 | Accuracy: 0.359375 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 021 | Total loss: 3.235 | Reg loss: 0.033 | Tree loss: 3.235 | Accuracy: 0.363281 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 021 | Total loss: 3.181 | Reg loss: 0.033 | Tree loss: 3.181 | Accuracy: 0.375000 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 021 | Total loss: 3.176 | Reg loss: 0.033 | Tree loss: 3.176 | Accuracy: 0.367188 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 021 | Total loss: 3.145 | Reg loss: 0.033 | Tree loss: 3.145 | Accuracy: 0.361328 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 021 | Total loss: 3.162 | Reg loss: 0.033 | Tree loss: 3.162 | Accuracy: 0.357422 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 021 | Total loss: 3.071 | Reg loss: 0.033 | Tree loss: 3.071 | Accuracy: 0.373047 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 021 | Total loss: 3.080 | Reg loss: 0.034 | Tree loss: 3.080 | Accuracy: 0.376953 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 021 | Total loss: 3.014 | Reg loss: 0.034 | Tree loss: 3.014 | Accuracy: 0.357422 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 021 | Total loss: 3.012 | Reg loss: 0.034 | Tree loss: 3.012 | Accuracy: 0.365234 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 021 | Total loss: 2.947 | Reg loss: 0.034 | Tree loss: 2.947 | Accuracy: 0.392578 | 0.108 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 021 | Total loss: 2.924 | Reg loss: 0.035 | Tree loss: 2.924 | Accuracy: 0.305344 | 0.108 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 51 | Batch: 000 / 021 | Total loss: 3.461 | Reg loss: 0.032 | Tree loss: 3.461 | Accuracy: 0.349609 | 0.108 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | Batch: 001 / 021 | Total loss: 3.336 | Reg loss: 0.032 | Tree loss: 3.336 | Accuracy: 0.425781 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 021 | Total loss: 3.372 | Reg loss: 0.032 | Tree loss: 3.372 | Accuracy: 0.367188 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 021 | Total loss: 3.347 | Reg loss: 0.032 | Tree loss: 3.347 | Accuracy: 0.404297 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 021 | Total loss: 3.283 | Reg loss: 0.032 | Tree loss: 3.283 | Accuracy: 0.384766 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 021 | Total loss: 3.271 | Reg loss: 0.032 | Tree loss: 3.271 | Accuracy: 0.390625 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 021 | Total loss: 3.267 | Reg loss: 0.032 | Tree loss: 3.267 | Accuracy: 0.365234 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 021 | Total loss: 3.219 | Reg loss: 0.032 | Tree loss: 3.219 | Accuracy: 0.382812 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 021 | Total loss: 3.231 | Reg loss: 0.032 | Tree loss: 3.231 | Accuracy: 0.369141 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 021 | Total loss: 3.177 | Reg loss: 0.032 | Tree loss: 3.177 | Accuracy: 0.376953 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 021 | Total loss: 3.121 | Reg loss: 0.033 | Tree loss: 3.121 | Accuracy: 0.378906 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 021 | Total loss: 3.137 | Reg loss: 0.033 | Tree loss: 3.137 | Accuracy: 0.355469 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 021 | Total loss: 3.084 | Reg loss: 0.033 | Tree loss: 3.084 | Accuracy: 0.371094 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 021 | Total loss: 3.053 | Reg loss: 0.033 | Tree loss: 3.053 | Accuracy: 0.378906 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 021 | Total loss: 3.016 | Reg loss: 0.033 | Tree loss: 3.016 | Accuracy: 0.343750 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 021 | Total loss: 3.007 | Reg loss: 0.034 | Tree loss: 3.007 | Accuracy: 0.341797 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 021 | Total loss: 2.975 | Reg loss: 0.034 | Tree loss: 2.975 | Accuracy: 0.333984 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 021 | Total loss: 2.939 | Reg loss: 0.034 | Tree loss: 2.939 | Accuracy: 0.339844 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 021 | Total loss: 2.866 | Reg loss: 0.034 | Tree loss: 2.866 | Accuracy: 0.361328 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 019 / 021 | Total loss: 2.824 | Reg loss: 0.035 | Tree loss: 2.824 | Accuracy: 0.406250 | 0.108 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 021 | Total loss: 2.837 | Reg loss: 0.035 | Tree loss: 2.837 | Accuracy: 0.389313 | 0.108 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 52 | Batch: 000 / 021 | Total loss: 3.325 | Reg loss: 0.032 | Tree loss: 3.325 | Accuracy: 0.373047 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 021 | Total loss: 3.349 | Reg loss: 0.032 | Tree loss: 3.349 | Accuracy: 0.365234 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 021 | Total loss: 3.345 | Reg loss: 0.032 | Tree loss: 3.345 | Accuracy: 0.343750 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 021 | Total loss: 3.276 | Reg loss: 0.032 | Tree loss: 3.276 | Accuracy: 0.361328 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 021 | Total loss: 3.204 | Reg loss: 0.032 | Tree loss: 3.204 | Accuracy: 0.367188 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 021 | Total loss: 3.200 | Reg loss: 0.032 | Tree loss: 3.200 | Accuracy: 0.388672 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 021 | Total loss: 3.159 | Reg loss: 0.032 | Tree loss: 3.159 | Accuracy: 0.363281 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 021 | Total loss: 3.142 | Reg loss: 0.032 | Tree loss: 3.142 | Accuracy: 0.396484 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 021 | Total loss: 3.095 | Reg loss: 0.032 | Tree loss: 3.095 | Accuracy: 0.345703 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 021 | Total loss: 3.015 | Reg loss: 0.033 | Tree loss: 3.015 | Accuracy: 0.361328 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 021 | Total loss: 3.042 | Reg loss: 0.033 | Tree loss: 3.042 | Accuracy: 0.378906 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 021 | Total loss: 3.005 | Reg loss: 0.033 | Tree loss: 3.005 | Accuracy: 0.384766 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 021 | Total loss: 2.948 | Reg loss: 0.033 | Tree loss: 2.948 | Accuracy: 0.373047 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 021 | Total loss: 2.932 | Reg loss: 0.033 | Tree loss: 2.932 | Accuracy: 0.367188 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 021 | Total loss: 2.881 | Reg loss: 0.034 | Tree loss: 2.881 | Accuracy: 0.394531 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 021 | Total loss: 2.872 | Reg loss: 0.034 | Tree loss: 2.872 | Accuracy: 0.392578 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 021 | Total loss: 2.821 | Reg loss: 0.034 | Tree loss: 2.821 | Accuracy: 0.396484 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 021 | Total loss: 2.845 | Reg loss: 0.034 | Tree loss: 2.845 | Accuracy: 0.347656 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 021 | Total loss: 2.792 | Reg loss: 0.035 | Tree loss: 2.792 | Accuracy: 0.373047 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 021 | Total loss: 2.733 | Reg loss: 0.035 | Tree loss: 2.733 | Accuracy: 0.365234 | 0.108 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 021 | Total loss: 2.832 | Reg loss: 0.035 | Tree loss: 2.832 | Accuracy: 0.343511 | 0.108 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 53 | Batch: 000 / 021 | Total loss: 3.260 | Reg loss: 0.032 | Tree loss: 3.260 | Accuracy: 0.369141 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 021 | Total loss: 3.248 | Reg loss: 0.032 | Tree loss: 3.248 | Accuracy: 0.324219 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 021 | Total loss: 3.194 | Reg loss: 0.032 | Tree loss: 3.194 | Accuracy: 0.365234 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 021 | Total loss: 3.169 | Reg loss: 0.032 | Tree loss: 3.169 | Accuracy: 0.361328 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 021 | Total loss: 3.145 | Reg loss: 0.032 | Tree loss: 3.145 | Accuracy: 0.322266 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 021 | Total loss: 3.094 | Reg loss: 0.032 | Tree loss: 3.094 | Accuracy: 0.337891 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 021 | Total loss: 3.051 | Reg loss: 0.033 | Tree loss: 3.051 | Accuracy: 0.380859 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 021 | Total loss: 3.024 | Reg loss: 0.033 | Tree loss: 3.024 | Accuracy: 0.382812 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 021 | Total loss: 3.003 | Reg loss: 0.033 | Tree loss: 3.003 | Accuracy: 0.384766 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 021 | Total loss: 2.974 | Reg loss: 0.033 | Tree loss: 2.974 | Accuracy: 0.359375 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 021 | Total loss: 2.942 | Reg loss: 0.033 | Tree loss: 2.942 | Accuracy: 0.376953 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 021 | Total loss: 2.966 | Reg loss: 0.033 | Tree loss: 2.966 | Accuracy: 0.398438 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 021 | Total loss: 2.815 | Reg loss: 0.034 | Tree loss: 2.815 | Accuracy: 0.419922 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 021 | Total loss: 2.847 | Reg loss: 0.034 | Tree loss: 2.847 | Accuracy: 0.365234 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 021 | Total loss: 2.807 | Reg loss: 0.034 | Tree loss: 2.807 | Accuracy: 0.378906 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 021 | Total loss: 2.750 | Reg loss: 0.034 | Tree loss: 2.750 | Accuracy: 0.375000 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 021 | Total loss: 2.756 | Reg loss: 0.035 | Tree loss: 2.756 | Accuracy: 0.365234 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 021 | Total loss: 2.694 | Reg loss: 0.035 | Tree loss: 2.694 | Accuracy: 0.378906 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 021 | Total loss: 2.692 | Reg loss: 0.035 | Tree loss: 2.692 | Accuracy: 0.365234 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 021 | Total loss: 2.644 | Reg loss: 0.035 | Tree loss: 2.644 | Accuracy: 0.414062 | 0.108 sec/iter\n",
      "Epoch: 53 | Batch: 020 / 021 | Total loss: 2.601 | Reg loss: 0.036 | Tree loss: 2.601 | Accuracy: 0.396947 | 0.108 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 | Batch: 000 / 021 | Total loss: 3.161 | Reg loss: 0.033 | Tree loss: 3.161 | Accuracy: 0.359375 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 021 | Total loss: 3.085 | Reg loss: 0.033 | Tree loss: 3.085 | Accuracy: 0.402344 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 021 | Total loss: 3.112 | Reg loss: 0.033 | Tree loss: 3.112 | Accuracy: 0.378906 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 021 | Total loss: 3.015 | Reg loss: 0.033 | Tree loss: 3.015 | Accuracy: 0.429688 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 021 | Total loss: 3.000 | Reg loss: 0.033 | Tree loss: 3.000 | Accuracy: 0.359375 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 021 | Total loss: 3.015 | Reg loss: 0.033 | Tree loss: 3.015 | Accuracy: 0.355469 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 021 | Total loss: 3.006 | Reg loss: 0.033 | Tree loss: 3.006 | Accuracy: 0.371094 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 021 | Total loss: 2.876 | Reg loss: 0.033 | Tree loss: 2.876 | Accuracy: 0.398438 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 021 | Total loss: 2.887 | Reg loss: 0.034 | Tree loss: 2.887 | Accuracy: 0.396484 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 021 | Total loss: 2.827 | Reg loss: 0.034 | Tree loss: 2.827 | Accuracy: 0.363281 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 021 | Total loss: 2.862 | Reg loss: 0.034 | Tree loss: 2.862 | Accuracy: 0.378906 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 021 | Total loss: 2.898 | Reg loss: 0.034 | Tree loss: 2.898 | Accuracy: 0.339844 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 021 | Total loss: 2.761 | Reg loss: 0.034 | Tree loss: 2.761 | Accuracy: 0.382812 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 021 | Total loss: 2.780 | Reg loss: 0.035 | Tree loss: 2.780 | Accuracy: 0.353516 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 021 | Total loss: 2.745 | Reg loss: 0.035 | Tree loss: 2.745 | Accuracy: 0.369141 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 021 | Total loss: 2.721 | Reg loss: 0.035 | Tree loss: 2.721 | Accuracy: 0.361328 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 021 | Total loss: 2.624 | Reg loss: 0.035 | Tree loss: 2.624 | Accuracy: 0.382812 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 021 | Total loss: 2.676 | Reg loss: 0.035 | Tree loss: 2.676 | Accuracy: 0.330078 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 021 | Total loss: 2.607 | Reg loss: 0.036 | Tree loss: 2.607 | Accuracy: 0.335938 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 021 | Total loss: 2.569 | Reg loss: 0.036 | Tree loss: 2.569 | Accuracy: 0.371094 | 0.108 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 021 | Total loss: 2.519 | Reg loss: 0.036 | Tree loss: 2.519 | Accuracy: 0.419847 | 0.108 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 55 | Batch: 000 / 021 | Total loss: 3.053 | Reg loss: 0.033 | Tree loss: 3.053 | Accuracy: 0.394531 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 021 | Total loss: 3.040 | Reg loss: 0.034 | Tree loss: 3.040 | Accuracy: 0.373047 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 021 | Total loss: 2.963 | Reg loss: 0.034 | Tree loss: 2.963 | Accuracy: 0.414062 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 021 | Total loss: 2.984 | Reg loss: 0.034 | Tree loss: 2.984 | Accuracy: 0.384766 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 021 | Total loss: 2.951 | Reg loss: 0.034 | Tree loss: 2.951 | Accuracy: 0.355469 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 021 | Total loss: 2.908 | Reg loss: 0.034 | Tree loss: 2.908 | Accuracy: 0.376953 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 021 | Total loss: 2.847 | Reg loss: 0.034 | Tree loss: 2.847 | Accuracy: 0.373047 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 021 | Total loss: 2.871 | Reg loss: 0.034 | Tree loss: 2.871 | Accuracy: 0.347656 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 021 | Total loss: 2.857 | Reg loss: 0.034 | Tree loss: 2.857 | Accuracy: 0.369141 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 021 | Total loss: 2.838 | Reg loss: 0.034 | Tree loss: 2.838 | Accuracy: 0.353516 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 021 | Total loss: 2.770 | Reg loss: 0.035 | Tree loss: 2.770 | Accuracy: 0.367188 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 021 | Total loss: 2.722 | Reg loss: 0.035 | Tree loss: 2.722 | Accuracy: 0.375000 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 021 | Total loss: 2.758 | Reg loss: 0.035 | Tree loss: 2.758 | Accuracy: 0.353516 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 021 | Total loss: 2.645 | Reg loss: 0.035 | Tree loss: 2.645 | Accuracy: 0.365234 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 021 | Total loss: 2.678 | Reg loss: 0.035 | Tree loss: 2.678 | Accuracy: 0.361328 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 021 | Total loss: 2.553 | Reg loss: 0.035 | Tree loss: 2.553 | Accuracy: 0.376953 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 021 | Total loss: 2.533 | Reg loss: 0.036 | Tree loss: 2.533 | Accuracy: 0.398438 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 021 | Total loss: 2.536 | Reg loss: 0.036 | Tree loss: 2.536 | Accuracy: 0.369141 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 021 | Total loss: 2.569 | Reg loss: 0.036 | Tree loss: 2.569 | Accuracy: 0.351562 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 021 | Total loss: 2.536 | Reg loss: 0.036 | Tree loss: 2.536 | Accuracy: 0.380859 | 0.108 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 021 | Total loss: 2.457 | Reg loss: 0.037 | Tree loss: 2.457 | Accuracy: 0.335878 | 0.108 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 56 | Batch: 000 / 021 | Total loss: 2.967 | Reg loss: 0.034 | Tree loss: 2.967 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 021 | Total loss: 3.077 | Reg loss: 0.034 | Tree loss: 3.077 | Accuracy: 0.328125 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 021 | Total loss: 2.919 | Reg loss: 0.034 | Tree loss: 2.919 | Accuracy: 0.355469 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 021 | Total loss: 2.878 | Reg loss: 0.034 | Tree loss: 2.878 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 021 | Total loss: 2.848 | Reg loss: 0.034 | Tree loss: 2.848 | Accuracy: 0.396484 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 021 | Total loss: 2.824 | Reg loss: 0.034 | Tree loss: 2.824 | Accuracy: 0.361328 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 021 | Total loss: 2.823 | Reg loss: 0.035 | Tree loss: 2.823 | Accuracy: 0.330078 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 021 | Total loss: 2.802 | Reg loss: 0.035 | Tree loss: 2.802 | Accuracy: 0.380859 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 021 | Total loss: 2.777 | Reg loss: 0.035 | Tree loss: 2.777 | Accuracy: 0.347656 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 021 | Total loss: 2.732 | Reg loss: 0.035 | Tree loss: 2.732 | Accuracy: 0.371094 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 021 | Total loss: 2.700 | Reg loss: 0.035 | Tree loss: 2.700 | Accuracy: 0.396484 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 021 | Total loss: 2.656 | Reg loss: 0.035 | Tree loss: 2.656 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 021 | Total loss: 2.597 | Reg loss: 0.035 | Tree loss: 2.597 | Accuracy: 0.396484 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 021 | Total loss: 2.571 | Reg loss: 0.036 | Tree loss: 2.571 | Accuracy: 0.416016 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 021 | Total loss: 2.543 | Reg loss: 0.036 | Tree loss: 2.543 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 021 | Total loss: 2.522 | Reg loss: 0.036 | Tree loss: 2.522 | Accuracy: 0.361328 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 021 | Total loss: 2.531 | Reg loss: 0.036 | Tree loss: 2.531 | Accuracy: 0.326172 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 021 | Total loss: 2.484 | Reg loss: 0.036 | Tree loss: 2.484 | Accuracy: 0.388672 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 021 | Total loss: 2.461 | Reg loss: 0.037 | Tree loss: 2.461 | Accuracy: 0.394531 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 021 | Total loss: 2.419 | Reg loss: 0.037 | Tree loss: 2.419 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 021 | Total loss: 2.342 | Reg loss: 0.037 | Tree loss: 2.342 | Accuracy: 0.381679 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 57 | Batch: 000 / 021 | Total loss: 2.927 | Reg loss: 0.035 | Tree loss: 2.927 | Accuracy: 0.335938 | 0.109 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57 | Batch: 001 / 021 | Total loss: 2.926 | Reg loss: 0.035 | Tree loss: 2.926 | Accuracy: 0.371094 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 021 | Total loss: 2.841 | Reg loss: 0.035 | Tree loss: 2.841 | Accuracy: 0.380859 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 021 | Total loss: 2.789 | Reg loss: 0.035 | Tree loss: 2.789 | Accuracy: 0.369141 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 021 | Total loss: 2.802 | Reg loss: 0.035 | Tree loss: 2.802 | Accuracy: 0.355469 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 021 | Total loss: 2.723 | Reg loss: 0.035 | Tree loss: 2.723 | Accuracy: 0.388672 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 021 | Total loss: 2.773 | Reg loss: 0.035 | Tree loss: 2.773 | Accuracy: 0.390625 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 021 | Total loss: 2.730 | Reg loss: 0.035 | Tree loss: 2.730 | Accuracy: 0.404297 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 021 | Total loss: 2.670 | Reg loss: 0.035 | Tree loss: 2.670 | Accuracy: 0.365234 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 021 | Total loss: 2.583 | Reg loss: 0.035 | Tree loss: 2.583 | Accuracy: 0.390625 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 021 | Total loss: 2.613 | Reg loss: 0.036 | Tree loss: 2.613 | Accuracy: 0.365234 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 021 | Total loss: 2.609 | Reg loss: 0.036 | Tree loss: 2.609 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 021 | Total loss: 2.520 | Reg loss: 0.036 | Tree loss: 2.520 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 021 | Total loss: 2.557 | Reg loss: 0.036 | Tree loss: 2.557 | Accuracy: 0.337891 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 021 | Total loss: 2.535 | Reg loss: 0.036 | Tree loss: 2.535 | Accuracy: 0.345703 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 021 | Total loss: 2.455 | Reg loss: 0.036 | Tree loss: 2.455 | Accuracy: 0.396484 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 021 | Total loss: 2.422 | Reg loss: 0.037 | Tree loss: 2.422 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 021 | Total loss: 2.415 | Reg loss: 0.037 | Tree loss: 2.415 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 021 | Total loss: 2.388 | Reg loss: 0.037 | Tree loss: 2.388 | Accuracy: 0.390625 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 021 | Total loss: 2.442 | Reg loss: 0.037 | Tree loss: 2.442 | Accuracy: 0.335938 | 0.109 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 021 | Total loss: 2.413 | Reg loss: 0.037 | Tree loss: 2.413 | Accuracy: 0.374046 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 58 | Batch: 000 / 021 | Total loss: 2.847 | Reg loss: 0.035 | Tree loss: 2.847 | Accuracy: 0.392578 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 021 | Total loss: 2.844 | Reg loss: 0.035 | Tree loss: 2.844 | Accuracy: 0.357422 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 021 | Total loss: 2.823 | Reg loss: 0.035 | Tree loss: 2.823 | Accuracy: 0.355469 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 021 | Total loss: 2.835 | Reg loss: 0.035 | Tree loss: 2.835 | Accuracy: 0.332031 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 021 | Total loss: 2.698 | Reg loss: 0.035 | Tree loss: 2.698 | Accuracy: 0.390625 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 021 | Total loss: 2.694 | Reg loss: 0.035 | Tree loss: 2.694 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 021 | Total loss: 2.655 | Reg loss: 0.035 | Tree loss: 2.655 | Accuracy: 0.369141 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 021 | Total loss: 2.724 | Reg loss: 0.036 | Tree loss: 2.724 | Accuracy: 0.328125 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 021 | Total loss: 2.585 | Reg loss: 0.036 | Tree loss: 2.585 | Accuracy: 0.376953 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 021 | Total loss: 2.591 | Reg loss: 0.036 | Tree loss: 2.591 | Accuracy: 0.388672 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 021 | Total loss: 2.562 | Reg loss: 0.036 | Tree loss: 2.562 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 021 | Total loss: 2.558 | Reg loss: 0.036 | Tree loss: 2.558 | Accuracy: 0.357422 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 021 | Total loss: 2.447 | Reg loss: 0.036 | Tree loss: 2.447 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 021 | Total loss: 2.400 | Reg loss: 0.036 | Tree loss: 2.400 | Accuracy: 0.396484 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 021 | Total loss: 2.459 | Reg loss: 0.037 | Tree loss: 2.459 | Accuracy: 0.361328 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 021 | Total loss: 2.397 | Reg loss: 0.037 | Tree loss: 2.397 | Accuracy: 0.380859 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 021 | Total loss: 2.352 | Reg loss: 0.037 | Tree loss: 2.352 | Accuracy: 0.388672 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 021 | Total loss: 2.378 | Reg loss: 0.037 | Tree loss: 2.378 | Accuracy: 0.351562 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 021 | Total loss: 2.323 | Reg loss: 0.037 | Tree loss: 2.323 | Accuracy: 0.400391 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 019 / 021 | Total loss: 2.257 | Reg loss: 0.038 | Tree loss: 2.257 | Accuracy: 0.398438 | 0.109 sec/iter\n",
      "Epoch: 58 | Batch: 020 / 021 | Total loss: 2.311 | Reg loss: 0.038 | Tree loss: 2.311 | Accuracy: 0.374046 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 59 | Batch: 000 / 021 | Total loss: 2.830 | Reg loss: 0.036 | Tree loss: 2.830 | Accuracy: 0.335938 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 021 | Total loss: 2.736 | Reg loss: 0.036 | Tree loss: 2.736 | Accuracy: 0.402344 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 021 | Total loss: 2.740 | Reg loss: 0.036 | Tree loss: 2.740 | Accuracy: 0.349609 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 021 | Total loss: 2.773 | Reg loss: 0.036 | Tree loss: 2.773 | Accuracy: 0.367188 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 021 | Total loss: 2.697 | Reg loss: 0.036 | Tree loss: 2.697 | Accuracy: 0.345703 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 021 | Total loss: 2.680 | Reg loss: 0.036 | Tree loss: 2.680 | Accuracy: 0.339844 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 021 | Total loss: 2.599 | Reg loss: 0.036 | Tree loss: 2.599 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 021 | Total loss: 2.608 | Reg loss: 0.036 | Tree loss: 2.608 | Accuracy: 0.353516 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 021 | Total loss: 2.554 | Reg loss: 0.036 | Tree loss: 2.554 | Accuracy: 0.371094 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 021 | Total loss: 2.495 | Reg loss: 0.036 | Tree loss: 2.495 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 021 | Total loss: 2.439 | Reg loss: 0.036 | Tree loss: 2.439 | Accuracy: 0.410156 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 021 | Total loss: 2.486 | Reg loss: 0.036 | Tree loss: 2.486 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 021 | Total loss: 2.474 | Reg loss: 0.037 | Tree loss: 2.474 | Accuracy: 0.333984 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 021 | Total loss: 2.463 | Reg loss: 0.037 | Tree loss: 2.463 | Accuracy: 0.375000 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 021 | Total loss: 2.392 | Reg loss: 0.037 | Tree loss: 2.392 | Accuracy: 0.330078 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 021 | Total loss: 2.308 | Reg loss: 0.037 | Tree loss: 2.308 | Accuracy: 0.375000 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 021 | Total loss: 2.301 | Reg loss: 0.037 | Tree loss: 2.301 | Accuracy: 0.376953 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 021 | Total loss: 2.219 | Reg loss: 0.037 | Tree loss: 2.219 | Accuracy: 0.398438 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 018 / 021 | Total loss: 2.223 | Reg loss: 0.038 | Tree loss: 2.223 | Accuracy: 0.439453 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 021 | Total loss: 2.245 | Reg loss: 0.038 | Tree loss: 2.245 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 021 | Total loss: 2.210 | Reg loss: 0.038 | Tree loss: 2.210 | Accuracy: 0.389313 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 | Batch: 000 / 021 | Total loss: 2.705 | Reg loss: 0.036 | Tree loss: 2.705 | Accuracy: 0.367188 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 021 | Total loss: 2.691 | Reg loss: 0.036 | Tree loss: 2.691 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 021 | Total loss: 2.663 | Reg loss: 0.036 | Tree loss: 2.663 | Accuracy: 0.351562 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 021 | Total loss: 2.610 | Reg loss: 0.036 | Tree loss: 2.610 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 021 | Total loss: 2.620 | Reg loss: 0.036 | Tree loss: 2.620 | Accuracy: 0.390625 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 021 | Total loss: 2.585 | Reg loss: 0.036 | Tree loss: 2.585 | Accuracy: 0.388672 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 021 | Total loss: 2.526 | Reg loss: 0.036 | Tree loss: 2.526 | Accuracy: 0.375000 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 021 | Total loss: 2.533 | Reg loss: 0.036 | Tree loss: 2.533 | Accuracy: 0.341797 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 021 | Total loss: 2.490 | Reg loss: 0.036 | Tree loss: 2.490 | Accuracy: 0.404297 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 021 | Total loss: 2.459 | Reg loss: 0.037 | Tree loss: 2.459 | Accuracy: 0.361328 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 021 | Total loss: 2.455 | Reg loss: 0.037 | Tree loss: 2.455 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 021 | Total loss: 2.489 | Reg loss: 0.037 | Tree loss: 2.489 | Accuracy: 0.339844 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 021 | Total loss: 2.378 | Reg loss: 0.037 | Tree loss: 2.378 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 021 | Total loss: 2.336 | Reg loss: 0.037 | Tree loss: 2.336 | Accuracy: 0.347656 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 021 | Total loss: 2.327 | Reg loss: 0.037 | Tree loss: 2.327 | Accuracy: 0.367188 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 021 | Total loss: 2.307 | Reg loss: 0.037 | Tree loss: 2.307 | Accuracy: 0.376953 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 021 | Total loss: 2.236 | Reg loss: 0.038 | Tree loss: 2.236 | Accuracy: 0.419922 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 021 | Total loss: 2.249 | Reg loss: 0.038 | Tree loss: 2.249 | Accuracy: 0.365234 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 021 | Total loss: 2.295 | Reg loss: 0.038 | Tree loss: 2.295 | Accuracy: 0.324219 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 021 | Total loss: 2.182 | Reg loss: 0.038 | Tree loss: 2.182 | Accuracy: 0.398438 | 0.109 sec/iter\n",
      "Epoch: 60 | Batch: 020 / 021 | Total loss: 2.216 | Reg loss: 0.038 | Tree loss: 2.216 | Accuracy: 0.335878 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 61 | Batch: 000 / 021 | Total loss: 2.660 | Reg loss: 0.036 | Tree loss: 2.660 | Accuracy: 0.347656 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 021 | Total loss: 2.627 | Reg loss: 0.036 | Tree loss: 2.627 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 021 | Total loss: 2.583 | Reg loss: 0.036 | Tree loss: 2.583 | Accuracy: 0.396484 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 021 | Total loss: 2.570 | Reg loss: 0.036 | Tree loss: 2.570 | Accuracy: 0.390625 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 021 | Total loss: 2.560 | Reg loss: 0.036 | Tree loss: 2.560 | Accuracy: 0.388672 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 021 | Total loss: 2.547 | Reg loss: 0.036 | Tree loss: 2.547 | Accuracy: 0.351562 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 021 | Total loss: 2.464 | Reg loss: 0.036 | Tree loss: 2.464 | Accuracy: 0.392578 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 021 | Total loss: 2.525 | Reg loss: 0.037 | Tree loss: 2.525 | Accuracy: 0.343750 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 021 | Total loss: 2.541 | Reg loss: 0.037 | Tree loss: 2.541 | Accuracy: 0.351562 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 021 | Total loss: 2.366 | Reg loss: 0.037 | Tree loss: 2.366 | Accuracy: 0.380859 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 021 | Total loss: 2.404 | Reg loss: 0.037 | Tree loss: 2.404 | Accuracy: 0.369141 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 021 | Total loss: 2.299 | Reg loss: 0.037 | Tree loss: 2.299 | Accuracy: 0.408203 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 021 | Total loss: 2.288 | Reg loss: 0.037 | Tree loss: 2.288 | Accuracy: 0.402344 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 021 | Total loss: 2.328 | Reg loss: 0.037 | Tree loss: 2.328 | Accuracy: 0.326172 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 021 | Total loss: 2.292 | Reg loss: 0.037 | Tree loss: 2.292 | Accuracy: 0.376953 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 021 | Total loss: 2.265 | Reg loss: 0.038 | Tree loss: 2.265 | Accuracy: 0.367188 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 021 | Total loss: 2.192 | Reg loss: 0.038 | Tree loss: 2.192 | Accuracy: 0.361328 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 021 | Total loss: 2.251 | Reg loss: 0.038 | Tree loss: 2.251 | Accuracy: 0.357422 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 021 | Total loss: 2.191 | Reg loss: 0.038 | Tree loss: 2.191 | Accuracy: 0.380859 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 021 | Total loss: 2.130 | Reg loss: 0.038 | Tree loss: 2.130 | Accuracy: 0.361328 | 0.109 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 021 | Total loss: 2.177 | Reg loss: 0.038 | Tree loss: 2.177 | Accuracy: 0.343511 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 62 | Batch: 000 / 021 | Total loss: 2.624 | Reg loss: 0.037 | Tree loss: 2.624 | Accuracy: 0.341797 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 021 | Total loss: 2.606 | Reg loss: 0.037 | Tree loss: 2.606 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 021 | Total loss: 2.549 | Reg loss: 0.037 | Tree loss: 2.549 | Accuracy: 0.390625 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 021 | Total loss: 2.503 | Reg loss: 0.037 | Tree loss: 2.503 | Accuracy: 0.357422 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 021 | Total loss: 2.515 | Reg loss: 0.037 | Tree loss: 2.515 | Accuracy: 0.367188 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 021 | Total loss: 2.445 | Reg loss: 0.037 | Tree loss: 2.445 | Accuracy: 0.400391 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 021 | Total loss: 2.444 | Reg loss: 0.037 | Tree loss: 2.444 | Accuracy: 0.400391 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 021 | Total loss: 2.463 | Reg loss: 0.037 | Tree loss: 2.463 | Accuracy: 0.341797 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 021 | Total loss: 2.403 | Reg loss: 0.037 | Tree loss: 2.403 | Accuracy: 0.343750 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 021 | Total loss: 2.340 | Reg loss: 0.037 | Tree loss: 2.340 | Accuracy: 0.380859 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 021 | Total loss: 2.397 | Reg loss: 0.037 | Tree loss: 2.397 | Accuracy: 0.376953 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 021 | Total loss: 2.278 | Reg loss: 0.037 | Tree loss: 2.278 | Accuracy: 0.353516 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 021 | Total loss: 2.212 | Reg loss: 0.037 | Tree loss: 2.212 | Accuracy: 0.394531 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 021 | Total loss: 2.265 | Reg loss: 0.038 | Tree loss: 2.265 | Accuracy: 0.359375 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 021 | Total loss: 2.206 | Reg loss: 0.038 | Tree loss: 2.206 | Accuracy: 0.376953 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 021 | Total loss: 2.194 | Reg loss: 0.038 | Tree loss: 2.194 | Accuracy: 0.375000 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 021 | Total loss: 2.154 | Reg loss: 0.038 | Tree loss: 2.154 | Accuracy: 0.390625 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 021 | Total loss: 2.185 | Reg loss: 0.038 | Tree loss: 2.185 | Accuracy: 0.365234 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 021 | Total loss: 2.138 | Reg loss: 0.038 | Tree loss: 2.138 | Accuracy: 0.394531 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 021 | Total loss: 2.134 | Reg loss: 0.038 | Tree loss: 2.134 | Accuracy: 0.371094 | 0.109 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 021 | Total loss: 2.263 | Reg loss: 0.039 | Tree loss: 2.263 | Accuracy: 0.320611 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63 | Batch: 000 / 021 | Total loss: 2.516 | Reg loss: 0.037 | Tree loss: 2.516 | Accuracy: 0.367188 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 021 | Total loss: 2.506 | Reg loss: 0.037 | Tree loss: 2.506 | Accuracy: 0.408203 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 021 | Total loss: 2.494 | Reg loss: 0.037 | Tree loss: 2.494 | Accuracy: 0.369141 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 021 | Total loss: 2.520 | Reg loss: 0.037 | Tree loss: 2.520 | Accuracy: 0.337891 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 021 | Total loss: 2.446 | Reg loss: 0.037 | Tree loss: 2.446 | Accuracy: 0.361328 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 021 | Total loss: 2.431 | Reg loss: 0.037 | Tree loss: 2.431 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 021 | Total loss: 2.477 | Reg loss: 0.037 | Tree loss: 2.477 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 021 | Total loss: 2.382 | Reg loss: 0.037 | Tree loss: 2.382 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 021 | Total loss: 2.342 | Reg loss: 0.037 | Tree loss: 2.342 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 021 | Total loss: 2.288 | Reg loss: 0.037 | Tree loss: 2.288 | Accuracy: 0.380859 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 021 | Total loss: 2.242 | Reg loss: 0.037 | Tree loss: 2.242 | Accuracy: 0.402344 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 021 | Total loss: 2.285 | Reg loss: 0.037 | Tree loss: 2.285 | Accuracy: 0.312500 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 021 | Total loss: 2.223 | Reg loss: 0.038 | Tree loss: 2.223 | Accuracy: 0.355469 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 021 | Total loss: 2.213 | Reg loss: 0.038 | Tree loss: 2.213 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 021 | Total loss: 2.199 | Reg loss: 0.038 | Tree loss: 2.199 | Accuracy: 0.324219 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 021 | Total loss: 2.142 | Reg loss: 0.038 | Tree loss: 2.142 | Accuracy: 0.402344 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 021 | Total loss: 2.118 | Reg loss: 0.038 | Tree loss: 2.118 | Accuracy: 0.392578 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 021 | Total loss: 2.145 | Reg loss: 0.038 | Tree loss: 2.145 | Accuracy: 0.376953 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 021 | Total loss: 2.132 | Reg loss: 0.038 | Tree loss: 2.132 | Accuracy: 0.392578 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 021 | Total loss: 2.043 | Reg loss: 0.039 | Tree loss: 2.043 | Accuracy: 0.398438 | 0.109 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 021 | Total loss: 2.170 | Reg loss: 0.039 | Tree loss: 2.170 | Accuracy: 0.312977 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 64 | Batch: 000 / 021 | Total loss: 2.496 | Reg loss: 0.037 | Tree loss: 2.496 | Accuracy: 0.349609 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 021 | Total loss: 2.533 | Reg loss: 0.037 | Tree loss: 2.533 | Accuracy: 0.335938 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 021 | Total loss: 2.480 | Reg loss: 0.037 | Tree loss: 2.480 | Accuracy: 0.376953 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 021 | Total loss: 2.410 | Reg loss: 0.037 | Tree loss: 2.410 | Accuracy: 0.388672 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 021 | Total loss: 2.455 | Reg loss: 0.037 | Tree loss: 2.455 | Accuracy: 0.390625 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 021 | Total loss: 2.347 | Reg loss: 0.037 | Tree loss: 2.347 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 021 | Total loss: 2.390 | Reg loss: 0.037 | Tree loss: 2.390 | Accuracy: 0.333984 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 021 | Total loss: 2.364 | Reg loss: 0.037 | Tree loss: 2.364 | Accuracy: 0.353516 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 021 | Total loss: 2.290 | Reg loss: 0.037 | Tree loss: 2.290 | Accuracy: 0.402344 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 021 | Total loss: 2.269 | Reg loss: 0.037 | Tree loss: 2.269 | Accuracy: 0.345703 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 021 | Total loss: 2.230 | Reg loss: 0.037 | Tree loss: 2.230 | Accuracy: 0.376953 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 021 | Total loss: 2.178 | Reg loss: 0.038 | Tree loss: 2.178 | Accuracy: 0.367188 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 021 | Total loss: 2.157 | Reg loss: 0.038 | Tree loss: 2.157 | Accuracy: 0.382812 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 021 | Total loss: 2.156 | Reg loss: 0.038 | Tree loss: 2.156 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 021 | Total loss: 2.188 | Reg loss: 0.038 | Tree loss: 2.188 | Accuracy: 0.392578 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 021 | Total loss: 2.110 | Reg loss: 0.038 | Tree loss: 2.110 | Accuracy: 0.369141 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 021 | Total loss: 2.096 | Reg loss: 0.038 | Tree loss: 2.096 | Accuracy: 0.357422 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 021 | Total loss: 2.064 | Reg loss: 0.038 | Tree loss: 2.064 | Accuracy: 0.369141 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 021 | Total loss: 2.062 | Reg loss: 0.039 | Tree loss: 2.062 | Accuracy: 0.402344 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 019 / 021 | Total loss: 2.032 | Reg loss: 0.039 | Tree loss: 2.032 | Accuracy: 0.365234 | 0.109 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 021 | Total loss: 1.929 | Reg loss: 0.039 | Tree loss: 1.929 | Accuracy: 0.404580 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 65 | Batch: 000 / 021 | Total loss: 2.458 | Reg loss: 0.037 | Tree loss: 2.458 | Accuracy: 0.359375 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 021 | Total loss: 2.376 | Reg loss: 0.037 | Tree loss: 2.376 | Accuracy: 0.414062 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 021 | Total loss: 2.409 | Reg loss: 0.037 | Tree loss: 2.409 | Accuracy: 0.355469 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 021 | Total loss: 2.405 | Reg loss: 0.037 | Tree loss: 2.405 | Accuracy: 0.365234 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 021 | Total loss: 2.370 | Reg loss: 0.037 | Tree loss: 2.370 | Accuracy: 0.361328 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 021 | Total loss: 2.266 | Reg loss: 0.037 | Tree loss: 2.266 | Accuracy: 0.376953 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 021 | Total loss: 2.347 | Reg loss: 0.037 | Tree loss: 2.347 | Accuracy: 0.367188 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 021 | Total loss: 2.307 | Reg loss: 0.037 | Tree loss: 2.307 | Accuracy: 0.355469 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 021 | Total loss: 2.298 | Reg loss: 0.037 | Tree loss: 2.298 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 021 | Total loss: 2.203 | Reg loss: 0.038 | Tree loss: 2.203 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 021 | Total loss: 2.165 | Reg loss: 0.038 | Tree loss: 2.165 | Accuracy: 0.390625 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 021 | Total loss: 2.189 | Reg loss: 0.038 | Tree loss: 2.189 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 021 | Total loss: 2.201 | Reg loss: 0.038 | Tree loss: 2.201 | Accuracy: 0.371094 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 021 | Total loss: 2.174 | Reg loss: 0.038 | Tree loss: 2.174 | Accuracy: 0.339844 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 021 | Total loss: 2.112 | Reg loss: 0.038 | Tree loss: 2.112 | Accuracy: 0.382812 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 021 | Total loss: 2.077 | Reg loss: 0.038 | Tree loss: 2.077 | Accuracy: 0.365234 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 021 | Total loss: 2.044 | Reg loss: 0.038 | Tree loss: 2.044 | Accuracy: 0.355469 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 021 | Total loss: 2.022 | Reg loss: 0.038 | Tree loss: 2.022 | Accuracy: 0.412109 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 021 | Total loss: 2.017 | Reg loss: 0.039 | Tree loss: 2.017 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 021 | Total loss: 2.003 | Reg loss: 0.039 | Tree loss: 2.003 | Accuracy: 0.388672 | 0.109 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 021 | Total loss: 2.035 | Reg loss: 0.039 | Tree loss: 2.035 | Accuracy: 0.366412 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 | Batch: 000 / 021 | Total loss: 2.394 | Reg loss: 0.037 | Tree loss: 2.394 | Accuracy: 0.388672 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 021 | Total loss: 2.390 | Reg loss: 0.037 | Tree loss: 2.390 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 021 | Total loss: 2.383 | Reg loss: 0.037 | Tree loss: 2.383 | Accuracy: 0.380859 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 021 | Total loss: 2.329 | Reg loss: 0.037 | Tree loss: 2.329 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 021 | Total loss: 2.297 | Reg loss: 0.037 | Tree loss: 2.297 | Accuracy: 0.349609 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 021 | Total loss: 2.301 | Reg loss: 0.037 | Tree loss: 2.301 | Accuracy: 0.351562 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 021 | Total loss: 2.220 | Reg loss: 0.037 | Tree loss: 2.220 | Accuracy: 0.400391 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 021 | Total loss: 2.207 | Reg loss: 0.038 | Tree loss: 2.207 | Accuracy: 0.376953 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 021 | Total loss: 2.209 | Reg loss: 0.038 | Tree loss: 2.209 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 021 | Total loss: 2.182 | Reg loss: 0.038 | Tree loss: 2.182 | Accuracy: 0.396484 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 021 | Total loss: 2.127 | Reg loss: 0.038 | Tree loss: 2.127 | Accuracy: 0.396484 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 021 | Total loss: 2.203 | Reg loss: 0.038 | Tree loss: 2.203 | Accuracy: 0.335938 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 021 | Total loss: 2.127 | Reg loss: 0.038 | Tree loss: 2.127 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 021 | Total loss: 2.070 | Reg loss: 0.038 | Tree loss: 2.070 | Accuracy: 0.361328 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 021 | Total loss: 2.085 | Reg loss: 0.038 | Tree loss: 2.085 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 021 | Total loss: 2.026 | Reg loss: 0.038 | Tree loss: 2.026 | Accuracy: 0.382812 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 021 | Total loss: 2.081 | Reg loss: 0.038 | Tree loss: 2.081 | Accuracy: 0.330078 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 021 | Total loss: 2.025 | Reg loss: 0.039 | Tree loss: 2.025 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 021 | Total loss: 1.991 | Reg loss: 0.039 | Tree loss: 1.991 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 021 | Total loss: 2.000 | Reg loss: 0.039 | Tree loss: 2.000 | Accuracy: 0.353516 | 0.109 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 021 | Total loss: 1.978 | Reg loss: 0.039 | Tree loss: 1.978 | Accuracy: 0.343511 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 67 | Batch: 000 / 021 | Total loss: 2.383 | Reg loss: 0.037 | Tree loss: 2.383 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 021 | Total loss: 2.340 | Reg loss: 0.037 | Tree loss: 2.340 | Accuracy: 0.398438 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 021 | Total loss: 2.401 | Reg loss: 0.037 | Tree loss: 2.401 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 021 | Total loss: 2.270 | Reg loss: 0.037 | Tree loss: 2.270 | Accuracy: 0.410156 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 021 | Total loss: 2.294 | Reg loss: 0.037 | Tree loss: 2.294 | Accuracy: 0.347656 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 021 | Total loss: 2.231 | Reg loss: 0.037 | Tree loss: 2.231 | Accuracy: 0.382812 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 021 | Total loss: 2.273 | Reg loss: 0.038 | Tree loss: 2.273 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 021 | Total loss: 2.212 | Reg loss: 0.038 | Tree loss: 2.212 | Accuracy: 0.343750 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 021 | Total loss: 2.176 | Reg loss: 0.038 | Tree loss: 2.176 | Accuracy: 0.335938 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 021 | Total loss: 2.144 | Reg loss: 0.038 | Tree loss: 2.144 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 021 | Total loss: 2.115 | Reg loss: 0.038 | Tree loss: 2.115 | Accuracy: 0.375000 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 021 | Total loss: 2.139 | Reg loss: 0.038 | Tree loss: 2.139 | Accuracy: 0.359375 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 021 | Total loss: 2.076 | Reg loss: 0.038 | Tree loss: 2.076 | Accuracy: 0.369141 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 021 | Total loss: 2.109 | Reg loss: 0.038 | Tree loss: 2.109 | Accuracy: 0.367188 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 021 | Total loss: 2.028 | Reg loss: 0.038 | Tree loss: 2.028 | Accuracy: 0.371094 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 021 | Total loss: 2.003 | Reg loss: 0.038 | Tree loss: 2.003 | Accuracy: 0.414062 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 021 | Total loss: 1.957 | Reg loss: 0.039 | Tree loss: 1.957 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 021 | Total loss: 1.904 | Reg loss: 0.039 | Tree loss: 1.904 | Accuracy: 0.388672 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 021 | Total loss: 1.913 | Reg loss: 0.039 | Tree loss: 1.913 | Accuracy: 0.371094 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 021 | Total loss: 1.962 | Reg loss: 0.039 | Tree loss: 1.962 | Accuracy: 0.349609 | 0.109 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 021 | Total loss: 1.920 | Reg loss: 0.039 | Tree loss: 1.920 | Accuracy: 0.328244 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 68 | Batch: 000 / 021 | Total loss: 2.334 | Reg loss: 0.037 | Tree loss: 2.334 | Accuracy: 0.392578 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 021 | Total loss: 2.309 | Reg loss: 0.037 | Tree loss: 2.309 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 021 | Total loss: 2.280 | Reg loss: 0.037 | Tree loss: 2.280 | Accuracy: 0.380859 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 021 | Total loss: 2.274 | Reg loss: 0.037 | Tree loss: 2.274 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 021 | Total loss: 2.235 | Reg loss: 0.038 | Tree loss: 2.235 | Accuracy: 0.347656 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 021 | Total loss: 2.184 | Reg loss: 0.038 | Tree loss: 2.184 | Accuracy: 0.380859 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 021 | Total loss: 2.156 | Reg loss: 0.038 | Tree loss: 2.156 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 021 | Total loss: 2.198 | Reg loss: 0.038 | Tree loss: 2.198 | Accuracy: 0.351562 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 021 | Total loss: 2.139 | Reg loss: 0.038 | Tree loss: 2.139 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 021 | Total loss: 2.155 | Reg loss: 0.038 | Tree loss: 2.155 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 021 | Total loss: 2.111 | Reg loss: 0.038 | Tree loss: 2.111 | Accuracy: 0.318359 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 021 | Total loss: 2.084 | Reg loss: 0.038 | Tree loss: 2.084 | Accuracy: 0.375000 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 021 | Total loss: 2.040 | Reg loss: 0.038 | Tree loss: 2.040 | Accuracy: 0.392578 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 021 | Total loss: 2.040 | Reg loss: 0.038 | Tree loss: 2.040 | Accuracy: 0.371094 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 021 | Total loss: 2.035 | Reg loss: 0.038 | Tree loss: 2.035 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 021 | Total loss: 1.987 | Reg loss: 0.038 | Tree loss: 1.987 | Accuracy: 0.359375 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 021 | Total loss: 1.927 | Reg loss: 0.039 | Tree loss: 1.927 | Accuracy: 0.357422 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 021 | Total loss: 1.915 | Reg loss: 0.039 | Tree loss: 1.915 | Accuracy: 0.376953 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 021 | Total loss: 1.896 | Reg loss: 0.039 | Tree loss: 1.896 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 021 | Total loss: 1.936 | Reg loss: 0.039 | Tree loss: 1.936 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 021 | Total loss: 1.829 | Reg loss: 0.039 | Tree loss: 1.829 | Accuracy: 0.442748 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 | Batch: 000 / 021 | Total loss: 2.338 | Reg loss: 0.038 | Tree loss: 2.338 | Accuracy: 0.376953 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 021 | Total loss: 2.227 | Reg loss: 0.038 | Tree loss: 2.227 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 021 | Total loss: 2.236 | Reg loss: 0.038 | Tree loss: 2.236 | Accuracy: 0.369141 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 021 | Total loss: 2.207 | Reg loss: 0.038 | Tree loss: 2.207 | Accuracy: 0.390625 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 021 | Total loss: 2.180 | Reg loss: 0.038 | Tree loss: 2.180 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 021 | Total loss: 2.149 | Reg loss: 0.038 | Tree loss: 2.149 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 021 | Total loss: 2.122 | Reg loss: 0.038 | Tree loss: 2.122 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 021 | Total loss: 2.119 | Reg loss: 0.038 | Tree loss: 2.119 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 021 | Total loss: 2.103 | Reg loss: 0.038 | Tree loss: 2.103 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 021 | Total loss: 2.088 | Reg loss: 0.038 | Tree loss: 2.088 | Accuracy: 0.353516 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 021 | Total loss: 2.022 | Reg loss: 0.038 | Tree loss: 2.022 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 021 | Total loss: 2.023 | Reg loss: 0.038 | Tree loss: 2.023 | Accuracy: 0.355469 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 021 | Total loss: 2.010 | Reg loss: 0.038 | Tree loss: 2.010 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 021 | Total loss: 2.032 | Reg loss: 0.038 | Tree loss: 2.032 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 021 | Total loss: 2.012 | Reg loss: 0.038 | Tree loss: 2.012 | Accuracy: 0.380859 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 021 | Total loss: 2.025 | Reg loss: 0.038 | Tree loss: 2.025 | Accuracy: 0.335938 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 021 | Total loss: 1.932 | Reg loss: 0.039 | Tree loss: 1.932 | Accuracy: 0.332031 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 021 | Total loss: 1.937 | Reg loss: 0.039 | Tree loss: 1.937 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 021 | Total loss: 1.891 | Reg loss: 0.039 | Tree loss: 1.891 | Accuracy: 0.369141 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 021 | Total loss: 1.896 | Reg loss: 0.039 | Tree loss: 1.896 | Accuracy: 0.400391 | 0.109 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 021 | Total loss: 1.833 | Reg loss: 0.039 | Tree loss: 1.833 | Accuracy: 0.374046 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 70 | Batch: 000 / 021 | Total loss: 2.272 | Reg loss: 0.038 | Tree loss: 2.272 | Accuracy: 0.382812 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 021 | Total loss: 2.200 | Reg loss: 0.038 | Tree loss: 2.200 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 021 | Total loss: 2.221 | Reg loss: 0.038 | Tree loss: 2.221 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 021 | Total loss: 2.186 | Reg loss: 0.038 | Tree loss: 2.186 | Accuracy: 0.365234 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 021 | Total loss: 2.196 | Reg loss: 0.038 | Tree loss: 2.196 | Accuracy: 0.347656 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 021 | Total loss: 2.197 | Reg loss: 0.038 | Tree loss: 2.197 | Accuracy: 0.318359 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 021 | Total loss: 2.120 | Reg loss: 0.038 | Tree loss: 2.120 | Accuracy: 0.369141 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 021 | Total loss: 2.111 | Reg loss: 0.038 | Tree loss: 2.111 | Accuracy: 0.369141 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 021 | Total loss: 2.094 | Reg loss: 0.038 | Tree loss: 2.094 | Accuracy: 0.353516 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 021 | Total loss: 2.074 | Reg loss: 0.038 | Tree loss: 2.074 | Accuracy: 0.349609 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 021 | Total loss: 1.999 | Reg loss: 0.038 | Tree loss: 1.999 | Accuracy: 0.431641 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 021 | Total loss: 1.982 | Reg loss: 0.038 | Tree loss: 1.982 | Accuracy: 0.414062 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 021 | Total loss: 1.958 | Reg loss: 0.038 | Tree loss: 1.958 | Accuracy: 0.380859 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 021 | Total loss: 1.925 | Reg loss: 0.038 | Tree loss: 1.925 | Accuracy: 0.367188 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 021 | Total loss: 1.905 | Reg loss: 0.038 | Tree loss: 1.905 | Accuracy: 0.376953 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 021 | Total loss: 1.929 | Reg loss: 0.039 | Tree loss: 1.929 | Accuracy: 0.369141 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 021 | Total loss: 1.922 | Reg loss: 0.039 | Tree loss: 1.922 | Accuracy: 0.394531 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 021 | Total loss: 1.894 | Reg loss: 0.039 | Tree loss: 1.894 | Accuracy: 0.371094 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 021 | Total loss: 1.878 | Reg loss: 0.039 | Tree loss: 1.878 | Accuracy: 0.355469 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 021 | Total loss: 1.854 | Reg loss: 0.039 | Tree loss: 1.854 | Accuracy: 0.337891 | 0.109 sec/iter\n",
      "Epoch: 70 | Batch: 020 / 021 | Total loss: 1.780 | Reg loss: 0.039 | Tree loss: 1.780 | Accuracy: 0.404580 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 71 | Batch: 000 / 021 | Total loss: 2.264 | Reg loss: 0.038 | Tree loss: 2.264 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 021 | Total loss: 2.198 | Reg loss: 0.038 | Tree loss: 2.198 | Accuracy: 0.332031 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 021 | Total loss: 2.231 | Reg loss: 0.038 | Tree loss: 2.231 | Accuracy: 0.349609 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 021 | Total loss: 2.119 | Reg loss: 0.038 | Tree loss: 2.119 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 021 | Total loss: 2.113 | Reg loss: 0.038 | Tree loss: 2.113 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 021 | Total loss: 2.131 | Reg loss: 0.038 | Tree loss: 2.131 | Accuracy: 0.343750 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 021 | Total loss: 2.100 | Reg loss: 0.038 | Tree loss: 2.100 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 021 | Total loss: 2.068 | Reg loss: 0.038 | Tree loss: 2.068 | Accuracy: 0.355469 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 021 | Total loss: 2.054 | Reg loss: 0.038 | Tree loss: 2.054 | Accuracy: 0.357422 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 021 | Total loss: 2.028 | Reg loss: 0.038 | Tree loss: 2.028 | Accuracy: 0.371094 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 021 | Total loss: 2.014 | Reg loss: 0.038 | Tree loss: 2.014 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 021 | Total loss: 1.945 | Reg loss: 0.038 | Tree loss: 1.945 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 021 | Total loss: 1.930 | Reg loss: 0.038 | Tree loss: 1.930 | Accuracy: 0.369141 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 021 | Total loss: 1.921 | Reg loss: 0.038 | Tree loss: 1.921 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 021 | Total loss: 1.899 | Reg loss: 0.038 | Tree loss: 1.899 | Accuracy: 0.375000 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 021 | Total loss: 1.919 | Reg loss: 0.039 | Tree loss: 1.919 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 021 | Total loss: 1.837 | Reg loss: 0.039 | Tree loss: 1.837 | Accuracy: 0.408203 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 021 | Total loss: 1.843 | Reg loss: 0.039 | Tree loss: 1.843 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 021 | Total loss: 1.897 | Reg loss: 0.039 | Tree loss: 1.897 | Accuracy: 0.365234 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 021 | Total loss: 1.814 | Reg loss: 0.039 | Tree loss: 1.814 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 021 | Total loss: 1.794 | Reg loss: 0.039 | Tree loss: 1.794 | Accuracy: 0.320611 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 | Batch: 000 / 021 | Total loss: 2.179 | Reg loss: 0.038 | Tree loss: 2.179 | Accuracy: 0.390625 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 021 | Total loss: 2.211 | Reg loss: 0.038 | Tree loss: 2.211 | Accuracy: 0.332031 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 021 | Total loss: 2.143 | Reg loss: 0.038 | Tree loss: 2.143 | Accuracy: 0.412109 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 021 | Total loss: 2.087 | Reg loss: 0.038 | Tree loss: 2.087 | Accuracy: 0.371094 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 021 | Total loss: 2.067 | Reg loss: 0.038 | Tree loss: 2.067 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 021 | Total loss: 2.122 | Reg loss: 0.038 | Tree loss: 2.122 | Accuracy: 0.324219 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 021 | Total loss: 2.103 | Reg loss: 0.038 | Tree loss: 2.103 | Accuracy: 0.339844 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 021 | Total loss: 2.051 | Reg loss: 0.038 | Tree loss: 2.051 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 021 | Total loss: 2.040 | Reg loss: 0.038 | Tree loss: 2.040 | Accuracy: 0.380859 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 021 | Total loss: 1.975 | Reg loss: 0.038 | Tree loss: 1.975 | Accuracy: 0.408203 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 021 | Total loss: 1.923 | Reg loss: 0.038 | Tree loss: 1.923 | Accuracy: 0.337891 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 021 | Total loss: 1.906 | Reg loss: 0.038 | Tree loss: 1.906 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 021 | Total loss: 1.992 | Reg loss: 0.038 | Tree loss: 1.992 | Accuracy: 0.353516 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 021 | Total loss: 1.865 | Reg loss: 0.038 | Tree loss: 1.865 | Accuracy: 0.390625 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 021 | Total loss: 1.868 | Reg loss: 0.038 | Tree loss: 1.868 | Accuracy: 0.398438 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 021 | Total loss: 1.927 | Reg loss: 0.039 | Tree loss: 1.927 | Accuracy: 0.351562 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 021 | Total loss: 1.868 | Reg loss: 0.039 | Tree loss: 1.868 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 021 | Total loss: 1.777 | Reg loss: 0.039 | Tree loss: 1.777 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 021 | Total loss: 1.778 | Reg loss: 0.039 | Tree loss: 1.778 | Accuracy: 0.394531 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 021 | Total loss: 1.844 | Reg loss: 0.039 | Tree loss: 1.844 | Accuracy: 0.355469 | 0.109 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 021 | Total loss: 1.870 | Reg loss: 0.039 | Tree loss: 1.870 | Accuracy: 0.358779 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 73 | Batch: 000 / 021 | Total loss: 2.134 | Reg loss: 0.038 | Tree loss: 2.134 | Accuracy: 0.408203 | 0.11 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 021 | Total loss: 2.125 | Reg loss: 0.038 | Tree loss: 2.125 | Accuracy: 0.355469 | 0.11 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 021 | Total loss: 2.084 | Reg loss: 0.038 | Tree loss: 2.084 | Accuracy: 0.357422 | 0.109 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 021 | Total loss: 2.097 | Reg loss: 0.038 | Tree loss: 2.097 | Accuracy: 0.373047 | 0.109 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 021 | Total loss: 2.092 | Reg loss: 0.038 | Tree loss: 2.092 | Accuracy: 0.375000 | 0.109 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 021 | Total loss: 2.059 | Reg loss: 0.038 | Tree loss: 2.059 | Accuracy: 0.343750 | 0.109 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 021 | Total loss: 2.040 | Reg loss: 0.038 | Tree loss: 2.040 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 021 | Total loss: 2.021 | Reg loss: 0.038 | Tree loss: 2.021 | Accuracy: 0.400391 | 0.109 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 021 | Total loss: 1.972 | Reg loss: 0.038 | Tree loss: 1.972 | Accuracy: 0.382812 | 0.109 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 021 | Total loss: 1.983 | Reg loss: 0.038 | Tree loss: 1.983 | Accuracy: 0.369141 | 0.109 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 021 | Total loss: 1.924 | Reg loss: 0.038 | Tree loss: 1.924 | Accuracy: 0.386719 | 0.11 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 021 | Total loss: 1.883 | Reg loss: 0.038 | Tree loss: 1.883 | Accuracy: 0.394531 | 0.11 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 021 | Total loss: 1.910 | Reg loss: 0.038 | Tree loss: 1.910 | Accuracy: 0.353516 | 0.11 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 021 | Total loss: 1.893 | Reg loss: 0.038 | Tree loss: 1.893 | Accuracy: 0.380859 | 0.11 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 021 | Total loss: 1.873 | Reg loss: 0.038 | Tree loss: 1.873 | Accuracy: 0.363281 | 0.11 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 021 | Total loss: 1.839 | Reg loss: 0.039 | Tree loss: 1.839 | Accuracy: 0.345703 | 0.11 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 021 | Total loss: 1.865 | Reg loss: 0.039 | Tree loss: 1.865 | Accuracy: 0.373047 | 0.11 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 021 | Total loss: 1.811 | Reg loss: 0.039 | Tree loss: 1.811 | Accuracy: 0.382812 | 0.11 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 021 | Total loss: 1.803 | Reg loss: 0.039 | Tree loss: 1.803 | Accuracy: 0.363281 | 0.11 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 021 | Total loss: 1.819 | Reg loss: 0.039 | Tree loss: 1.819 | Accuracy: 0.376953 | 0.11 sec/iter\n",
      "Epoch: 73 | Batch: 020 / 021 | Total loss: 1.672 | Reg loss: 0.039 | Tree loss: 1.672 | Accuracy: 0.305344 | 0.11 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 74 | Batch: 000 / 021 | Total loss: 2.186 | Reg loss: 0.038 | Tree loss: 2.186 | Accuracy: 0.367188 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 021 | Total loss: 2.114 | Reg loss: 0.038 | Tree loss: 2.114 | Accuracy: 0.355469 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 021 | Total loss: 2.144 | Reg loss: 0.038 | Tree loss: 2.144 | Accuracy: 0.371094 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 021 | Total loss: 2.084 | Reg loss: 0.038 | Tree loss: 2.084 | Accuracy: 0.384766 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 021 | Total loss: 2.016 | Reg loss: 0.038 | Tree loss: 2.016 | Accuracy: 0.390625 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 021 | Total loss: 2.014 | Reg loss: 0.038 | Tree loss: 2.014 | Accuracy: 0.375000 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 021 | Total loss: 2.000 | Reg loss: 0.038 | Tree loss: 2.000 | Accuracy: 0.371094 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 021 | Total loss: 1.963 | Reg loss: 0.038 | Tree loss: 1.963 | Accuracy: 0.345703 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 021 | Total loss: 1.949 | Reg loss: 0.038 | Tree loss: 1.949 | Accuracy: 0.326172 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 021 | Total loss: 1.956 | Reg loss: 0.038 | Tree loss: 1.956 | Accuracy: 0.373047 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 021 | Total loss: 1.885 | Reg loss: 0.038 | Tree loss: 1.885 | Accuracy: 0.337891 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 021 | Total loss: 1.864 | Reg loss: 0.038 | Tree loss: 1.864 | Accuracy: 0.363281 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 021 | Total loss: 1.863 | Reg loss: 0.038 | Tree loss: 1.863 | Accuracy: 0.373047 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 021 | Total loss: 1.851 | Reg loss: 0.038 | Tree loss: 1.851 | Accuracy: 0.320312 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 021 | Total loss: 1.884 | Reg loss: 0.038 | Tree loss: 1.884 | Accuracy: 0.376953 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 021 | Total loss: 1.885 | Reg loss: 0.038 | Tree loss: 1.885 | Accuracy: 0.410156 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 021 | Total loss: 1.825 | Reg loss: 0.039 | Tree loss: 1.825 | Accuracy: 0.382812 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 021 | Total loss: 1.778 | Reg loss: 0.039 | Tree loss: 1.778 | Accuracy: 0.380859 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 021 | Total loss: 1.757 | Reg loss: 0.039 | Tree loss: 1.757 | Accuracy: 0.408203 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 021 | Total loss: 1.733 | Reg loss: 0.039 | Tree loss: 1.733 | Accuracy: 0.402344 | 0.11 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 021 | Total loss: 1.641 | Reg loss: 0.039 | Tree loss: 1.641 | Accuracy: 0.435115 | 0.11 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 | Batch: 000 / 021 | Total loss: 2.099 | Reg loss: 0.038 | Tree loss: 2.099 | Accuracy: 0.365234 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 021 | Total loss: 2.107 | Reg loss: 0.038 | Tree loss: 2.107 | Accuracy: 0.386719 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 021 | Total loss: 2.069 | Reg loss: 0.038 | Tree loss: 2.069 | Accuracy: 0.375000 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 021 | Total loss: 1.983 | Reg loss: 0.038 | Tree loss: 1.983 | Accuracy: 0.388672 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 021 | Total loss: 1.987 | Reg loss: 0.038 | Tree loss: 1.987 | Accuracy: 0.392578 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 021 | Total loss: 2.006 | Reg loss: 0.038 | Tree loss: 2.006 | Accuracy: 0.404297 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 021 | Total loss: 2.022 | Reg loss: 0.038 | Tree loss: 2.022 | Accuracy: 0.333984 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 021 | Total loss: 1.953 | Reg loss: 0.038 | Tree loss: 1.953 | Accuracy: 0.417969 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 021 | Total loss: 1.892 | Reg loss: 0.038 | Tree loss: 1.892 | Accuracy: 0.373047 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 021 | Total loss: 1.928 | Reg loss: 0.038 | Tree loss: 1.928 | Accuracy: 0.365234 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 021 | Total loss: 1.907 | Reg loss: 0.038 | Tree loss: 1.907 | Accuracy: 0.343750 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 021 | Total loss: 1.885 | Reg loss: 0.038 | Tree loss: 1.885 | Accuracy: 0.431641 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 021 | Total loss: 1.866 | Reg loss: 0.038 | Tree loss: 1.866 | Accuracy: 0.363281 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 021 | Total loss: 1.813 | Reg loss: 0.038 | Tree loss: 1.813 | Accuracy: 0.361328 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 021 | Total loss: 1.848 | Reg loss: 0.038 | Tree loss: 1.848 | Accuracy: 0.326172 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 021 | Total loss: 1.773 | Reg loss: 0.038 | Tree loss: 1.773 | Accuracy: 0.359375 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 021 | Total loss: 1.855 | Reg loss: 0.039 | Tree loss: 1.855 | Accuracy: 0.357422 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 021 | Total loss: 1.743 | Reg loss: 0.039 | Tree loss: 1.743 | Accuracy: 0.351562 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 021 | Total loss: 1.759 | Reg loss: 0.039 | Tree loss: 1.759 | Accuracy: 0.359375 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 021 | Total loss: 1.705 | Reg loss: 0.039 | Tree loss: 1.705 | Accuracy: 0.375000 | 0.11 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 021 | Total loss: 1.846 | Reg loss: 0.039 | Tree loss: 1.846 | Accuracy: 0.374046 | 0.11 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 76 | Batch: 000 / 021 | Total loss: 2.076 | Reg loss: 0.038 | Tree loss: 2.076 | Accuracy: 0.369141 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 021 | Total loss: 2.020 | Reg loss: 0.038 | Tree loss: 2.020 | Accuracy: 0.373047 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 021 | Total loss: 2.033 | Reg loss: 0.038 | Tree loss: 2.033 | Accuracy: 0.371094 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 021 | Total loss: 2.043 | Reg loss: 0.038 | Tree loss: 2.043 | Accuracy: 0.341797 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 021 | Total loss: 2.036 | Reg loss: 0.038 | Tree loss: 2.036 | Accuracy: 0.357422 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 021 | Total loss: 2.050 | Reg loss: 0.038 | Tree loss: 2.050 | Accuracy: 0.341797 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 021 | Total loss: 1.963 | Reg loss: 0.038 | Tree loss: 1.963 | Accuracy: 0.341797 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 021 | Total loss: 1.929 | Reg loss: 0.038 | Tree loss: 1.929 | Accuracy: 0.384766 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 021 | Total loss: 1.902 | Reg loss: 0.038 | Tree loss: 1.902 | Accuracy: 0.382812 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 021 | Total loss: 1.919 | Reg loss: 0.038 | Tree loss: 1.919 | Accuracy: 0.345703 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 021 | Total loss: 1.821 | Reg loss: 0.038 | Tree loss: 1.821 | Accuracy: 0.402344 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 021 | Total loss: 1.874 | Reg loss: 0.038 | Tree loss: 1.874 | Accuracy: 0.355469 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 021 | Total loss: 1.812 | Reg loss: 0.038 | Tree loss: 1.812 | Accuracy: 0.361328 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 021 | Total loss: 1.808 | Reg loss: 0.038 | Tree loss: 1.808 | Accuracy: 0.369141 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 021 | Total loss: 1.786 | Reg loss: 0.038 | Tree loss: 1.786 | Accuracy: 0.410156 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 021 | Total loss: 1.782 | Reg loss: 0.038 | Tree loss: 1.782 | Accuracy: 0.388672 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 021 | Total loss: 1.754 | Reg loss: 0.038 | Tree loss: 1.754 | Accuracy: 0.367188 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 021 | Total loss: 1.728 | Reg loss: 0.039 | Tree loss: 1.728 | Accuracy: 0.367188 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 021 | Total loss: 1.763 | Reg loss: 0.039 | Tree loss: 1.763 | Accuracy: 0.376953 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 021 | Total loss: 1.674 | Reg loss: 0.039 | Tree loss: 1.674 | Accuracy: 0.402344 | 0.11 sec/iter\n",
      "Epoch: 76 | Batch: 020 / 021 | Total loss: 1.646 | Reg loss: 0.039 | Tree loss: 1.646 | Accuracy: 0.458015 | 0.11 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 77 | Batch: 000 / 021 | Total loss: 2.028 | Reg loss: 0.038 | Tree loss: 2.028 | Accuracy: 0.390625 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 021 | Total loss: 2.052 | Reg loss: 0.038 | Tree loss: 2.052 | Accuracy: 0.365234 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 021 | Total loss: 1.987 | Reg loss: 0.038 | Tree loss: 1.987 | Accuracy: 0.367188 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 021 | Total loss: 1.950 | Reg loss: 0.038 | Tree loss: 1.950 | Accuracy: 0.367188 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 021 | Total loss: 1.976 | Reg loss: 0.038 | Tree loss: 1.976 | Accuracy: 0.378906 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 021 | Total loss: 1.880 | Reg loss: 0.038 | Tree loss: 1.880 | Accuracy: 0.402344 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 021 | Total loss: 1.908 | Reg loss: 0.038 | Tree loss: 1.908 | Accuracy: 0.382812 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 021 | Total loss: 1.941 | Reg loss: 0.038 | Tree loss: 1.941 | Accuracy: 0.345703 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 021 | Total loss: 1.884 | Reg loss: 0.038 | Tree loss: 1.884 | Accuracy: 0.378906 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 021 | Total loss: 1.866 | Reg loss: 0.038 | Tree loss: 1.866 | Accuracy: 0.357422 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 021 | Total loss: 1.825 | Reg loss: 0.038 | Tree loss: 1.825 | Accuracy: 0.376953 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 021 | Total loss: 1.844 | Reg loss: 0.038 | Tree loss: 1.844 | Accuracy: 0.347656 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 021 | Total loss: 1.855 | Reg loss: 0.038 | Tree loss: 1.855 | Accuracy: 0.386719 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 021 | Total loss: 1.827 | Reg loss: 0.038 | Tree loss: 1.827 | Accuracy: 0.388672 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 021 | Total loss: 1.771 | Reg loss: 0.038 | Tree loss: 1.771 | Accuracy: 0.380859 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 021 | Total loss: 1.825 | Reg loss: 0.038 | Tree loss: 1.825 | Accuracy: 0.367188 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 021 | Total loss: 1.724 | Reg loss: 0.038 | Tree loss: 1.724 | Accuracy: 0.345703 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 021 | Total loss: 1.755 | Reg loss: 0.039 | Tree loss: 1.755 | Accuracy: 0.404297 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 021 | Total loss: 1.719 | Reg loss: 0.039 | Tree loss: 1.719 | Accuracy: 0.357422 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 021 | Total loss: 1.707 | Reg loss: 0.039 | Tree loss: 1.707 | Accuracy: 0.351562 | 0.11 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 021 | Total loss: 1.640 | Reg loss: 0.039 | Tree loss: 1.640 | Accuracy: 0.419847 | 0.11 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 78 | Batch: 000 / 021 | Total loss: 1.974 | Reg loss: 0.038 | Tree loss: 1.974 | Accuracy: 0.423828 | 0.11 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78 | Batch: 001 / 021 | Total loss: 1.985 | Reg loss: 0.038 | Tree loss: 1.985 | Accuracy: 0.388672 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 021 | Total loss: 1.977 | Reg loss: 0.038 | Tree loss: 1.977 | Accuracy: 0.398438 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 021 | Total loss: 1.951 | Reg loss: 0.038 | Tree loss: 1.951 | Accuracy: 0.386719 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 021 | Total loss: 1.957 | Reg loss: 0.038 | Tree loss: 1.957 | Accuracy: 0.433594 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 021 | Total loss: 1.915 | Reg loss: 0.038 | Tree loss: 1.915 | Accuracy: 0.359375 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 021 | Total loss: 1.941 | Reg loss: 0.038 | Tree loss: 1.941 | Accuracy: 0.365234 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 021 | Total loss: 1.956 | Reg loss: 0.038 | Tree loss: 1.956 | Accuracy: 0.347656 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 021 | Total loss: 1.840 | Reg loss: 0.038 | Tree loss: 1.840 | Accuracy: 0.380859 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 021 | Total loss: 1.919 | Reg loss: 0.038 | Tree loss: 1.919 | Accuracy: 0.433594 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 021 | Total loss: 1.892 | Reg loss: 0.038 | Tree loss: 1.892 | Accuracy: 0.318359 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 021 | Total loss: 1.810 | Reg loss: 0.038 | Tree loss: 1.810 | Accuracy: 0.355469 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 021 | Total loss: 1.741 | Reg loss: 0.038 | Tree loss: 1.741 | Accuracy: 0.375000 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 021 | Total loss: 1.750 | Reg loss: 0.038 | Tree loss: 1.750 | Accuracy: 0.404297 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 021 | Total loss: 1.759 | Reg loss: 0.038 | Tree loss: 1.759 | Accuracy: 0.388672 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 021 | Total loss: 1.793 | Reg loss: 0.038 | Tree loss: 1.793 | Accuracy: 0.351562 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 021 | Total loss: 1.689 | Reg loss: 0.038 | Tree loss: 1.689 | Accuracy: 0.335938 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 021 | Total loss: 1.650 | Reg loss: 0.038 | Tree loss: 1.650 | Accuracy: 0.400391 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 021 | Total loss: 1.741 | Reg loss: 0.039 | Tree loss: 1.741 | Accuracy: 0.355469 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 021 | Total loss: 1.659 | Reg loss: 0.039 | Tree loss: 1.659 | Accuracy: 0.376953 | 0.11 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 021 | Total loss: 1.673 | Reg loss: 0.039 | Tree loss: 1.673 | Accuracy: 0.389313 | 0.11 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 79 | Batch: 000 / 021 | Total loss: 1.980 | Reg loss: 0.038 | Tree loss: 1.980 | Accuracy: 0.410156 | 0.11 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 021 | Total loss: 1.992 | Reg loss: 0.038 | Tree loss: 1.992 | Accuracy: 0.414062 | 0.11 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 021 | Total loss: 1.925 | Reg loss: 0.038 | Tree loss: 1.925 | Accuracy: 0.404297 | 0.11 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 021 | Total loss: 1.947 | Reg loss: 0.038 | Tree loss: 1.947 | Accuracy: 0.382812 | 0.11 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 021 | Total loss: 1.935 | Reg loss: 0.038 | Tree loss: 1.935 | Accuracy: 0.390625 | 0.11 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 021 | Total loss: 1.860 | Reg loss: 0.038 | Tree loss: 1.860 | Accuracy: 0.433594 | 0.11 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 021 | Total loss: 1.894 | Reg loss: 0.038 | Tree loss: 1.894 | Accuracy: 0.380859 | 0.11 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 021 | Total loss: 1.867 | Reg loss: 0.038 | Tree loss: 1.867 | Accuracy: 0.410156 | 0.11 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 021 | Total loss: 1.785 | Reg loss: 0.038 | Tree loss: 1.785 | Accuracy: 0.386719 | 0.11 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 021 | Total loss: 1.833 | Reg loss: 0.038 | Tree loss: 1.833 | Accuracy: 0.371094 | 0.11 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 021 | Total loss: 1.812 | Reg loss: 0.038 | Tree loss: 1.812 | Accuracy: 0.382812 | 0.11 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 021 | Total loss: 1.789 | Reg loss: 0.038 | Tree loss: 1.789 | Accuracy: 0.375000 | 0.109 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 021 | Total loss: 1.776 | Reg loss: 0.038 | Tree loss: 1.776 | Accuracy: 0.339844 | 0.109 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 021 | Total loss: 1.839 | Reg loss: 0.038 | Tree loss: 1.839 | Accuracy: 0.347656 | 0.109 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 021 | Total loss: 1.740 | Reg loss: 0.038 | Tree loss: 1.740 | Accuracy: 0.371094 | 0.109 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 021 | Total loss: 1.746 | Reg loss: 0.038 | Tree loss: 1.746 | Accuracy: 0.382812 | 0.109 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 021 | Total loss: 1.767 | Reg loss: 0.038 | Tree loss: 1.767 | Accuracy: 0.355469 | 0.109 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 021 | Total loss: 1.620 | Reg loss: 0.038 | Tree loss: 1.620 | Accuracy: 0.439453 | 0.109 sec/iter\n",
      "Epoch: 79 | Batch: 018 / 021 | Total loss: 1.713 | Reg loss: 0.038 | Tree loss: 1.713 | Accuracy: 0.324219 | 0.109 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 021 | Total loss: 1.670 | Reg loss: 0.039 | Tree loss: 1.670 | Accuracy: 0.371094 | 0.109 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 021 | Total loss: 1.738 | Reg loss: 0.039 | Tree loss: 1.738 | Accuracy: 0.366412 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 80 | Batch: 000 / 021 | Total loss: 1.956 | Reg loss: 0.038 | Tree loss: 1.956 | Accuracy: 0.439453 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 021 | Total loss: 1.966 | Reg loss: 0.037 | Tree loss: 1.966 | Accuracy: 0.402344 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 021 | Total loss: 1.899 | Reg loss: 0.038 | Tree loss: 1.899 | Accuracy: 0.427734 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 021 | Total loss: 2.005 | Reg loss: 0.038 | Tree loss: 2.005 | Accuracy: 0.429688 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 021 | Total loss: 1.974 | Reg loss: 0.038 | Tree loss: 1.974 | Accuracy: 0.376953 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 021 | Total loss: 1.840 | Reg loss: 0.038 | Tree loss: 1.840 | Accuracy: 0.429688 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 021 | Total loss: 1.785 | Reg loss: 0.038 | Tree loss: 1.785 | Accuracy: 0.423828 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 021 | Total loss: 1.842 | Reg loss: 0.038 | Tree loss: 1.842 | Accuracy: 0.376953 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 021 | Total loss: 1.845 | Reg loss: 0.038 | Tree loss: 1.845 | Accuracy: 0.398438 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 021 | Total loss: 1.774 | Reg loss: 0.038 | Tree loss: 1.774 | Accuracy: 0.400391 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 021 | Total loss: 1.801 | Reg loss: 0.038 | Tree loss: 1.801 | Accuracy: 0.402344 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 021 | Total loss: 1.789 | Reg loss: 0.038 | Tree loss: 1.789 | Accuracy: 0.367188 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 021 | Total loss: 1.750 | Reg loss: 0.038 | Tree loss: 1.750 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 021 | Total loss: 1.691 | Reg loss: 0.038 | Tree loss: 1.691 | Accuracy: 0.398438 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 021 | Total loss: 1.723 | Reg loss: 0.038 | Tree loss: 1.723 | Accuracy: 0.398438 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 021 | Total loss: 1.743 | Reg loss: 0.038 | Tree loss: 1.743 | Accuracy: 0.380859 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 021 | Total loss: 1.684 | Reg loss: 0.038 | Tree loss: 1.684 | Accuracy: 0.375000 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 021 | Total loss: 1.664 | Reg loss: 0.038 | Tree loss: 1.664 | Accuracy: 0.375000 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 021 | Total loss: 1.664 | Reg loss: 0.038 | Tree loss: 1.664 | Accuracy: 0.349609 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 021 | Total loss: 1.734 | Reg loss: 0.038 | Tree loss: 1.734 | Accuracy: 0.363281 | 0.109 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 021 | Total loss: 1.613 | Reg loss: 0.039 | Tree loss: 1.613 | Accuracy: 0.427481 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 81 | Batch: 000 / 021 | Total loss: 1.938 | Reg loss: 0.037 | Tree loss: 1.938 | Accuracy: 0.431641 | 0.109 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81 | Batch: 001 / 021 | Total loss: 1.893 | Reg loss: 0.037 | Tree loss: 1.893 | Accuracy: 0.445312 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 021 | Total loss: 1.842 | Reg loss: 0.037 | Tree loss: 1.842 | Accuracy: 0.437500 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 021 | Total loss: 2.043 | Reg loss: 0.037 | Tree loss: 2.043 | Accuracy: 0.380859 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 021 | Total loss: 1.894 | Reg loss: 0.037 | Tree loss: 1.894 | Accuracy: 0.375000 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 021 | Total loss: 1.811 | Reg loss: 0.037 | Tree loss: 1.811 | Accuracy: 0.410156 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 021 | Total loss: 1.827 | Reg loss: 0.038 | Tree loss: 1.827 | Accuracy: 0.419922 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 021 | Total loss: 1.870 | Reg loss: 0.038 | Tree loss: 1.870 | Accuracy: 0.402344 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 021 | Total loss: 1.824 | Reg loss: 0.038 | Tree loss: 1.824 | Accuracy: 0.410156 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 021 | Total loss: 1.774 | Reg loss: 0.038 | Tree loss: 1.774 | Accuracy: 0.427734 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 021 | Total loss: 1.774 | Reg loss: 0.038 | Tree loss: 1.774 | Accuracy: 0.406250 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 021 | Total loss: 1.795 | Reg loss: 0.038 | Tree loss: 1.795 | Accuracy: 0.382812 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 021 | Total loss: 1.754 | Reg loss: 0.038 | Tree loss: 1.754 | Accuracy: 0.404297 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 021 | Total loss: 1.699 | Reg loss: 0.038 | Tree loss: 1.699 | Accuracy: 0.421875 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 021 | Total loss: 1.741 | Reg loss: 0.038 | Tree loss: 1.741 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 021 | Total loss: 1.719 | Reg loss: 0.038 | Tree loss: 1.719 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 021 | Total loss: 1.680 | Reg loss: 0.038 | Tree loss: 1.680 | Accuracy: 0.388672 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 021 | Total loss: 1.638 | Reg loss: 0.038 | Tree loss: 1.638 | Accuracy: 0.404297 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 021 | Total loss: 1.655 | Reg loss: 0.038 | Tree loss: 1.655 | Accuracy: 0.425781 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 021 | Total loss: 1.560 | Reg loss: 0.038 | Tree loss: 1.560 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 021 | Total loss: 1.787 | Reg loss: 0.039 | Tree loss: 1.787 | Accuracy: 0.335878 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 82 | Batch: 000 / 021 | Total loss: 1.915 | Reg loss: 0.037 | Tree loss: 1.915 | Accuracy: 0.406250 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 021 | Total loss: 1.999 | Reg loss: 0.037 | Tree loss: 1.999 | Accuracy: 0.365234 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 021 | Total loss: 1.905 | Reg loss: 0.037 | Tree loss: 1.905 | Accuracy: 0.419922 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 021 | Total loss: 1.855 | Reg loss: 0.037 | Tree loss: 1.855 | Accuracy: 0.425781 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 021 | Total loss: 1.884 | Reg loss: 0.037 | Tree loss: 1.884 | Accuracy: 0.408203 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 021 | Total loss: 1.860 | Reg loss: 0.037 | Tree loss: 1.860 | Accuracy: 0.412109 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 021 | Total loss: 1.858 | Reg loss: 0.037 | Tree loss: 1.858 | Accuracy: 0.425781 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 021 | Total loss: 1.787 | Reg loss: 0.037 | Tree loss: 1.787 | Accuracy: 0.386719 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 021 | Total loss: 1.808 | Reg loss: 0.038 | Tree loss: 1.808 | Accuracy: 0.414062 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 021 | Total loss: 1.820 | Reg loss: 0.038 | Tree loss: 1.820 | Accuracy: 0.388672 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 021 | Total loss: 1.696 | Reg loss: 0.038 | Tree loss: 1.696 | Accuracy: 0.429688 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 021 | Total loss: 1.748 | Reg loss: 0.038 | Tree loss: 1.748 | Accuracy: 0.417969 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 021 | Total loss: 1.724 | Reg loss: 0.038 | Tree loss: 1.724 | Accuracy: 0.404297 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 021 | Total loss: 1.688 | Reg loss: 0.038 | Tree loss: 1.688 | Accuracy: 0.423828 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 021 | Total loss: 1.705 | Reg loss: 0.038 | Tree loss: 1.705 | Accuracy: 0.419922 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 021 | Total loss: 1.640 | Reg loss: 0.038 | Tree loss: 1.640 | Accuracy: 0.402344 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 021 | Total loss: 1.649 | Reg loss: 0.038 | Tree loss: 1.649 | Accuracy: 0.417969 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 021 | Total loss: 1.671 | Reg loss: 0.038 | Tree loss: 1.671 | Accuracy: 0.382812 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 021 | Total loss: 1.610 | Reg loss: 0.038 | Tree loss: 1.610 | Accuracy: 0.416016 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 021 | Total loss: 1.620 | Reg loss: 0.038 | Tree loss: 1.620 | Accuracy: 0.414062 | 0.109 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 021 | Total loss: 1.542 | Reg loss: 0.038 | Tree loss: 1.542 | Accuracy: 0.442748 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 83 | Batch: 000 / 021 | Total loss: 1.883 | Reg loss: 0.037 | Tree loss: 1.883 | Accuracy: 0.404297 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 021 | Total loss: 1.954 | Reg loss: 0.037 | Tree loss: 1.954 | Accuracy: 0.392578 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 021 | Total loss: 1.935 | Reg loss: 0.037 | Tree loss: 1.935 | Accuracy: 0.396484 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 021 | Total loss: 1.819 | Reg loss: 0.037 | Tree loss: 1.819 | Accuracy: 0.433594 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 021 | Total loss: 1.827 | Reg loss: 0.037 | Tree loss: 1.827 | Accuracy: 0.404297 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 021 | Total loss: 1.818 | Reg loss: 0.037 | Tree loss: 1.818 | Accuracy: 0.371094 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 021 | Total loss: 1.786 | Reg loss: 0.037 | Tree loss: 1.786 | Accuracy: 0.427734 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 021 | Total loss: 1.807 | Reg loss: 0.037 | Tree loss: 1.807 | Accuracy: 0.408203 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 021 | Total loss: 1.818 | Reg loss: 0.037 | Tree loss: 1.818 | Accuracy: 0.396484 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 021 | Total loss: 1.747 | Reg loss: 0.038 | Tree loss: 1.747 | Accuracy: 0.433594 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 021 | Total loss: 1.764 | Reg loss: 0.038 | Tree loss: 1.764 | Accuracy: 0.394531 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 021 | Total loss: 1.699 | Reg loss: 0.038 | Tree loss: 1.699 | Accuracy: 0.429688 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 021 | Total loss: 1.707 | Reg loss: 0.038 | Tree loss: 1.707 | Accuracy: 0.441406 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 021 | Total loss: 1.728 | Reg loss: 0.038 | Tree loss: 1.728 | Accuracy: 0.414062 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 021 | Total loss: 1.696 | Reg loss: 0.038 | Tree loss: 1.696 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 021 | Total loss: 1.648 | Reg loss: 0.038 | Tree loss: 1.648 | Accuracy: 0.435547 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 021 | Total loss: 1.663 | Reg loss: 0.038 | Tree loss: 1.663 | Accuracy: 0.388672 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 021 | Total loss: 1.588 | Reg loss: 0.038 | Tree loss: 1.588 | Accuracy: 0.398438 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 021 | Total loss: 1.589 | Reg loss: 0.038 | Tree loss: 1.589 | Accuracy: 0.437500 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 021 | Total loss: 1.604 | Reg loss: 0.038 | Tree loss: 1.604 | Accuracy: 0.392578 | 0.109 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 021 | Total loss: 1.709 | Reg loss: 0.038 | Tree loss: 1.709 | Accuracy: 0.381679 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 84 | Batch: 000 / 021 | Total loss: 1.921 | Reg loss: 0.037 | Tree loss: 1.921 | Accuracy: 0.427734 | 0.109 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84 | Batch: 001 / 021 | Total loss: 1.880 | Reg loss: 0.037 | Tree loss: 1.880 | Accuracy: 0.431641 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 021 | Total loss: 1.868 | Reg loss: 0.037 | Tree loss: 1.868 | Accuracy: 0.376953 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 021 | Total loss: 1.832 | Reg loss: 0.037 | Tree loss: 1.832 | Accuracy: 0.427734 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 021 | Total loss: 1.815 | Reg loss: 0.037 | Tree loss: 1.815 | Accuracy: 0.439453 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 021 | Total loss: 1.787 | Reg loss: 0.037 | Tree loss: 1.787 | Accuracy: 0.423828 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 021 | Total loss: 1.716 | Reg loss: 0.037 | Tree loss: 1.716 | Accuracy: 0.410156 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 021 | Total loss: 1.778 | Reg loss: 0.037 | Tree loss: 1.778 | Accuracy: 0.375000 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 021 | Total loss: 1.779 | Reg loss: 0.037 | Tree loss: 1.779 | Accuracy: 0.410156 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 021 | Total loss: 1.736 | Reg loss: 0.037 | Tree loss: 1.736 | Accuracy: 0.392578 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 021 | Total loss: 1.706 | Reg loss: 0.038 | Tree loss: 1.706 | Accuracy: 0.406250 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 021 | Total loss: 1.683 | Reg loss: 0.038 | Tree loss: 1.683 | Accuracy: 0.460938 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 021 | Total loss: 1.774 | Reg loss: 0.038 | Tree loss: 1.774 | Accuracy: 0.398438 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 021 | Total loss: 1.608 | Reg loss: 0.038 | Tree loss: 1.608 | Accuracy: 0.421875 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 021 | Total loss: 1.748 | Reg loss: 0.038 | Tree loss: 1.748 | Accuracy: 0.396484 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 015 / 021 | Total loss: 1.663 | Reg loss: 0.038 | Tree loss: 1.663 | Accuracy: 0.375000 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 021 | Total loss: 1.620 | Reg loss: 0.038 | Tree loss: 1.620 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 021 | Total loss: 1.671 | Reg loss: 0.038 | Tree loss: 1.671 | Accuracy: 0.414062 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 021 | Total loss: 1.557 | Reg loss: 0.038 | Tree loss: 1.557 | Accuracy: 0.431641 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 021 | Total loss: 1.653 | Reg loss: 0.038 | Tree loss: 1.653 | Accuracy: 0.390625 | 0.109 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 021 | Total loss: 1.615 | Reg loss: 0.038 | Tree loss: 1.615 | Accuracy: 0.412214 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 85 | Batch: 000 / 021 | Total loss: 1.919 | Reg loss: 0.037 | Tree loss: 1.919 | Accuracy: 0.416016 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 021 | Total loss: 1.922 | Reg loss: 0.037 | Tree loss: 1.922 | Accuracy: 0.402344 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 021 | Total loss: 1.843 | Reg loss: 0.037 | Tree loss: 1.843 | Accuracy: 0.394531 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 021 | Total loss: 1.808 | Reg loss: 0.037 | Tree loss: 1.808 | Accuracy: 0.416016 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 021 | Total loss: 1.799 | Reg loss: 0.037 | Tree loss: 1.799 | Accuracy: 0.404297 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 021 | Total loss: 1.769 | Reg loss: 0.037 | Tree loss: 1.769 | Accuracy: 0.441406 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 021 | Total loss: 1.812 | Reg loss: 0.037 | Tree loss: 1.812 | Accuracy: 0.414062 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 021 | Total loss: 1.771 | Reg loss: 0.037 | Tree loss: 1.771 | Accuracy: 0.417969 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 021 | Total loss: 1.730 | Reg loss: 0.037 | Tree loss: 1.730 | Accuracy: 0.427734 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 021 | Total loss: 1.733 | Reg loss: 0.037 | Tree loss: 1.733 | Accuracy: 0.406250 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 021 | Total loss: 1.684 | Reg loss: 0.037 | Tree loss: 1.684 | Accuracy: 0.429688 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 021 | Total loss: 1.670 | Reg loss: 0.037 | Tree loss: 1.670 | Accuracy: 0.398438 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 021 | Total loss: 1.685 | Reg loss: 0.038 | Tree loss: 1.685 | Accuracy: 0.398438 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 021 | Total loss: 1.616 | Reg loss: 0.038 | Tree loss: 1.616 | Accuracy: 0.380859 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 021 | Total loss: 1.733 | Reg loss: 0.038 | Tree loss: 1.733 | Accuracy: 0.396484 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 021 | Total loss: 1.653 | Reg loss: 0.038 | Tree loss: 1.653 | Accuracy: 0.431641 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 021 | Total loss: 1.559 | Reg loss: 0.038 | Tree loss: 1.559 | Accuracy: 0.457031 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 021 | Total loss: 1.575 | Reg loss: 0.038 | Tree loss: 1.575 | Accuracy: 0.396484 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 021 | Total loss: 1.629 | Reg loss: 0.038 | Tree loss: 1.629 | Accuracy: 0.408203 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 021 | Total loss: 1.619 | Reg loss: 0.038 | Tree loss: 1.619 | Accuracy: 0.384766 | 0.109 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 021 | Total loss: 1.518 | Reg loss: 0.038 | Tree loss: 1.518 | Accuracy: 0.404580 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 86 | Batch: 000 / 021 | Total loss: 1.909 | Reg loss: 0.037 | Tree loss: 1.909 | Accuracy: 0.404297 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 021 | Total loss: 1.835 | Reg loss: 0.037 | Tree loss: 1.835 | Accuracy: 0.414062 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 021 | Total loss: 1.844 | Reg loss: 0.037 | Tree loss: 1.844 | Accuracy: 0.398438 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 021 | Total loss: 1.849 | Reg loss: 0.037 | Tree loss: 1.849 | Accuracy: 0.435547 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 021 | Total loss: 1.792 | Reg loss: 0.037 | Tree loss: 1.792 | Accuracy: 0.439453 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 021 | Total loss: 1.771 | Reg loss: 0.037 | Tree loss: 1.771 | Accuracy: 0.406250 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 021 | Total loss: 1.767 | Reg loss: 0.037 | Tree loss: 1.767 | Accuracy: 0.402344 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 021 | Total loss: 1.788 | Reg loss: 0.037 | Tree loss: 1.788 | Accuracy: 0.414062 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 021 | Total loss: 1.755 | Reg loss: 0.037 | Tree loss: 1.755 | Accuracy: 0.429688 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 021 | Total loss: 1.674 | Reg loss: 0.037 | Tree loss: 1.674 | Accuracy: 0.408203 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 021 | Total loss: 1.719 | Reg loss: 0.037 | Tree loss: 1.719 | Accuracy: 0.404297 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 021 | Total loss: 1.645 | Reg loss: 0.037 | Tree loss: 1.645 | Accuracy: 0.443359 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 021 | Total loss: 1.677 | Reg loss: 0.037 | Tree loss: 1.677 | Accuracy: 0.443359 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 021 | Total loss: 1.690 | Reg loss: 0.038 | Tree loss: 1.690 | Accuracy: 0.427734 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 021 | Total loss: 1.621 | Reg loss: 0.038 | Tree loss: 1.621 | Accuracy: 0.398438 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 021 | Total loss: 1.588 | Reg loss: 0.038 | Tree loss: 1.588 | Accuracy: 0.419922 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 021 | Total loss: 1.603 | Reg loss: 0.038 | Tree loss: 1.603 | Accuracy: 0.388672 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 021 | Total loss: 1.578 | Reg loss: 0.038 | Tree loss: 1.578 | Accuracy: 0.416016 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 021 | Total loss: 1.570 | Reg loss: 0.038 | Tree loss: 1.570 | Accuracy: 0.437500 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 021 | Total loss: 1.545 | Reg loss: 0.038 | Tree loss: 1.545 | Accuracy: 0.390625 | 0.109 sec/iter\n",
      "Epoch: 86 | Batch: 020 / 021 | Total loss: 1.625 | Reg loss: 0.038 | Tree loss: 1.625 | Accuracy: 0.419847 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 | Batch: 000 / 021 | Total loss: 1.850 | Reg loss: 0.037 | Tree loss: 1.850 | Accuracy: 0.427734 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 021 | Total loss: 1.905 | Reg loss: 0.037 | Tree loss: 1.905 | Accuracy: 0.437500 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 021 | Total loss: 1.794 | Reg loss: 0.037 | Tree loss: 1.794 | Accuracy: 0.437500 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 021 | Total loss: 1.759 | Reg loss: 0.037 | Tree loss: 1.759 | Accuracy: 0.408203 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 021 | Total loss: 1.750 | Reg loss: 0.037 | Tree loss: 1.750 | Accuracy: 0.392578 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 021 | Total loss: 1.764 | Reg loss: 0.037 | Tree loss: 1.764 | Accuracy: 0.431641 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 021 | Total loss: 1.771 | Reg loss: 0.037 | Tree loss: 1.771 | Accuracy: 0.447266 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 021 | Total loss: 1.713 | Reg loss: 0.037 | Tree loss: 1.713 | Accuracy: 0.437500 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 021 | Total loss: 1.722 | Reg loss: 0.037 | Tree loss: 1.722 | Accuracy: 0.441406 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 021 | Total loss: 1.685 | Reg loss: 0.037 | Tree loss: 1.685 | Accuracy: 0.458984 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 021 | Total loss: 1.669 | Reg loss: 0.037 | Tree loss: 1.669 | Accuracy: 0.433594 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 021 | Total loss: 1.690 | Reg loss: 0.037 | Tree loss: 1.690 | Accuracy: 0.404297 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 021 | Total loss: 1.685 | Reg loss: 0.037 | Tree loss: 1.685 | Accuracy: 0.400391 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 021 | Total loss: 1.624 | Reg loss: 0.037 | Tree loss: 1.624 | Accuracy: 0.414062 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 021 | Total loss: 1.642 | Reg loss: 0.038 | Tree loss: 1.642 | Accuracy: 0.400391 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 021 | Total loss: 1.625 | Reg loss: 0.038 | Tree loss: 1.625 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 021 | Total loss: 1.566 | Reg loss: 0.038 | Tree loss: 1.566 | Accuracy: 0.382812 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 021 | Total loss: 1.612 | Reg loss: 0.038 | Tree loss: 1.612 | Accuracy: 0.437500 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 021 | Total loss: 1.572 | Reg loss: 0.038 | Tree loss: 1.572 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 021 | Total loss: 1.558 | Reg loss: 0.038 | Tree loss: 1.558 | Accuracy: 0.392578 | 0.109 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 021 | Total loss: 1.597 | Reg loss: 0.038 | Tree loss: 1.597 | Accuracy: 0.450382 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 88 | Batch: 000 / 021 | Total loss: 1.878 | Reg loss: 0.037 | Tree loss: 1.878 | Accuracy: 0.447266 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 021 | Total loss: 1.877 | Reg loss: 0.037 | Tree loss: 1.877 | Accuracy: 0.412109 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 021 | Total loss: 1.828 | Reg loss: 0.037 | Tree loss: 1.828 | Accuracy: 0.394531 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 021 | Total loss: 1.791 | Reg loss: 0.037 | Tree loss: 1.791 | Accuracy: 0.443359 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 021 | Total loss: 1.759 | Reg loss: 0.037 | Tree loss: 1.759 | Accuracy: 0.429688 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 021 | Total loss: 1.748 | Reg loss: 0.037 | Tree loss: 1.748 | Accuracy: 0.443359 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 021 | Total loss: 1.731 | Reg loss: 0.037 | Tree loss: 1.731 | Accuracy: 0.453125 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 021 | Total loss: 1.688 | Reg loss: 0.037 | Tree loss: 1.688 | Accuracy: 0.419922 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 021 | Total loss: 1.732 | Reg loss: 0.037 | Tree loss: 1.732 | Accuracy: 0.421875 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 021 | Total loss: 1.666 | Reg loss: 0.037 | Tree loss: 1.666 | Accuracy: 0.398438 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 021 | Total loss: 1.692 | Reg loss: 0.037 | Tree loss: 1.692 | Accuracy: 0.410156 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 021 | Total loss: 1.656 | Reg loss: 0.037 | Tree loss: 1.656 | Accuracy: 0.394531 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 021 | Total loss: 1.646 | Reg loss: 0.037 | Tree loss: 1.646 | Accuracy: 0.394531 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 021 | Total loss: 1.565 | Reg loss: 0.037 | Tree loss: 1.565 | Accuracy: 0.447266 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 021 | Total loss: 1.596 | Reg loss: 0.037 | Tree loss: 1.596 | Accuracy: 0.421875 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 021 | Total loss: 1.581 | Reg loss: 0.037 | Tree loss: 1.581 | Accuracy: 0.416016 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 021 | Total loss: 1.608 | Reg loss: 0.038 | Tree loss: 1.608 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 021 | Total loss: 1.556 | Reg loss: 0.038 | Tree loss: 1.556 | Accuracy: 0.433594 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 021 | Total loss: 1.576 | Reg loss: 0.038 | Tree loss: 1.576 | Accuracy: 0.388672 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 021 | Total loss: 1.558 | Reg loss: 0.038 | Tree loss: 1.558 | Accuracy: 0.443359 | 0.109 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 021 | Total loss: 1.453 | Reg loss: 0.038 | Tree loss: 1.453 | Accuracy: 0.404580 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 89 | Batch: 000 / 021 | Total loss: 1.851 | Reg loss: 0.037 | Tree loss: 1.851 | Accuracy: 0.423828 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 021 | Total loss: 1.751 | Reg loss: 0.037 | Tree loss: 1.751 | Accuracy: 0.443359 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 021 | Total loss: 1.793 | Reg loss: 0.037 | Tree loss: 1.793 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 021 | Total loss: 1.741 | Reg loss: 0.037 | Tree loss: 1.741 | Accuracy: 0.396484 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 021 | Total loss: 1.738 | Reg loss: 0.037 | Tree loss: 1.738 | Accuracy: 0.460938 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 021 | Total loss: 1.758 | Reg loss: 0.037 | Tree loss: 1.758 | Accuracy: 0.425781 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 021 | Total loss: 1.737 | Reg loss: 0.037 | Tree loss: 1.737 | Accuracy: 0.429688 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 021 | Total loss: 1.689 | Reg loss: 0.037 | Tree loss: 1.689 | Accuracy: 0.464844 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 021 | Total loss: 1.675 | Reg loss: 0.037 | Tree loss: 1.675 | Accuracy: 0.392578 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 021 | Total loss: 1.667 | Reg loss: 0.037 | Tree loss: 1.667 | Accuracy: 0.402344 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 021 | Total loss: 1.659 | Reg loss: 0.037 | Tree loss: 1.659 | Accuracy: 0.421875 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 021 | Total loss: 1.613 | Reg loss: 0.037 | Tree loss: 1.613 | Accuracy: 0.410156 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 021 | Total loss: 1.642 | Reg loss: 0.037 | Tree loss: 1.642 | Accuracy: 0.423828 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 021 | Total loss: 1.620 | Reg loss: 0.037 | Tree loss: 1.620 | Accuracy: 0.462891 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 021 | Total loss: 1.623 | Reg loss: 0.037 | Tree loss: 1.623 | Accuracy: 0.457031 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 021 | Total loss: 1.629 | Reg loss: 0.037 | Tree loss: 1.629 | Accuracy: 0.417969 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 021 | Total loss: 1.591 | Reg loss: 0.037 | Tree loss: 1.591 | Accuracy: 0.429688 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 021 | Total loss: 1.563 | Reg loss: 0.038 | Tree loss: 1.563 | Accuracy: 0.398438 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 021 | Total loss: 1.558 | Reg loss: 0.038 | Tree loss: 1.558 | Accuracy: 0.378906 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 021 | Total loss: 1.582 | Reg loss: 0.038 | Tree loss: 1.582 | Accuracy: 0.380859 | 0.109 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 021 | Total loss: 1.507 | Reg loss: 0.038 | Tree loss: 1.507 | Accuracy: 0.366412 | 0.109 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 90 | Batch: 000 / 021 | Total loss: 1.837 | Reg loss: 0.037 | Tree loss: 1.837 | Accuracy: 0.441406 | 0.11 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 | Batch: 001 / 021 | Total loss: 1.767 | Reg loss: 0.037 | Tree loss: 1.767 | Accuracy: 0.414062 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 021 | Total loss: 1.775 | Reg loss: 0.037 | Tree loss: 1.775 | Accuracy: 0.449219 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 021 | Total loss: 1.748 | Reg loss: 0.037 | Tree loss: 1.748 | Accuracy: 0.412109 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 021 | Total loss: 1.751 | Reg loss: 0.037 | Tree loss: 1.751 | Accuracy: 0.388672 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 021 | Total loss: 1.683 | Reg loss: 0.037 | Tree loss: 1.683 | Accuracy: 0.429688 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 021 | Total loss: 1.685 | Reg loss: 0.037 | Tree loss: 1.685 | Accuracy: 0.410156 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 021 | Total loss: 1.696 | Reg loss: 0.037 | Tree loss: 1.696 | Accuracy: 0.412109 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 021 | Total loss: 1.671 | Reg loss: 0.037 | Tree loss: 1.671 | Accuracy: 0.402344 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 021 | Total loss: 1.646 | Reg loss: 0.037 | Tree loss: 1.646 | Accuracy: 0.460938 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 021 | Total loss: 1.643 | Reg loss: 0.037 | Tree loss: 1.643 | Accuracy: 0.425781 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 021 | Total loss: 1.635 | Reg loss: 0.037 | Tree loss: 1.635 | Accuracy: 0.412109 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 021 | Total loss: 1.614 | Reg loss: 0.037 | Tree loss: 1.614 | Accuracy: 0.412109 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 021 | Total loss: 1.589 | Reg loss: 0.037 | Tree loss: 1.589 | Accuracy: 0.417969 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 021 | Total loss: 1.612 | Reg loss: 0.037 | Tree loss: 1.612 | Accuracy: 0.394531 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 021 | Total loss: 1.603 | Reg loss: 0.037 | Tree loss: 1.603 | Accuracy: 0.425781 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 021 | Total loss: 1.535 | Reg loss: 0.037 | Tree loss: 1.535 | Accuracy: 0.416016 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 021 | Total loss: 1.574 | Reg loss: 0.037 | Tree loss: 1.574 | Accuracy: 0.455078 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 021 | Total loss: 1.591 | Reg loss: 0.038 | Tree loss: 1.591 | Accuracy: 0.398438 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 021 | Total loss: 1.537 | Reg loss: 0.038 | Tree loss: 1.537 | Accuracy: 0.449219 | 0.11 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 021 | Total loss: 1.681 | Reg loss: 0.038 | Tree loss: 1.681 | Accuracy: 0.366412 | 0.11 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 91 | Batch: 000 / 021 | Total loss: 1.796 | Reg loss: 0.037 | Tree loss: 1.796 | Accuracy: 0.394531 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 021 | Total loss: 1.749 | Reg loss: 0.037 | Tree loss: 1.749 | Accuracy: 0.474609 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 021 | Total loss: 1.776 | Reg loss: 0.037 | Tree loss: 1.776 | Accuracy: 0.466797 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 021 | Total loss: 1.774 | Reg loss: 0.037 | Tree loss: 1.774 | Accuracy: 0.427734 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 021 | Total loss: 1.735 | Reg loss: 0.037 | Tree loss: 1.735 | Accuracy: 0.449219 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 021 | Total loss: 1.726 | Reg loss: 0.037 | Tree loss: 1.726 | Accuracy: 0.437500 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 021 | Total loss: 1.656 | Reg loss: 0.037 | Tree loss: 1.656 | Accuracy: 0.416016 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 021 | Total loss: 1.661 | Reg loss: 0.037 | Tree loss: 1.661 | Accuracy: 0.402344 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 021 | Total loss: 1.623 | Reg loss: 0.037 | Tree loss: 1.623 | Accuracy: 0.441406 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 021 | Total loss: 1.654 | Reg loss: 0.037 | Tree loss: 1.654 | Accuracy: 0.376953 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 021 | Total loss: 1.602 | Reg loss: 0.037 | Tree loss: 1.602 | Accuracy: 0.445312 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 021 | Total loss: 1.613 | Reg loss: 0.037 | Tree loss: 1.613 | Accuracy: 0.410156 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 021 | Total loss: 1.683 | Reg loss: 0.037 | Tree loss: 1.683 | Accuracy: 0.416016 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 021 | Total loss: 1.565 | Reg loss: 0.037 | Tree loss: 1.565 | Accuracy: 0.414062 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 021 | Total loss: 1.603 | Reg loss: 0.037 | Tree loss: 1.603 | Accuracy: 0.404297 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 021 | Total loss: 1.584 | Reg loss: 0.037 | Tree loss: 1.584 | Accuracy: 0.398438 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 021 | Total loss: 1.577 | Reg loss: 0.037 | Tree loss: 1.577 | Accuracy: 0.392578 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 021 | Total loss: 1.619 | Reg loss: 0.037 | Tree loss: 1.619 | Accuracy: 0.416016 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 021 | Total loss: 1.511 | Reg loss: 0.037 | Tree loss: 1.511 | Accuracy: 0.455078 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 019 / 021 | Total loss: 1.484 | Reg loss: 0.038 | Tree loss: 1.484 | Accuracy: 0.414062 | 0.11 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 021 | Total loss: 1.594 | Reg loss: 0.038 | Tree loss: 1.594 | Accuracy: 0.389313 | 0.11 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 92 | Batch: 000 / 021 | Total loss: 1.796 | Reg loss: 0.037 | Tree loss: 1.796 | Accuracy: 0.406250 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 021 | Total loss: 1.820 | Reg loss: 0.037 | Tree loss: 1.820 | Accuracy: 0.402344 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 021 | Total loss: 1.684 | Reg loss: 0.037 | Tree loss: 1.684 | Accuracy: 0.410156 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 021 | Total loss: 1.706 | Reg loss: 0.037 | Tree loss: 1.706 | Accuracy: 0.425781 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 021 | Total loss: 1.661 | Reg loss: 0.037 | Tree loss: 1.661 | Accuracy: 0.482422 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 021 | Total loss: 1.640 | Reg loss: 0.037 | Tree loss: 1.640 | Accuracy: 0.455078 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 021 | Total loss: 1.689 | Reg loss: 0.037 | Tree loss: 1.689 | Accuracy: 0.455078 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 021 | Total loss: 1.807 | Reg loss: 0.037 | Tree loss: 1.807 | Accuracy: 0.433594 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 021 | Total loss: 1.630 | Reg loss: 0.037 | Tree loss: 1.630 | Accuracy: 0.441406 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 021 | Total loss: 1.677 | Reg loss: 0.037 | Tree loss: 1.677 | Accuracy: 0.390625 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 021 | Total loss: 1.623 | Reg loss: 0.037 | Tree loss: 1.623 | Accuracy: 0.460938 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 021 | Total loss: 1.629 | Reg loss: 0.037 | Tree loss: 1.629 | Accuracy: 0.410156 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 021 | Total loss: 1.581 | Reg loss: 0.037 | Tree loss: 1.581 | Accuracy: 0.417969 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 021 | Total loss: 1.577 | Reg loss: 0.037 | Tree loss: 1.577 | Accuracy: 0.384766 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 021 | Total loss: 1.592 | Reg loss: 0.037 | Tree loss: 1.592 | Accuracy: 0.417969 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 021 | Total loss: 1.574 | Reg loss: 0.037 | Tree loss: 1.574 | Accuracy: 0.412109 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 021 | Total loss: 1.581 | Reg loss: 0.037 | Tree loss: 1.581 | Accuracy: 0.384766 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 021 | Total loss: 1.521 | Reg loss: 0.037 | Tree loss: 1.521 | Accuracy: 0.445312 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 021 | Total loss: 1.501 | Reg loss: 0.037 | Tree loss: 1.501 | Accuracy: 0.396484 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 019 / 021 | Total loss: 1.510 | Reg loss: 0.037 | Tree loss: 1.510 | Accuracy: 0.441406 | 0.11 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 021 | Total loss: 1.518 | Reg loss: 0.038 | Tree loss: 1.518 | Accuracy: 0.427481 | 0.11 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 93 | Batch: 000 / 021 | Total loss: 1.748 | Reg loss: 0.037 | Tree loss: 1.748 | Accuracy: 0.396484 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 021 | Total loss: 1.729 | Reg loss: 0.037 | Tree loss: 1.729 | Accuracy: 0.451172 | 0.11 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93 | Batch: 002 / 021 | Total loss: 1.714 | Reg loss: 0.037 | Tree loss: 1.714 | Accuracy: 0.419922 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 021 | Total loss: 1.696 | Reg loss: 0.037 | Tree loss: 1.696 | Accuracy: 0.455078 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 021 | Total loss: 1.755 | Reg loss: 0.037 | Tree loss: 1.755 | Accuracy: 0.441406 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 021 | Total loss: 1.711 | Reg loss: 0.037 | Tree loss: 1.711 | Accuracy: 0.406250 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 021 | Total loss: 1.679 | Reg loss: 0.037 | Tree loss: 1.679 | Accuracy: 0.439453 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 021 | Total loss: 1.700 | Reg loss: 0.037 | Tree loss: 1.700 | Accuracy: 0.402344 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 021 | Total loss: 1.642 | Reg loss: 0.037 | Tree loss: 1.642 | Accuracy: 0.423828 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 021 | Total loss: 1.687 | Reg loss: 0.037 | Tree loss: 1.687 | Accuracy: 0.384766 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 021 | Total loss: 1.662 | Reg loss: 0.037 | Tree loss: 1.662 | Accuracy: 0.396484 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 021 | Total loss: 1.610 | Reg loss: 0.037 | Tree loss: 1.610 | Accuracy: 0.437500 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 021 | Total loss: 1.565 | Reg loss: 0.037 | Tree loss: 1.565 | Accuracy: 0.466797 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 021 | Total loss: 1.572 | Reg loss: 0.037 | Tree loss: 1.572 | Accuracy: 0.388672 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 021 | Total loss: 1.572 | Reg loss: 0.037 | Tree loss: 1.572 | Accuracy: 0.417969 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 021 | Total loss: 1.506 | Reg loss: 0.037 | Tree loss: 1.506 | Accuracy: 0.435547 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 021 | Total loss: 1.571 | Reg loss: 0.037 | Tree loss: 1.571 | Accuracy: 0.398438 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 021 | Total loss: 1.484 | Reg loss: 0.037 | Tree loss: 1.484 | Accuracy: 0.419922 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 021 | Total loss: 1.528 | Reg loss: 0.037 | Tree loss: 1.528 | Accuracy: 0.468750 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 021 | Total loss: 1.485 | Reg loss: 0.037 | Tree loss: 1.485 | Accuracy: 0.445312 | 0.11 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 021 | Total loss: 1.437 | Reg loss: 0.037 | Tree loss: 1.437 | Accuracy: 0.389313 | 0.11 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 94 | Batch: 000 / 021 | Total loss: 1.793 | Reg loss: 0.036 | Tree loss: 1.793 | Accuracy: 0.417969 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 021 | Total loss: 1.767 | Reg loss: 0.036 | Tree loss: 1.767 | Accuracy: 0.457031 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 021 | Total loss: 1.696 | Reg loss: 0.036 | Tree loss: 1.696 | Accuracy: 0.437500 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 021 | Total loss: 1.696 | Reg loss: 0.036 | Tree loss: 1.696 | Accuracy: 0.410156 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 021 | Total loss: 1.663 | Reg loss: 0.036 | Tree loss: 1.663 | Accuracy: 0.425781 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 021 | Total loss: 1.759 | Reg loss: 0.036 | Tree loss: 1.759 | Accuracy: 0.433594 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 021 | Total loss: 1.699 | Reg loss: 0.036 | Tree loss: 1.699 | Accuracy: 0.435547 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 021 | Total loss: 1.668 | Reg loss: 0.037 | Tree loss: 1.668 | Accuracy: 0.380859 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 021 | Total loss: 1.657 | Reg loss: 0.037 | Tree loss: 1.657 | Accuracy: 0.435547 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 021 | Total loss: 1.566 | Reg loss: 0.037 | Tree loss: 1.566 | Accuracy: 0.427734 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 021 | Total loss: 1.616 | Reg loss: 0.037 | Tree loss: 1.616 | Accuracy: 0.425781 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 021 | Total loss: 1.584 | Reg loss: 0.037 | Tree loss: 1.584 | Accuracy: 0.398438 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 021 | Total loss: 1.556 | Reg loss: 0.037 | Tree loss: 1.556 | Accuracy: 0.410156 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 021 | Total loss: 1.578 | Reg loss: 0.037 | Tree loss: 1.578 | Accuracy: 0.421875 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 021 | Total loss: 1.538 | Reg loss: 0.037 | Tree loss: 1.538 | Accuracy: 0.447266 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 021 | Total loss: 1.547 | Reg loss: 0.037 | Tree loss: 1.547 | Accuracy: 0.439453 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 021 | Total loss: 1.485 | Reg loss: 0.037 | Tree loss: 1.485 | Accuracy: 0.458984 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 021 | Total loss: 1.537 | Reg loss: 0.037 | Tree loss: 1.537 | Accuracy: 0.425781 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 021 | Total loss: 1.503 | Reg loss: 0.037 | Tree loss: 1.503 | Accuracy: 0.425781 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 021 | Total loss: 1.478 | Reg loss: 0.037 | Tree loss: 1.478 | Accuracy: 0.421875 | 0.11 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 021 | Total loss: 1.584 | Reg loss: 0.037 | Tree loss: 1.584 | Accuracy: 0.374046 | 0.11 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 95 | Batch: 000 / 021 | Total loss: 1.761 | Reg loss: 0.036 | Tree loss: 1.761 | Accuracy: 0.417969 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 021 | Total loss: 1.686 | Reg loss: 0.036 | Tree loss: 1.686 | Accuracy: 0.421875 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 021 | Total loss: 1.734 | Reg loss: 0.036 | Tree loss: 1.734 | Accuracy: 0.453125 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 021 | Total loss: 1.749 | Reg loss: 0.036 | Tree loss: 1.749 | Accuracy: 0.439453 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 021 | Total loss: 1.649 | Reg loss: 0.036 | Tree loss: 1.649 | Accuracy: 0.417969 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 021 | Total loss: 1.685 | Reg loss: 0.036 | Tree loss: 1.685 | Accuracy: 0.410156 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 021 | Total loss: 1.677 | Reg loss: 0.036 | Tree loss: 1.677 | Accuracy: 0.458984 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 021 | Total loss: 1.607 | Reg loss: 0.036 | Tree loss: 1.607 | Accuracy: 0.468750 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 021 | Total loss: 1.654 | Reg loss: 0.036 | Tree loss: 1.654 | Accuracy: 0.386719 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 021 | Total loss: 1.758 | Reg loss: 0.037 | Tree loss: 1.758 | Accuracy: 0.437500 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 021 | Total loss: 1.566 | Reg loss: 0.037 | Tree loss: 1.566 | Accuracy: 0.453125 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 021 | Total loss: 1.594 | Reg loss: 0.037 | Tree loss: 1.594 | Accuracy: 0.423828 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 021 | Total loss: 1.580 | Reg loss: 0.037 | Tree loss: 1.580 | Accuracy: 0.433594 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 021 | Total loss: 1.559 | Reg loss: 0.037 | Tree loss: 1.559 | Accuracy: 0.408203 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 021 | Total loss: 1.517 | Reg loss: 0.037 | Tree loss: 1.517 | Accuracy: 0.406250 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 021 | Total loss: 1.512 | Reg loss: 0.037 | Tree loss: 1.512 | Accuracy: 0.443359 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 021 | Total loss: 1.470 | Reg loss: 0.037 | Tree loss: 1.470 | Accuracy: 0.455078 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 021 | Total loss: 1.495 | Reg loss: 0.037 | Tree loss: 1.495 | Accuracy: 0.435547 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 021 | Total loss: 1.462 | Reg loss: 0.037 | Tree loss: 1.462 | Accuracy: 0.410156 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 021 | Total loss: 1.525 | Reg loss: 0.037 | Tree loss: 1.525 | Accuracy: 0.392578 | 0.11 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 021 | Total loss: 1.471 | Reg loss: 0.037 | Tree loss: 1.471 | Accuracy: 0.396947 | 0.11 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 96 | Batch: 000 / 021 | Total loss: 1.773 | Reg loss: 0.036 | Tree loss: 1.773 | Accuracy: 0.402344 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 021 | Total loss: 1.739 | Reg loss: 0.036 | Tree loss: 1.739 | Accuracy: 0.462891 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 021 | Total loss: 1.710 | Reg loss: 0.036 | Tree loss: 1.710 | Accuracy: 0.416016 | 0.11 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96 | Batch: 003 / 021 | Total loss: 1.697 | Reg loss: 0.036 | Tree loss: 1.697 | Accuracy: 0.443359 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 021 | Total loss: 1.661 | Reg loss: 0.036 | Tree loss: 1.661 | Accuracy: 0.425781 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 021 | Total loss: 1.644 | Reg loss: 0.036 | Tree loss: 1.644 | Accuracy: 0.417969 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 021 | Total loss: 1.612 | Reg loss: 0.036 | Tree loss: 1.612 | Accuracy: 0.417969 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 021 | Total loss: 1.633 | Reg loss: 0.036 | Tree loss: 1.633 | Accuracy: 0.439453 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 021 | Total loss: 1.629 | Reg loss: 0.036 | Tree loss: 1.629 | Accuracy: 0.453125 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 021 | Total loss: 1.643 | Reg loss: 0.036 | Tree loss: 1.643 | Accuracy: 0.435547 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 021 | Total loss: 1.596 | Reg loss: 0.036 | Tree loss: 1.596 | Accuracy: 0.386719 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 021 | Total loss: 1.581 | Reg loss: 0.037 | Tree loss: 1.581 | Accuracy: 0.453125 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 021 | Total loss: 1.604 | Reg loss: 0.037 | Tree loss: 1.604 | Accuracy: 0.402344 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 021 | Total loss: 1.576 | Reg loss: 0.037 | Tree loss: 1.576 | Accuracy: 0.406250 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 021 | Total loss: 1.536 | Reg loss: 0.037 | Tree loss: 1.536 | Accuracy: 0.457031 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 021 | Total loss: 1.492 | Reg loss: 0.037 | Tree loss: 1.492 | Accuracy: 0.400391 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 021 | Total loss: 1.538 | Reg loss: 0.037 | Tree loss: 1.538 | Accuracy: 0.427734 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 021 | Total loss: 1.503 | Reg loss: 0.037 | Tree loss: 1.503 | Accuracy: 0.453125 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 021 | Total loss: 1.442 | Reg loss: 0.037 | Tree loss: 1.442 | Accuracy: 0.447266 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 021 | Total loss: 1.467 | Reg loss: 0.037 | Tree loss: 1.467 | Accuracy: 0.425781 | 0.11 sec/iter\n",
      "Epoch: 96 | Batch: 020 / 021 | Total loss: 1.417 | Reg loss: 0.037 | Tree loss: 1.417 | Accuracy: 0.374046 | 0.11 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 97 | Batch: 000 / 021 | Total loss: 1.739 | Reg loss: 0.036 | Tree loss: 1.739 | Accuracy: 0.388672 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 021 | Total loss: 1.695 | Reg loss: 0.036 | Tree loss: 1.695 | Accuracy: 0.419922 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 021 | Total loss: 1.695 | Reg loss: 0.036 | Tree loss: 1.695 | Accuracy: 0.427734 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 021 | Total loss: 1.656 | Reg loss: 0.036 | Tree loss: 1.656 | Accuracy: 0.460938 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 021 | Total loss: 1.694 | Reg loss: 0.036 | Tree loss: 1.694 | Accuracy: 0.419922 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 021 | Total loss: 1.634 | Reg loss: 0.036 | Tree loss: 1.634 | Accuracy: 0.408203 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 021 | Total loss: 1.594 | Reg loss: 0.036 | Tree loss: 1.594 | Accuracy: 0.457031 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 021 | Total loss: 1.609 | Reg loss: 0.036 | Tree loss: 1.609 | Accuracy: 0.458984 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 021 | Total loss: 1.625 | Reg loss: 0.036 | Tree loss: 1.625 | Accuracy: 0.470703 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 021 | Total loss: 1.596 | Reg loss: 0.036 | Tree loss: 1.596 | Accuracy: 0.404297 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 021 | Total loss: 1.574 | Reg loss: 0.036 | Tree loss: 1.574 | Accuracy: 0.414062 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 021 | Total loss: 1.524 | Reg loss: 0.036 | Tree loss: 1.524 | Accuracy: 0.445312 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 021 | Total loss: 1.609 | Reg loss: 0.037 | Tree loss: 1.609 | Accuracy: 0.390625 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 021 | Total loss: 1.511 | Reg loss: 0.037 | Tree loss: 1.511 | Accuracy: 0.408203 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 021 | Total loss: 1.522 | Reg loss: 0.037 | Tree loss: 1.522 | Accuracy: 0.437500 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 021 | Total loss: 1.513 | Reg loss: 0.037 | Tree loss: 1.513 | Accuracy: 0.414062 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 021 | Total loss: 1.549 | Reg loss: 0.037 | Tree loss: 1.549 | Accuracy: 0.427734 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 021 | Total loss: 1.576 | Reg loss: 0.037 | Tree loss: 1.576 | Accuracy: 0.441406 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 021 | Total loss: 1.459 | Reg loss: 0.037 | Tree loss: 1.459 | Accuracy: 0.431641 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 021 | Total loss: 1.470 | Reg loss: 0.037 | Tree loss: 1.470 | Accuracy: 0.425781 | 0.11 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 021 | Total loss: 1.626 | Reg loss: 0.037 | Tree loss: 1.626 | Accuracy: 0.473282 | 0.11 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 98 | Batch: 000 / 021 | Total loss: 1.728 | Reg loss: 0.036 | Tree loss: 1.728 | Accuracy: 0.439453 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 021 | Total loss: 1.703 | Reg loss: 0.036 | Tree loss: 1.703 | Accuracy: 0.445312 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 021 | Total loss: 1.637 | Reg loss: 0.036 | Tree loss: 1.637 | Accuracy: 0.421875 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 021 | Total loss: 1.686 | Reg loss: 0.036 | Tree loss: 1.686 | Accuracy: 0.433594 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 021 | Total loss: 1.627 | Reg loss: 0.036 | Tree loss: 1.627 | Accuracy: 0.431641 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 021 | Total loss: 1.651 | Reg loss: 0.036 | Tree loss: 1.651 | Accuracy: 0.417969 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 021 | Total loss: 1.603 | Reg loss: 0.036 | Tree loss: 1.603 | Accuracy: 0.445312 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 021 | Total loss: 1.608 | Reg loss: 0.036 | Tree loss: 1.608 | Accuracy: 0.425781 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 021 | Total loss: 1.611 | Reg loss: 0.036 | Tree loss: 1.611 | Accuracy: 0.412109 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 021 | Total loss: 1.619 | Reg loss: 0.036 | Tree loss: 1.619 | Accuracy: 0.447266 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 021 | Total loss: 1.545 | Reg loss: 0.036 | Tree loss: 1.545 | Accuracy: 0.441406 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 021 | Total loss: 1.584 | Reg loss: 0.036 | Tree loss: 1.584 | Accuracy: 0.435547 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 021 | Total loss: 1.490 | Reg loss: 0.036 | Tree loss: 1.490 | Accuracy: 0.417969 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 021 | Total loss: 1.543 | Reg loss: 0.036 | Tree loss: 1.543 | Accuracy: 0.437500 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 021 | Total loss: 1.576 | Reg loss: 0.037 | Tree loss: 1.576 | Accuracy: 0.380859 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 021 | Total loss: 1.497 | Reg loss: 0.037 | Tree loss: 1.497 | Accuracy: 0.435547 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 021 | Total loss: 1.560 | Reg loss: 0.037 | Tree loss: 1.560 | Accuracy: 0.437500 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 021 | Total loss: 1.460 | Reg loss: 0.037 | Tree loss: 1.460 | Accuracy: 0.421875 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 021 | Total loss: 1.476 | Reg loss: 0.037 | Tree loss: 1.476 | Accuracy: 0.425781 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 021 | Total loss: 1.544 | Reg loss: 0.037 | Tree loss: 1.544 | Accuracy: 0.421875 | 0.11 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 021 | Total loss: 1.391 | Reg loss: 0.037 | Tree loss: 1.391 | Accuracy: 0.419847 | 0.11 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 99 | Batch: 000 / 021 | Total loss: 1.726 | Reg loss: 0.036 | Tree loss: 1.726 | Accuracy: 0.412109 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 021 | Total loss: 1.699 | Reg loss: 0.036 | Tree loss: 1.699 | Accuracy: 0.404297 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 021 | Total loss: 1.707 | Reg loss: 0.036 | Tree loss: 1.707 | Accuracy: 0.406250 | 0.11 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 | Batch: 003 / 021 | Total loss: 1.633 | Reg loss: 0.036 | Tree loss: 1.633 | Accuracy: 0.445312 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 021 | Total loss: 1.672 | Reg loss: 0.036 | Tree loss: 1.672 | Accuracy: 0.439453 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 021 | Total loss: 1.665 | Reg loss: 0.036 | Tree loss: 1.665 | Accuracy: 0.423828 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 021 | Total loss: 1.574 | Reg loss: 0.036 | Tree loss: 1.574 | Accuracy: 0.441406 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 021 | Total loss: 1.611 | Reg loss: 0.036 | Tree loss: 1.611 | Accuracy: 0.394531 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 021 | Total loss: 1.561 | Reg loss: 0.036 | Tree loss: 1.561 | Accuracy: 0.453125 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 021 | Total loss: 1.655 | Reg loss: 0.036 | Tree loss: 1.655 | Accuracy: 0.394531 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 021 | Total loss: 1.536 | Reg loss: 0.036 | Tree loss: 1.536 | Accuracy: 0.451172 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 021 | Total loss: 1.597 | Reg loss: 0.036 | Tree loss: 1.597 | Accuracy: 0.460938 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 021 | Total loss: 1.528 | Reg loss: 0.036 | Tree loss: 1.528 | Accuracy: 0.460938 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 021 | Total loss: 1.563 | Reg loss: 0.036 | Tree loss: 1.563 | Accuracy: 0.431641 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 021 | Total loss: 1.466 | Reg loss: 0.036 | Tree loss: 1.466 | Accuracy: 0.445312 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 021 | Total loss: 1.504 | Reg loss: 0.036 | Tree loss: 1.504 | Accuracy: 0.414062 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 021 | Total loss: 1.427 | Reg loss: 0.037 | Tree loss: 1.427 | Accuracy: 0.437500 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 021 | Total loss: 1.519 | Reg loss: 0.037 | Tree loss: 1.519 | Accuracy: 0.421875 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 021 | Total loss: 1.465 | Reg loss: 0.037 | Tree loss: 1.465 | Accuracy: 0.392578 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 019 / 021 | Total loss: 1.455 | Reg loss: 0.037 | Tree loss: 1.455 | Accuracy: 0.455078 | 0.11 sec/iter\n",
      "Epoch: 99 | Batch: 020 / 021 | Total loss: 1.477 | Reg loss: 0.037 | Tree loss: 1.477 | Accuracy: 0.419847 | 0.11 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd258c14483c40238cee0fc2d984a339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d042dbe89a490687572d65173e472b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a098ea6095d143f7b55131e7a0230f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bc0e4aa0bf402286890ac012a446f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 5.393939393939394\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 33\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/miniconda3/envs/rambo/lib/python3.8/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "8869\n",
      "============== Pattern 3 ==============\n",
      "542\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "211\n",
      "============== Pattern 21 ==============\n",
      "749\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "Average comprehensibility: 29.09090909090909\n",
      "std comprehensibility: 4.647342912165046\n",
      "var comprehensibility: 21.59779614325069\n",
      "minimum comprehensibility: 16\n",
      "maximum comprehensibility: 34\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
