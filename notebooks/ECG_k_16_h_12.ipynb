{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.mit_bih import MITBIH\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss, ClassificationKNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 16\n",
    "tree_depth = 12\n",
    "batch_size = 512\n",
    "device = 'cpu'\n",
    "train_data_path = r'<>/mitbih_train.csv'  # replace <> with the correct path of the dataset\n",
    "test_data_path = r'<>/mitbih_test.csv'  # replace <> with the correct path of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_iter = torch.utils.data.DataLoader(MITBIH(train_data_path),\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=1,\n",
    "                                             pin_memory=True,\n",
    "                                             drop_last=True)\n",
    "\n",
    "test_data_iter = torch.utils.data.DataLoader(MITBIH(test_data_path),\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=1,\n",
    "                                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(32, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv1d(32, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=5, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.conv1(y)\n",
    "        y = self.relu1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = y + x\n",
    "        y = self.relu2(y)\n",
    "        y = self.pool(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class ECGModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECGModel, self).__init__()\n",
    "        self.conv = nn.Conv1d(1, 32, kernel_size=5, stride=1, padding=1)\n",
    "        self.block1 = ConvBlock()\n",
    "        self.block2 = ConvBlock()\n",
    "        self.block3 = ConvBlock()\n",
    "        self.block4 = ConvBlock()\n",
    "        self.block5 = ConvBlock()\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 5)\n",
    "\n",
    "    def forward(self, x, return_interm=False):\n",
    "        x = self.conv(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        interm = x.flatten(1)\n",
    "        x = self.fc1(interm)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        if return_interm:\n",
    "            return x, interm\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_crt = ClassificationKNNLoss(k=k).to(device)\n",
    "\n",
    "def train(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, interm = model(batch, return_interm=True)\n",
    "        mse_loss = F.cross_entropy(outputs, target)\n",
    "        mse_loss = mse_loss.sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(interm, target)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = torch.tensor(0).to(device)\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0).to(device)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(loader)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | CLS Loss: {mse_loss.item()}\")\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    for iteration, (batch, target) in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        y_pred = model(batch).argmax(dim=-1)\n",
    "        correct += y_pred.eq(target.view(-1).data).sum()\n",
    "    \n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params: 53957\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "lr = 1e-3\n",
    "log_every = 10\n",
    "\n",
    "model = ECGModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "print(f'#Params: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 200 | iteration 0 / 171 | Total Loss: 7.289229393005371 | KNN Loss: 5.670152187347412 | CLS Loss: 1.6190773248672485\n",
      "Epoch 1 / 200 | iteration 10 / 171 | Total Loss: 4.54009485244751 | KNN Loss: 3.583883047103882 | CLS Loss: 0.9562118053436279\n",
      "Epoch 1 / 200 | iteration 20 / 171 | Total Loss: 4.100872993469238 | KNN Loss: 3.426361560821533 | CLS Loss: 0.6745115518569946\n",
      "Epoch 1 / 200 | iteration 30 / 171 | Total Loss: 3.9538044929504395 | KNN Loss: 3.323531150817871 | CLS Loss: 0.6302732229232788\n",
      "Epoch 1 / 200 | iteration 40 / 171 | Total Loss: 3.8361518383026123 | KNN Loss: 3.2286794185638428 | CLS Loss: 0.6074724793434143\n",
      "Epoch 1 / 200 | iteration 50 / 171 | Total Loss: 3.7472877502441406 | KNN Loss: 3.171922206878662 | CLS Loss: 0.5753654837608337\n",
      "Epoch 1 / 200 | iteration 60 / 171 | Total Loss: 3.6468231678009033 | KNN Loss: 3.116105556488037 | CLS Loss: 0.5307176113128662\n",
      "Epoch 1 / 200 | iteration 70 / 171 | Total Loss: 3.6574208736419678 | KNN Loss: 3.111997127532959 | CLS Loss: 0.545423686504364\n",
      "Epoch 1 / 200 | iteration 80 / 171 | Total Loss: 3.6286494731903076 | KNN Loss: 3.1217424869537354 | CLS Loss: 0.5069069862365723\n",
      "Epoch 1 / 200 | iteration 90 / 171 | Total Loss: 3.6339495182037354 | KNN Loss: 3.126410961151123 | CLS Loss: 0.5075384974479675\n",
      "Epoch 1 / 200 | iteration 100 / 171 | Total Loss: 3.638596534729004 | KNN Loss: 3.19911527633667 | CLS Loss: 0.4394811689853668\n",
      "Epoch 1 / 200 | iteration 110 / 171 | Total Loss: 3.553144931793213 | KNN Loss: 3.112509250640869 | CLS Loss: 0.4406357407569885\n",
      "Epoch 1 / 200 | iteration 120 / 171 | Total Loss: 3.4943554401397705 | KNN Loss: 3.1241137981414795 | CLS Loss: 0.37024158239364624\n",
      "Epoch 1 / 200 | iteration 130 / 171 | Total Loss: 3.4427285194396973 | KNN Loss: 3.0951650142669678 | CLS Loss: 0.34756359457969666\n",
      "Epoch 1 / 200 | iteration 140 / 171 | Total Loss: 3.4729552268981934 | KNN Loss: 3.1561198234558105 | CLS Loss: 0.31683531403541565\n",
      "Epoch 1 / 200 | iteration 150 / 171 | Total Loss: 3.4716293811798096 | KNN Loss: 3.1175272464752197 | CLS Loss: 0.3541020452976227\n",
      "Epoch 1 / 200 | iteration 160 / 171 | Total Loss: 3.4275126457214355 | KNN Loss: 3.1200242042541504 | CLS Loss: 0.3074883222579956\n",
      "Epoch 1 / 200 | iteration 170 / 171 | Total Loss: 3.4399263858795166 | KNN Loss: 3.0775716304779053 | CLS Loss: 0.36235472559928894\n",
      "Epoch: 001, Loss: 3.8388, Train: 0.9151, Valid: 0.9143, Best: 0.9143\n",
      "Epoch 2 / 200 | iteration 0 / 171 | Total Loss: 3.4991343021392822 | KNN Loss: 3.148385763168335 | CLS Loss: 0.35074856877326965\n",
      "Epoch 2 / 200 | iteration 10 / 171 | Total Loss: 3.5217175483703613 | KNN Loss: 3.1172935962677 | CLS Loss: 0.4044240117073059\n",
      "Epoch 2 / 200 | iteration 20 / 171 | Total Loss: 3.4234888553619385 | KNN Loss: 3.1070480346679688 | CLS Loss: 0.3164408206939697\n",
      "Epoch 2 / 200 | iteration 30 / 171 | Total Loss: 3.437802314758301 | KNN Loss: 3.107332944869995 | CLS Loss: 0.3304693400859833\n",
      "Epoch 2 / 200 | iteration 40 / 171 | Total Loss: 3.4815192222595215 | KNN Loss: 3.072628974914551 | CLS Loss: 0.40889036655426025\n",
      "Epoch 2 / 200 | iteration 50 / 171 | Total Loss: 3.4276180267333984 | KNN Loss: 3.086268901824951 | CLS Loss: 0.3413490653038025\n",
      "Epoch 2 / 200 | iteration 60 / 171 | Total Loss: 3.3820302486419678 | KNN Loss: 3.0958404541015625 | CLS Loss: 0.28618982434272766\n",
      "Epoch 2 / 200 | iteration 70 / 171 | Total Loss: 3.3893239498138428 | KNN Loss: 3.092557191848755 | CLS Loss: 0.2967666983604431\n",
      "Epoch 2 / 200 | iteration 80 / 171 | Total Loss: 3.37141489982605 | KNN Loss: 3.1074509620666504 | CLS Loss: 0.26396387815475464\n",
      "Epoch 2 / 200 | iteration 90 / 171 | Total Loss: 3.3451013565063477 | KNN Loss: 3.1110939979553223 | CLS Loss: 0.23400725424289703\n",
      "Epoch 2 / 200 | iteration 100 / 171 | Total Loss: 3.3714771270751953 | KNN Loss: 3.1006040573120117 | CLS Loss: 0.27087315917015076\n",
      "Epoch 2 / 200 | iteration 110 / 171 | Total Loss: 3.36137318611145 | KNN Loss: 3.127319812774658 | CLS Loss: 0.23405326902866364\n",
      "Epoch 2 / 200 | iteration 120 / 171 | Total Loss: 3.3103818893432617 | KNN Loss: 3.09778094291687 | CLS Loss: 0.21260106563568115\n",
      "Epoch 2 / 200 | iteration 130 / 171 | Total Loss: 3.3391199111938477 | KNN Loss: 3.089515447616577 | CLS Loss: 0.2496044933795929\n",
      "Epoch 2 / 200 | iteration 140 / 171 | Total Loss: 3.306222677230835 | KNN Loss: 3.0919222831726074 | CLS Loss: 0.2143004685640335\n",
      "Epoch 2 / 200 | iteration 150 / 171 | Total Loss: 3.309906244277954 | KNN Loss: 3.0291969776153564 | CLS Loss: 0.28070926666259766\n",
      "Epoch 2 / 200 | iteration 160 / 171 | Total Loss: 3.312023639678955 | KNN Loss: 3.077227830886841 | CLS Loss: 0.23479580879211426\n",
      "Epoch 2 / 200 | iteration 170 / 171 | Total Loss: 3.335801839828491 | KNN Loss: 3.0541789531707764 | CLS Loss: 0.28162291646003723\n",
      "Epoch: 002, Loss: 3.3776, Train: 0.9388, Valid: 0.9374, Best: 0.9374\n",
      "Epoch 3 / 200 | iteration 0 / 171 | Total Loss: 3.2646291255950928 | KNN Loss: 3.0572268962860107 | CLS Loss: 0.2074023187160492\n",
      "Epoch 3 / 200 | iteration 10 / 171 | Total Loss: 3.345008373260498 | KNN Loss: 3.1209309101104736 | CLS Loss: 0.22407746315002441\n",
      "Epoch 3 / 200 | iteration 20 / 171 | Total Loss: 3.3114190101623535 | KNN Loss: 3.0583608150482178 | CLS Loss: 0.2530581057071686\n",
      "Epoch 3 / 200 | iteration 30 / 171 | Total Loss: 3.251197338104248 | KNN Loss: 3.068509817123413 | CLS Loss: 0.18268759548664093\n",
      "Epoch 3 / 200 | iteration 40 / 171 | Total Loss: 3.2795257568359375 | KNN Loss: 3.0853629112243652 | CLS Loss: 0.1941627562046051\n",
      "Epoch 3 / 200 | iteration 50 / 171 | Total Loss: 3.3103630542755127 | KNN Loss: 3.081066131591797 | CLS Loss: 0.22929689288139343\n",
      "Epoch 3 / 200 | iteration 60 / 171 | Total Loss: 3.2417399883270264 | KNN Loss: 3.084292411804199 | CLS Loss: 0.15744748711585999\n",
      "Epoch 3 / 200 | iteration 70 / 171 | Total Loss: 3.2646713256835938 | KNN Loss: 3.0701520442962646 | CLS Loss: 0.1945192962884903\n",
      "Epoch 3 / 200 | iteration 80 / 171 | Total Loss: 3.2493386268615723 | KNN Loss: 3.114445924758911 | CLS Loss: 0.1348927617073059\n",
      "Epoch 3 / 200 | iteration 90 / 171 | Total Loss: 3.2619361877441406 | KNN Loss: 3.0794479846954346 | CLS Loss: 0.1824883222579956\n",
      "Epoch 3 / 200 | iteration 100 / 171 | Total Loss: 3.238130807876587 | KNN Loss: 3.1007089614868164 | CLS Loss: 0.13742183148860931\n",
      "Epoch 3 / 200 | iteration 110 / 171 | Total Loss: 3.280499219894409 | KNN Loss: 3.1023879051208496 | CLS Loss: 0.17811141908168793\n",
      "Epoch 3 / 200 | iteration 120 / 171 | Total Loss: 3.2969954013824463 | KNN Loss: 3.1073522567749023 | CLS Loss: 0.1896432489156723\n",
      "Epoch 3 / 200 | iteration 130 / 171 | Total Loss: 3.2430944442749023 | KNN Loss: 3.064349889755249 | CLS Loss: 0.17874443531036377\n",
      "Epoch 3 / 200 | iteration 140 / 171 | Total Loss: 3.26908278465271 | KNN Loss: 3.059629440307617 | CLS Loss: 0.20945340394973755\n",
      "Epoch 3 / 200 | iteration 150 / 171 | Total Loss: 3.247678279876709 | KNN Loss: 3.0655996799468994 | CLS Loss: 0.18207861483097076\n",
      "Epoch 3 / 200 | iteration 160 / 171 | Total Loss: 3.288226366043091 | KNN Loss: 3.066493034362793 | CLS Loss: 0.2217332422733307\n",
      "Epoch 3 / 200 | iteration 170 / 171 | Total Loss: 3.234579563140869 | KNN Loss: 3.083357334136963 | CLS Loss: 0.15122218430042267\n",
      "Epoch: 003, Loss: 3.2812, Train: 0.9574, Valid: 0.9565, Best: 0.9565\n",
      "Epoch 4 / 200 | iteration 0 / 171 | Total Loss: 3.237377166748047 | KNN Loss: 3.1051695346832275 | CLS Loss: 0.1322076916694641\n",
      "Epoch 4 / 200 | iteration 10 / 171 | Total Loss: 3.2362937927246094 | KNN Loss: 3.1040077209472656 | CLS Loss: 0.1322859525680542\n",
      "Epoch 4 / 200 | iteration 20 / 171 | Total Loss: 3.2985074520111084 | KNN Loss: 3.1233937740325928 | CLS Loss: 0.17511366307735443\n",
      "Epoch 4 / 200 | iteration 30 / 171 | Total Loss: 3.227590799331665 | KNN Loss: 3.0686488151550293 | CLS Loss: 0.1589420735836029\n",
      "Epoch 4 / 200 | iteration 40 / 171 | Total Loss: 3.2709243297576904 | KNN Loss: 3.0405304431915283 | CLS Loss: 0.23039382696151733\n",
      "Epoch 4 / 200 | iteration 50 / 171 | Total Loss: 3.2221643924713135 | KNN Loss: 3.064239025115967 | CLS Loss: 0.15792535245418549\n",
      "Epoch 4 / 200 | iteration 60 / 171 | Total Loss: 3.271247386932373 | KNN Loss: 3.081223726272583 | CLS Loss: 0.19002360105514526\n",
      "Epoch 4 / 200 | iteration 70 / 171 | Total Loss: 3.195162057876587 | KNN Loss: 3.06860613822937 | CLS Loss: 0.126555934548378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 / 200 | iteration 80 / 171 | Total Loss: 3.2328104972839355 | KNN Loss: 3.114835500717163 | CLS Loss: 0.1179749146103859\n",
      "Epoch 4 / 200 | iteration 90 / 171 | Total Loss: 3.244462728500366 | KNN Loss: 3.0576844215393066 | CLS Loss: 0.18677841126918793\n",
      "Epoch 4 / 200 | iteration 100 / 171 | Total Loss: 3.216850757598877 | KNN Loss: 3.074451208114624 | CLS Loss: 0.14239966869354248\n",
      "Epoch 4 / 200 | iteration 110 / 171 | Total Loss: 3.218621253967285 | KNN Loss: 3.038322687149048 | CLS Loss: 0.18029865622520447\n",
      "Epoch 4 / 200 | iteration 120 / 171 | Total Loss: 3.227478265762329 | KNN Loss: 3.080357313156128 | CLS Loss: 0.147120863199234\n",
      "Epoch 4 / 200 | iteration 130 / 171 | Total Loss: 3.2290446758270264 | KNN Loss: 3.0661919116973877 | CLS Loss: 0.16285265982151031\n",
      "Epoch 4 / 200 | iteration 140 / 171 | Total Loss: 3.168565511703491 | KNN Loss: 3.079369068145752 | CLS Loss: 0.08919636905193329\n",
      "Epoch 4 / 200 | iteration 150 / 171 | Total Loss: 3.2440149784088135 | KNN Loss: 3.034787893295288 | CLS Loss: 0.20922702550888062\n",
      "Epoch 4 / 200 | iteration 160 / 171 | Total Loss: 3.209301471710205 | KNN Loss: 3.090468168258667 | CLS Loss: 0.1188332810997963\n",
      "Epoch 4 / 200 | iteration 170 / 171 | Total Loss: 3.205538511276245 | KNN Loss: 3.0881402492523193 | CLS Loss: 0.11739816516637802\n",
      "Epoch: 004, Loss: 3.2297, Train: 0.9610, Valid: 0.9604, Best: 0.9604\n",
      "Epoch 5 / 200 | iteration 0 / 171 | Total Loss: 3.187124729156494 | KNN Loss: 3.043412685394287 | CLS Loss: 0.143712118268013\n",
      "Epoch 5 / 200 | iteration 10 / 171 | Total Loss: 3.2165327072143555 | KNN Loss: 3.0765442848205566 | CLS Loss: 0.13998830318450928\n",
      "Epoch 5 / 200 | iteration 20 / 171 | Total Loss: 3.1739423274993896 | KNN Loss: 3.067579507827759 | CLS Loss: 0.10636282712221146\n",
      "Epoch 5 / 200 | iteration 30 / 171 | Total Loss: 3.1480660438537598 | KNN Loss: 3.059201240539551 | CLS Loss: 0.0888647809624672\n",
      "Epoch 5 / 200 | iteration 40 / 171 | Total Loss: 3.265761375427246 | KNN Loss: 3.1209731101989746 | CLS Loss: 0.14478817582130432\n",
      "Epoch 5 / 200 | iteration 50 / 171 | Total Loss: 3.2142832279205322 | KNN Loss: 3.0817039012908936 | CLS Loss: 0.1325792670249939\n",
      "Epoch 5 / 200 | iteration 60 / 171 | Total Loss: 3.2144320011138916 | KNN Loss: 3.060731887817383 | CLS Loss: 0.15370002388954163\n",
      "Epoch 5 / 200 | iteration 70 / 171 | Total Loss: 3.190145969390869 | KNN Loss: 3.040037155151367 | CLS Loss: 0.15010890364646912\n",
      "Epoch 5 / 200 | iteration 80 / 171 | Total Loss: 3.204739570617676 | KNN Loss: 3.0564329624176025 | CLS Loss: 0.14830656349658966\n",
      "Epoch 5 / 200 | iteration 90 / 171 | Total Loss: 3.182176113128662 | KNN Loss: 3.0623064041137695 | CLS Loss: 0.1198696717619896\n",
      "Epoch 5 / 200 | iteration 100 / 171 | Total Loss: 3.1704912185668945 | KNN Loss: 3.0251591205596924 | CLS Loss: 0.14533209800720215\n",
      "Epoch 5 / 200 | iteration 110 / 171 | Total Loss: 3.201669692993164 | KNN Loss: 3.0515947341918945 | CLS Loss: 0.15007489919662476\n",
      "Epoch 5 / 200 | iteration 120 / 171 | Total Loss: 3.21321964263916 | KNN Loss: 3.109550952911377 | CLS Loss: 0.1036687046289444\n",
      "Epoch 5 / 200 | iteration 130 / 171 | Total Loss: 3.1804237365722656 | KNN Loss: 3.0151636600494385 | CLS Loss: 0.1652599722146988\n",
      "Epoch 5 / 200 | iteration 140 / 171 | Total Loss: 3.127595901489258 | KNN Loss: 3.0468530654907227 | CLS Loss: 0.08074277639389038\n",
      "Epoch 5 / 200 | iteration 150 / 171 | Total Loss: 3.255040168762207 | KNN Loss: 3.064814567565918 | CLS Loss: 0.19022560119628906\n",
      "Epoch 5 / 200 | iteration 160 / 171 | Total Loss: 3.250976800918579 | KNN Loss: 3.077087640762329 | CLS Loss: 0.1738891750574112\n",
      "Epoch 5 / 200 | iteration 170 / 171 | Total Loss: 3.171945810317993 | KNN Loss: 3.0775532722473145 | CLS Loss: 0.09439260512590408\n",
      "Epoch: 005, Loss: 3.2040, Train: 0.9678, Valid: 0.9666, Best: 0.9666\n",
      "Epoch 6 / 200 | iteration 0 / 171 | Total Loss: 3.186206817626953 | KNN Loss: 3.066284656524658 | CLS Loss: 0.11992209404706955\n",
      "Epoch 6 / 200 | iteration 10 / 171 | Total Loss: 3.222658634185791 | KNN Loss: 3.0736100673675537 | CLS Loss: 0.1490485668182373\n",
      "Epoch 6 / 200 | iteration 20 / 171 | Total Loss: 3.163912296295166 | KNN Loss: 3.06235671043396 | CLS Loss: 0.10155566781759262\n",
      "Epoch 6 / 200 | iteration 30 / 171 | Total Loss: 3.1681911945343018 | KNN Loss: 3.037899971008301 | CLS Loss: 0.13029129803180695\n",
      "Epoch 6 / 200 | iteration 40 / 171 | Total Loss: 3.2205350399017334 | KNN Loss: 3.0104563236236572 | CLS Loss: 0.21007876098155975\n",
      "Epoch 6 / 200 | iteration 50 / 171 | Total Loss: 3.158304452896118 | KNN Loss: 3.066584348678589 | CLS Loss: 0.09172014892101288\n",
      "Epoch 6 / 200 | iteration 60 / 171 | Total Loss: 3.1535074710845947 | KNN Loss: 3.0486416816711426 | CLS Loss: 0.10486570745706558\n",
      "Epoch 6 / 200 | iteration 70 / 171 | Total Loss: 3.186396360397339 | KNN Loss: 3.0850484371185303 | CLS Loss: 0.10134793817996979\n",
      "Epoch 6 / 200 | iteration 80 / 171 | Total Loss: 3.174720525741577 | KNN Loss: 3.046612501144409 | CLS Loss: 0.12810805439949036\n",
      "Epoch 6 / 200 | iteration 90 / 171 | Total Loss: 3.1367971897125244 | KNN Loss: 3.034031629562378 | CLS Loss: 0.10276557505130768\n",
      "Epoch 6 / 200 | iteration 100 / 171 | Total Loss: 3.1309804916381836 | KNN Loss: 3.0371286869049072 | CLS Loss: 0.09385175257921219\n",
      "Epoch 6 / 200 | iteration 110 / 171 | Total Loss: 3.1460320949554443 | KNN Loss: 3.065096855163574 | CLS Loss: 0.0809352919459343\n",
      "Epoch 6 / 200 | iteration 120 / 171 | Total Loss: 3.1911611557006836 | KNN Loss: 3.056671619415283 | CLS Loss: 0.1344895213842392\n",
      "Epoch 6 / 200 | iteration 130 / 171 | Total Loss: 3.210094451904297 | KNN Loss: 3.0673792362213135 | CLS Loss: 0.14271509647369385\n",
      "Epoch 6 / 200 | iteration 140 / 171 | Total Loss: 3.1782498359680176 | KNN Loss: 3.0302436351776123 | CLS Loss: 0.1480061262845993\n",
      "Epoch 6 / 200 | iteration 150 / 171 | Total Loss: 3.164050817489624 | KNN Loss: 3.068866491317749 | CLS Loss: 0.0951843410730362\n",
      "Epoch 6 / 200 | iteration 160 / 171 | Total Loss: 3.152130126953125 | KNN Loss: 3.0429162979125977 | CLS Loss: 0.1092139333486557\n",
      "Epoch 6 / 200 | iteration 170 / 171 | Total Loss: 3.16198468208313 | KNN Loss: 3.031832218170166 | CLS Loss: 0.13015246391296387\n",
      "Epoch: 006, Loss: 3.1727, Train: 0.9731, Valid: 0.9711, Best: 0.9711\n",
      "Epoch 7 / 200 | iteration 0 / 171 | Total Loss: 3.1782405376434326 | KNN Loss: 3.0654501914978027 | CLS Loss: 0.11279043555259705\n",
      "Epoch 7 / 200 | iteration 10 / 171 | Total Loss: 3.2008039951324463 | KNN Loss: 3.0407519340515137 | CLS Loss: 0.16005197167396545\n",
      "Epoch 7 / 200 | iteration 20 / 171 | Total Loss: 3.1990983486175537 | KNN Loss: 3.0735373497009277 | CLS Loss: 0.1255609542131424\n",
      "Epoch 7 / 200 | iteration 30 / 171 | Total Loss: 3.1655709743499756 | KNN Loss: 3.05815052986145 | CLS Loss: 0.1074204072356224\n",
      "Epoch 7 / 200 | iteration 40 / 171 | Total Loss: 3.162710428237915 | KNN Loss: 3.0151567459106445 | CLS Loss: 0.14755365252494812\n",
      "Epoch 7 / 200 | iteration 50 / 171 | Total Loss: 3.121304750442505 | KNN Loss: 3.044340133666992 | CLS Loss: 0.07696470618247986\n",
      "Epoch 7 / 200 | iteration 60 / 171 | Total Loss: 3.176898956298828 | KNN Loss: 3.057450771331787 | CLS Loss: 0.11944806575775146\n",
      "Epoch 7 / 200 | iteration 70 / 171 | Total Loss: 3.1626131534576416 | KNN Loss: 3.067756175994873 | CLS Loss: 0.09485705941915512\n",
      "Epoch 7 / 200 | iteration 80 / 171 | Total Loss: 3.1837711334228516 | KNN Loss: 3.0478675365448 | CLS Loss: 0.1359034776687622\n",
      "Epoch 7 / 200 | iteration 90 / 171 | Total Loss: 3.194082736968994 | KNN Loss: 3.0685575008392334 | CLS Loss: 0.1255253106355667\n",
      "Epoch 7 / 200 | iteration 100 / 171 | Total Loss: 3.1895456314086914 | KNN Loss: 3.0586254596710205 | CLS Loss: 0.13092029094696045\n",
      "Epoch 7 / 200 | iteration 110 / 171 | Total Loss: 3.2178292274475098 | KNN Loss: 3.049111843109131 | CLS Loss: 0.1687174290418625\n",
      "Epoch 7 / 200 | iteration 120 / 171 | Total Loss: 3.1862707138061523 | KNN Loss: 3.033332347869873 | CLS Loss: 0.15293847024440765\n",
      "Epoch 7 / 200 | iteration 130 / 171 | Total Loss: 3.14890718460083 | KNN Loss: 3.0707645416259766 | CLS Loss: 0.07814273238182068\n",
      "Epoch 7 / 200 | iteration 140 / 171 | Total Loss: 3.136265993118286 | KNN Loss: 3.0539495944976807 | CLS Loss: 0.08231649547815323\n",
      "Epoch 7 / 200 | iteration 150 / 171 | Total Loss: 3.1387319564819336 | KNN Loss: 3.035628318786621 | CLS Loss: 0.10310357064008713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 / 200 | iteration 160 / 171 | Total Loss: 3.1802217960357666 | KNN Loss: 3.0711419582366943 | CLS Loss: 0.10907985270023346\n",
      "Epoch 7 / 200 | iteration 170 / 171 | Total Loss: 3.1237998008728027 | KNN Loss: 3.055849552154541 | CLS Loss: 0.06795023381710052\n",
      "Epoch: 007, Loss: 3.1563, Train: 0.9763, Valid: 0.9734, Best: 0.9734\n",
      "Epoch 8 / 200 | iteration 0 / 171 | Total Loss: 3.1230552196502686 | KNN Loss: 3.047787666320801 | CLS Loss: 0.07526752352714539\n",
      "Epoch 8 / 200 | iteration 10 / 171 | Total Loss: 3.1652050018310547 | KNN Loss: 3.0534515380859375 | CLS Loss: 0.11175335198640823\n",
      "Epoch 8 / 200 | iteration 20 / 171 | Total Loss: 3.1704797744750977 | KNN Loss: 3.0480620861053467 | CLS Loss: 0.12241765111684799\n",
      "Epoch 8 / 200 | iteration 30 / 171 | Total Loss: 3.116887331008911 | KNN Loss: 3.026285409927368 | CLS Loss: 0.09060196578502655\n",
      "Epoch 8 / 200 | iteration 40 / 171 | Total Loss: 3.147453546524048 | KNN Loss: 3.0419952869415283 | CLS Loss: 0.10545828193426132\n",
      "Epoch 8 / 200 | iteration 50 / 171 | Total Loss: 3.104954481124878 | KNN Loss: 3.052185535430908 | CLS Loss: 0.05276886373758316\n",
      "Epoch 8 / 200 | iteration 60 / 171 | Total Loss: 3.1364152431488037 | KNN Loss: 3.0680360794067383 | CLS Loss: 0.06837927550077438\n",
      "Epoch 8 / 200 | iteration 70 / 171 | Total Loss: 3.178168296813965 | KNN Loss: 3.0559799671173096 | CLS Loss: 0.12218841910362244\n",
      "Epoch 8 / 200 | iteration 80 / 171 | Total Loss: 3.2274169921875 | KNN Loss: 3.1482653617858887 | CLS Loss: 0.07915174216032028\n",
      "Epoch 8 / 200 | iteration 90 / 171 | Total Loss: 3.1666998863220215 | KNN Loss: 3.054753541946411 | CLS Loss: 0.1119464561343193\n",
      "Epoch 8 / 200 | iteration 100 / 171 | Total Loss: 3.1238157749176025 | KNN Loss: 3.056196928024292 | CLS Loss: 0.06761886179447174\n",
      "Epoch 8 / 200 | iteration 110 / 171 | Total Loss: 3.1096794605255127 | KNN Loss: 3.04423451423645 | CLS Loss: 0.06544502824544907\n",
      "Epoch 8 / 200 | iteration 120 / 171 | Total Loss: 3.171109914779663 | KNN Loss: 3.050823926925659 | CLS Loss: 0.12028592079877853\n",
      "Epoch 8 / 200 | iteration 130 / 171 | Total Loss: 3.1841487884521484 | KNN Loss: 3.029033899307251 | CLS Loss: 0.1551148146390915\n",
      "Epoch 8 / 200 | iteration 140 / 171 | Total Loss: 3.178278684616089 | KNN Loss: 3.0872433185577393 | CLS Loss: 0.09103525429964066\n",
      "Epoch 8 / 200 | iteration 150 / 171 | Total Loss: 3.0810658931732178 | KNN Loss: 3.040790557861328 | CLS Loss: 0.04027532786130905\n",
      "Epoch 8 / 200 | iteration 160 / 171 | Total Loss: 3.105058193206787 | KNN Loss: 3.0093860626220703 | CLS Loss: 0.09567208588123322\n",
      "Epoch 8 / 200 | iteration 170 / 171 | Total Loss: 3.1599907875061035 | KNN Loss: 3.0659854412078857 | CLS Loss: 0.0940052792429924\n",
      "Epoch: 008, Loss: 3.1453, Train: 0.9772, Valid: 0.9744, Best: 0.9744\n",
      "Epoch 9 / 200 | iteration 0 / 171 | Total Loss: 3.146420955657959 | KNN Loss: 3.0666165351867676 | CLS Loss: 0.07980437576770782\n",
      "Epoch 9 / 200 | iteration 10 / 171 | Total Loss: 3.113081216812134 | KNN Loss: 3.003737688064575 | CLS Loss: 0.10934344679117203\n",
      "Epoch 9 / 200 | iteration 20 / 171 | Total Loss: 3.150012731552124 | KNN Loss: 3.039780616760254 | CLS Loss: 0.11023201048374176\n",
      "Epoch 9 / 200 | iteration 30 / 171 | Total Loss: 3.134528875350952 | KNN Loss: 3.046827793121338 | CLS Loss: 0.08770116418600082\n",
      "Epoch 9 / 200 | iteration 40 / 171 | Total Loss: 3.1249048709869385 | KNN Loss: 3.065451145172119 | CLS Loss: 0.05945373326539993\n",
      "Epoch 9 / 200 | iteration 50 / 171 | Total Loss: 3.1085407733917236 | KNN Loss: 3.0239217281341553 | CLS Loss: 0.08461903780698776\n",
      "Epoch 9 / 200 | iteration 60 / 171 | Total Loss: 3.101019859313965 | KNN Loss: 3.0252697467803955 | CLS Loss: 0.07575021684169769\n",
      "Epoch 9 / 200 | iteration 70 / 171 | Total Loss: 3.119384765625 | KNN Loss: 3.0586435794830322 | CLS Loss: 0.06074123829603195\n",
      "Epoch 9 / 200 | iteration 80 / 171 | Total Loss: 3.149090051651001 | KNN Loss: 3.0180704593658447 | CLS Loss: 0.13101957738399506\n",
      "Epoch 9 / 200 | iteration 90 / 171 | Total Loss: 3.1438944339752197 | KNN Loss: 3.0331809520721436 | CLS Loss: 0.11071348935365677\n",
      "Epoch 9 / 200 | iteration 100 / 171 | Total Loss: 3.1461849212646484 | KNN Loss: 3.0486867427825928 | CLS Loss: 0.09749811142683029\n",
      "Epoch 9 / 200 | iteration 110 / 171 | Total Loss: 3.1754424571990967 | KNN Loss: 3.094451904296875 | CLS Loss: 0.08099063485860825\n",
      "Epoch 9 / 200 | iteration 120 / 171 | Total Loss: 3.1791608333587646 | KNN Loss: 3.061997175216675 | CLS Loss: 0.11716373264789581\n",
      "Epoch 9 / 200 | iteration 130 / 171 | Total Loss: 3.1502630710601807 | KNN Loss: 3.0426387786865234 | CLS Loss: 0.10762433707714081\n",
      "Epoch 9 / 200 | iteration 140 / 171 | Total Loss: 3.119594097137451 | KNN Loss: 3.064826726913452 | CLS Loss: 0.05476733669638634\n",
      "Epoch 9 / 200 | iteration 150 / 171 | Total Loss: 3.1442441940307617 | KNN Loss: 3.0584027767181396 | CLS Loss: 0.08584143966436386\n",
      "Epoch 9 / 200 | iteration 160 / 171 | Total Loss: 3.1372296810150146 | KNN Loss: 3.0729899406433105 | CLS Loss: 0.0642397329211235\n",
      "Epoch 9 / 200 | iteration 170 / 171 | Total Loss: 3.119081497192383 | KNN Loss: 3.0229122638702393 | CLS Loss: 0.09616918116807938\n",
      "Epoch: 009, Loss: 3.1329, Train: 0.9797, Valid: 0.9763, Best: 0.9763\n",
      "Epoch 10 / 200 | iteration 0 / 171 | Total Loss: 3.1094985008239746 | KNN Loss: 3.0196003913879395 | CLS Loss: 0.08989806473255157\n",
      "Epoch 10 / 200 | iteration 10 / 171 | Total Loss: 3.1020476818084717 | KNN Loss: 3.0353922843933105 | CLS Loss: 0.06665528565645218\n",
      "Epoch 10 / 200 | iteration 20 / 171 | Total Loss: 3.1203713417053223 | KNN Loss: 3.0402908325195312 | CLS Loss: 0.08008039742708206\n",
      "Epoch 10 / 200 | iteration 30 / 171 | Total Loss: 3.1419565677642822 | KNN Loss: 3.0478713512420654 | CLS Loss: 0.09408530592918396\n",
      "Epoch 10 / 200 | iteration 40 / 171 | Total Loss: 3.142195224761963 | KNN Loss: 3.064072847366333 | CLS Loss: 0.07812244445085526\n",
      "Epoch 10 / 200 | iteration 50 / 171 | Total Loss: 3.113936424255371 | KNN Loss: 3.0504250526428223 | CLS Loss: 0.06351138651371002\n",
      "Epoch 10 / 200 | iteration 60 / 171 | Total Loss: 3.178100109100342 | KNN Loss: 3.057079792022705 | CLS Loss: 0.12102039158344269\n",
      "Epoch 10 / 200 | iteration 70 / 171 | Total Loss: 3.112755298614502 | KNN Loss: 3.0583384037017822 | CLS Loss: 0.05441686883568764\n",
      "Epoch 10 / 200 | iteration 80 / 171 | Total Loss: 3.155529737472534 | KNN Loss: 3.053532123565674 | CLS Loss: 0.10199753195047379\n",
      "Epoch 10 / 200 | iteration 90 / 171 | Total Loss: 3.1356663703918457 | KNN Loss: 2.998422622680664 | CLS Loss: 0.13724377751350403\n",
      "Epoch 10 / 200 | iteration 100 / 171 | Total Loss: 3.1194326877593994 | KNN Loss: 3.0398848056793213 | CLS Loss: 0.07954781502485275\n",
      "Epoch 10 / 200 | iteration 110 / 171 | Total Loss: 3.11948561668396 | KNN Loss: 3.0341086387634277 | CLS Loss: 0.08537694066762924\n",
      "Epoch 10 / 200 | iteration 120 / 171 | Total Loss: 3.093867540359497 | KNN Loss: 3.040523052215576 | CLS Loss: 0.05334438011050224\n",
      "Epoch 10 / 200 | iteration 130 / 171 | Total Loss: 3.106539249420166 | KNN Loss: 3.047454595565796 | CLS Loss: 0.059084728360176086\n",
      "Epoch 10 / 200 | iteration 140 / 171 | Total Loss: 3.1459085941314697 | KNN Loss: 3.048964262008667 | CLS Loss: 0.09694425016641617\n",
      "Epoch 10 / 200 | iteration 150 / 171 | Total Loss: 3.167287588119507 | KNN Loss: 3.0314877033233643 | CLS Loss: 0.13579979538917542\n",
      "Epoch 10 / 200 | iteration 160 / 171 | Total Loss: 3.1499156951904297 | KNN Loss: 3.065194606781006 | CLS Loss: 0.08472099155187607\n",
      "Epoch 10 / 200 | iteration 170 / 171 | Total Loss: 3.1094491481781006 | KNN Loss: 3.0519635677337646 | CLS Loss: 0.057485658675432205\n",
      "Epoch: 010, Loss: 3.1335, Train: 0.9811, Valid: 0.9783, Best: 0.9783\n",
      "Epoch 11 / 200 | iteration 0 / 171 | Total Loss: 3.0969836711883545 | KNN Loss: 3.02053165435791 | CLS Loss: 0.07645202428102493\n",
      "Epoch 11 / 200 | iteration 10 / 171 | Total Loss: 3.164341926574707 | KNN Loss: 3.054868459701538 | CLS Loss: 0.10947354882955551\n",
      "Epoch 11 / 200 | iteration 20 / 171 | Total Loss: 3.1358087062835693 | KNN Loss: 3.0300471782684326 | CLS Loss: 0.10576149076223373\n",
      "Epoch 11 / 200 | iteration 30 / 171 | Total Loss: 3.1140449047088623 | KNN Loss: 3.0601437091827393 | CLS Loss: 0.053901251405477524\n",
      "Epoch 11 / 200 | iteration 40 / 171 | Total Loss: 3.1358861923217773 | KNN Loss: 3.0521531105041504 | CLS Loss: 0.08373304456472397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 / 200 | iteration 50 / 171 | Total Loss: 3.1250813007354736 | KNN Loss: 3.0158817768096924 | CLS Loss: 0.10919956862926483\n",
      "Epoch 11 / 200 | iteration 60 / 171 | Total Loss: 3.102080821990967 | KNN Loss: 3.026804208755493 | CLS Loss: 0.07527655363082886\n",
      "Epoch 11 / 200 | iteration 70 / 171 | Total Loss: 3.106606960296631 | KNN Loss: 2.999403715133667 | CLS Loss: 0.10720334947109222\n",
      "Epoch 11 / 200 | iteration 80 / 171 | Total Loss: 3.098684310913086 | KNN Loss: 3.0441627502441406 | CLS Loss: 0.05452164262533188\n",
      "Epoch 11 / 200 | iteration 90 / 171 | Total Loss: 3.1193268299102783 | KNN Loss: 3.0435986518859863 | CLS Loss: 0.07572827488183975\n",
      "Epoch 11 / 200 | iteration 100 / 171 | Total Loss: 3.141392469406128 | KNN Loss: 3.0385537147521973 | CLS Loss: 0.10283870249986649\n",
      "Epoch 11 / 200 | iteration 110 / 171 | Total Loss: 3.1121749877929688 | KNN Loss: 3.045490026473999 | CLS Loss: 0.06668485701084137\n",
      "Epoch 11 / 200 | iteration 120 / 171 | Total Loss: 3.1047325134277344 | KNN Loss: 3.064607858657837 | CLS Loss: 0.040124569088220596\n",
      "Epoch 11 / 200 | iteration 130 / 171 | Total Loss: 3.1002120971679688 | KNN Loss: 3.0361404418945312 | CLS Loss: 0.0640716552734375\n",
      "Epoch 11 / 200 | iteration 140 / 171 | Total Loss: 3.126267671585083 | KNN Loss: 3.043807029724121 | CLS Loss: 0.08246069401502609\n",
      "Epoch 11 / 200 | iteration 150 / 171 | Total Loss: 3.124053478240967 | KNN Loss: 3.0711114406585693 | CLS Loss: 0.052942000329494476\n",
      "Epoch 11 / 200 | iteration 160 / 171 | Total Loss: 3.1775312423706055 | KNN Loss: 3.0661346912384033 | CLS Loss: 0.1113964319229126\n",
      "Epoch 11 / 200 | iteration 170 / 171 | Total Loss: 3.117795467376709 | KNN Loss: 3.0566534996032715 | CLS Loss: 0.061142049729824066\n",
      "Epoch: 011, Loss: 3.1241, Train: 0.9817, Valid: 0.9779, Best: 0.9783\n",
      "Epoch 12 / 200 | iteration 0 / 171 | Total Loss: 3.1079611778259277 | KNN Loss: 3.0224030017852783 | CLS Loss: 0.08555806428194046\n",
      "Epoch 12 / 200 | iteration 10 / 171 | Total Loss: 3.163935422897339 | KNN Loss: 3.067815065383911 | CLS Loss: 0.09612025320529938\n",
      "Epoch 12 / 200 | iteration 20 / 171 | Total Loss: 3.0974862575531006 | KNN Loss: 3.0212693214416504 | CLS Loss: 0.07621697336435318\n",
      "Epoch 12 / 200 | iteration 30 / 171 | Total Loss: 3.1152608394622803 | KNN Loss: 3.0333425998687744 | CLS Loss: 0.08191826194524765\n",
      "Epoch 12 / 200 | iteration 40 / 171 | Total Loss: 3.107428789138794 | KNN Loss: 3.0603835582733154 | CLS Loss: 0.047045279294252396\n",
      "Epoch 12 / 200 | iteration 50 / 171 | Total Loss: 3.0834550857543945 | KNN Loss: 3.024613857269287 | CLS Loss: 0.05884123593568802\n",
      "Epoch 12 / 200 | iteration 60 / 171 | Total Loss: 3.164297580718994 | KNN Loss: 3.073566198348999 | CLS Loss: 0.09073136746883392\n",
      "Epoch 12 / 200 | iteration 70 / 171 | Total Loss: 3.1304211616516113 | KNN Loss: 3.0626611709594727 | CLS Loss: 0.06775998324155807\n",
      "Epoch 12 / 200 | iteration 80 / 171 | Total Loss: 3.1215431690216064 | KNN Loss: 3.032177448272705 | CLS Loss: 0.08936575055122375\n",
      "Epoch 12 / 200 | iteration 90 / 171 | Total Loss: 3.124295711517334 | KNN Loss: 3.0386033058166504 | CLS Loss: 0.0856923758983612\n",
      "Epoch 12 / 200 | iteration 100 / 171 | Total Loss: 3.090847969055176 | KNN Loss: 3.030816078186035 | CLS Loss: 0.060031987726688385\n",
      "Epoch 12 / 200 | iteration 110 / 171 | Total Loss: 3.123175859451294 | KNN Loss: 3.040299415588379 | CLS Loss: 0.08287645131349564\n",
      "Epoch 12 / 200 | iteration 120 / 171 | Total Loss: 3.1452085971832275 | KNN Loss: 3.0421526432037354 | CLS Loss: 0.10305589437484741\n",
      "Epoch 12 / 200 | iteration 130 / 171 | Total Loss: 3.1190149784088135 | KNN Loss: 3.0412368774414062 | CLS Loss: 0.07777803391218185\n",
      "Epoch 12 / 200 | iteration 140 / 171 | Total Loss: 3.1603493690490723 | KNN Loss: 3.10343074798584 | CLS Loss: 0.056918609887361526\n",
      "Epoch 12 / 200 | iteration 150 / 171 | Total Loss: 3.082084894180298 | KNN Loss: 3.0329065322875977 | CLS Loss: 0.04917844384908676\n",
      "Epoch 12 / 200 | iteration 160 / 171 | Total Loss: 3.141106605529785 | KNN Loss: 3.035201072692871 | CLS Loss: 0.10590553283691406\n",
      "Epoch 12 / 200 | iteration 170 / 171 | Total Loss: 3.1171908378601074 | KNN Loss: 3.0464954376220703 | CLS Loss: 0.07069528847932816\n",
      "Epoch: 012, Loss: 3.1183, Train: 0.9831, Valid: 0.9799, Best: 0.9799\n",
      "Epoch 13 / 200 | iteration 0 / 171 | Total Loss: 3.0742549896240234 | KNN Loss: 3.0417697429656982 | CLS Loss: 0.03248531371355057\n",
      "Epoch 13 / 200 | iteration 10 / 171 | Total Loss: 3.088460922241211 | KNN Loss: 3.0229673385620117 | CLS Loss: 0.06549353897571564\n",
      "Epoch 13 / 200 | iteration 20 / 171 | Total Loss: 3.104948043823242 | KNN Loss: 3.051727294921875 | CLS Loss: 0.05322067067027092\n",
      "Epoch 13 / 200 | iteration 30 / 171 | Total Loss: 3.0867040157318115 | KNN Loss: 3.0314536094665527 | CLS Loss: 0.05525040253996849\n",
      "Epoch 13 / 200 | iteration 40 / 171 | Total Loss: 3.1093361377716064 | KNN Loss: 3.0381853580474854 | CLS Loss: 0.07115072011947632\n",
      "Epoch 13 / 200 | iteration 50 / 171 | Total Loss: 3.1114327907562256 | KNN Loss: 3.0403733253479004 | CLS Loss: 0.071059450507164\n",
      "Epoch 13 / 200 | iteration 60 / 171 | Total Loss: 3.117841958999634 | KNN Loss: 3.047602891921997 | CLS Loss: 0.07023917883634567\n",
      "Epoch 13 / 200 | iteration 70 / 171 | Total Loss: 3.155806064605713 | KNN Loss: 3.054034948348999 | CLS Loss: 0.10177116096019745\n",
      "Epoch 13 / 200 | iteration 80 / 171 | Total Loss: 3.1357579231262207 | KNN Loss: 3.0338757038116455 | CLS Loss: 0.10188211500644684\n",
      "Epoch 13 / 200 | iteration 90 / 171 | Total Loss: 3.1235504150390625 | KNN Loss: 3.042097568511963 | CLS Loss: 0.08145282417535782\n",
      "Epoch 13 / 200 | iteration 100 / 171 | Total Loss: 3.0602381229400635 | KNN Loss: 3.003682851791382 | CLS Loss: 0.056555259972810745\n",
      "Epoch 13 / 200 | iteration 110 / 171 | Total Loss: 3.112257719039917 | KNN Loss: 3.050583839416504 | CLS Loss: 0.06167377531528473\n",
      "Epoch 13 / 200 | iteration 120 / 171 | Total Loss: 3.0950655937194824 | KNN Loss: 3.037109375 | CLS Loss: 0.05795624479651451\n",
      "Epoch 13 / 200 | iteration 130 / 171 | Total Loss: 3.114335775375366 | KNN Loss: 3.040440320968628 | CLS Loss: 0.07389551401138306\n",
      "Epoch 13 / 200 | iteration 140 / 171 | Total Loss: 3.118112802505493 | KNN Loss: 3.0491783618927 | CLS Loss: 0.0689343735575676\n",
      "Epoch 13 / 200 | iteration 150 / 171 | Total Loss: 3.1709632873535156 | KNN Loss: 3.0958261489868164 | CLS Loss: 0.07513715326786041\n",
      "Epoch 13 / 200 | iteration 160 / 171 | Total Loss: 3.136591672897339 | KNN Loss: 3.0489389896392822 | CLS Loss: 0.0876527950167656\n",
      "Epoch 13 / 200 | iteration 170 / 171 | Total Loss: 3.1129329204559326 | KNN Loss: 3.0487306118011475 | CLS Loss: 0.06420224159955978\n",
      "Epoch: 013, Loss: 3.1135, Train: 0.9805, Valid: 0.9773, Best: 0.9799\n",
      "Epoch 14 / 200 | iteration 0 / 171 | Total Loss: 3.132613182067871 | KNN Loss: 3.04512357711792 | CLS Loss: 0.08748968690633774\n",
      "Epoch 14 / 200 | iteration 10 / 171 | Total Loss: 3.083937883377075 | KNN Loss: 3.0247204303741455 | CLS Loss: 0.0592174306511879\n",
      "Epoch 14 / 200 | iteration 20 / 171 | Total Loss: 3.0929646492004395 | KNN Loss: 3.0332586765289307 | CLS Loss: 0.05970603972673416\n",
      "Epoch 14 / 200 | iteration 30 / 171 | Total Loss: 3.1052074432373047 | KNN Loss: 3.059986114501953 | CLS Loss: 0.04522143304347992\n",
      "Epoch 14 / 200 | iteration 40 / 171 | Total Loss: 3.105729818344116 | KNN Loss: 3.0555312633514404 | CLS Loss: 0.0501985140144825\n",
      "Epoch 14 / 200 | iteration 50 / 171 | Total Loss: 3.1179420948028564 | KNN Loss: 3.045725107192993 | CLS Loss: 0.07221708446741104\n",
      "Epoch 14 / 200 | iteration 60 / 171 | Total Loss: 3.1331920623779297 | KNN Loss: 3.0494184494018555 | CLS Loss: 0.08377359807491302\n",
      "Epoch 14 / 200 | iteration 70 / 171 | Total Loss: 3.1202263832092285 | KNN Loss: 3.045900344848633 | CLS Loss: 0.07432601600885391\n",
      "Epoch 14 / 200 | iteration 80 / 171 | Total Loss: 3.1142590045928955 | KNN Loss: 3.0413622856140137 | CLS Loss: 0.0728967934846878\n",
      "Epoch 14 / 200 | iteration 90 / 171 | Total Loss: 3.1048552989959717 | KNN Loss: 3.049426794052124 | CLS Loss: 0.055428396910429\n",
      "Epoch 14 / 200 | iteration 100 / 171 | Total Loss: 3.09596586227417 | KNN Loss: 3.0629467964172363 | CLS Loss: 0.03301899880170822\n",
      "Epoch 14 / 200 | iteration 110 / 171 | Total Loss: 3.133446455001831 | KNN Loss: 3.043687343597412 | CLS Loss: 0.08975905179977417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 / 200 | iteration 120 / 171 | Total Loss: 3.1225779056549072 | KNN Loss: 3.07989239692688 | CLS Loss: 0.042685553431510925\n",
      "Epoch 14 / 200 | iteration 130 / 171 | Total Loss: 3.1413917541503906 | KNN Loss: 3.039933443069458 | CLS Loss: 0.10145843029022217\n",
      "Epoch 14 / 200 | iteration 140 / 171 | Total Loss: 3.1185741424560547 | KNN Loss: 3.0359280109405518 | CLS Loss: 0.08264607936143875\n",
      "Epoch 14 / 200 | iteration 150 / 171 | Total Loss: 3.107323408126831 | KNN Loss: 3.051271677017212 | CLS Loss: 0.0560518354177475\n",
      "Epoch 14 / 200 | iteration 160 / 171 | Total Loss: 3.114885091781616 | KNN Loss: 3.051020383834839 | CLS Loss: 0.06386478990316391\n",
      "Epoch 14 / 200 | iteration 170 / 171 | Total Loss: 3.0773465633392334 | KNN Loss: 3.0175700187683105 | CLS Loss: 0.05977657064795494\n",
      "Epoch: 014, Loss: 3.1123, Train: 0.9830, Valid: 0.9798, Best: 0.9799\n",
      "Epoch 15 / 200 | iteration 0 / 171 | Total Loss: 3.1257026195526123 | KNN Loss: 3.0633950233459473 | CLS Loss: 0.06230752915143967\n",
      "Epoch 15 / 200 | iteration 10 / 171 | Total Loss: 3.0823309421539307 | KNN Loss: 3.021920919418335 | CLS Loss: 0.060410067439079285\n",
      "Epoch 15 / 200 | iteration 20 / 171 | Total Loss: 3.1209638118743896 | KNN Loss: 3.0471620559692383 | CLS Loss: 0.07380180805921555\n",
      "Epoch 15 / 200 | iteration 30 / 171 | Total Loss: 3.102450370788574 | KNN Loss: 2.9980385303497314 | CLS Loss: 0.10441182553768158\n",
      "Epoch 15 / 200 | iteration 40 / 171 | Total Loss: 3.064509153366089 | KNN Loss: 3.0216305255889893 | CLS Loss: 0.04287860542535782\n",
      "Epoch 15 / 200 | iteration 50 / 171 | Total Loss: 3.0815188884735107 | KNN Loss: 2.9937052726745605 | CLS Loss: 0.08781351894140244\n",
      "Epoch 15 / 200 | iteration 60 / 171 | Total Loss: 3.0870285034179688 | KNN Loss: 3.0556821823120117 | CLS Loss: 0.031346388161182404\n",
      "Epoch 15 / 200 | iteration 70 / 171 | Total Loss: 3.0933573246002197 | KNN Loss: 3.0191311836242676 | CLS Loss: 0.07422622293233871\n",
      "Epoch 15 / 200 | iteration 80 / 171 | Total Loss: 3.093587875366211 | KNN Loss: 3.0269010066986084 | CLS Loss: 0.06668683886528015\n",
      "Epoch 15 / 200 | iteration 90 / 171 | Total Loss: 3.1208364963531494 | KNN Loss: 3.0601491928100586 | CLS Loss: 0.06068737059831619\n",
      "Epoch 15 / 200 | iteration 100 / 171 | Total Loss: 3.071161985397339 | KNN Loss: 3.02683424949646 | CLS Loss: 0.04432765766978264\n",
      "Epoch 15 / 200 | iteration 110 / 171 | Total Loss: 3.0855448246002197 | KNN Loss: 3.0513248443603516 | CLS Loss: 0.03421987220644951\n",
      "Epoch 15 / 200 | iteration 120 / 171 | Total Loss: 3.0969583988189697 | KNN Loss: 3.048556089401245 | CLS Loss: 0.0484023317694664\n",
      "Epoch 15 / 200 | iteration 130 / 171 | Total Loss: 3.099491596221924 | KNN Loss: 3.042922019958496 | CLS Loss: 0.05656968429684639\n",
      "Epoch 15 / 200 | iteration 140 / 171 | Total Loss: 3.1331160068511963 | KNN Loss: 3.055481433868408 | CLS Loss: 0.07763466984033585\n",
      "Epoch 15 / 200 | iteration 150 / 171 | Total Loss: 3.115446090698242 | KNN Loss: 3.0513997077941895 | CLS Loss: 0.06404636800289154\n",
      "Epoch 15 / 200 | iteration 160 / 171 | Total Loss: 3.0857045650482178 | KNN Loss: 3.0206360816955566 | CLS Loss: 0.06506852060556412\n",
      "Epoch 15 / 200 | iteration 170 / 171 | Total Loss: 3.090787649154663 | KNN Loss: 3.0061748027801514 | CLS Loss: 0.0846128910779953\n",
      "Epoch: 015, Loss: 3.1043, Train: 0.9841, Valid: 0.9809, Best: 0.9809\n",
      "Epoch 16 / 200 | iteration 0 / 171 | Total Loss: 3.0725889205932617 | KNN Loss: 3.03361439704895 | CLS Loss: 0.038974639028310776\n",
      "Epoch 16 / 200 | iteration 10 / 171 | Total Loss: 3.094696283340454 | KNN Loss: 3.005892753601074 | CLS Loss: 0.08880341798067093\n",
      "Epoch 16 / 200 | iteration 20 / 171 | Total Loss: 3.1027495861053467 | KNN Loss: 3.053018093109131 | CLS Loss: 0.04973139241337776\n",
      "Epoch 16 / 200 | iteration 30 / 171 | Total Loss: 3.1251699924468994 | KNN Loss: 3.041717767715454 | CLS Loss: 0.0834522619843483\n",
      "Epoch 16 / 200 | iteration 40 / 171 | Total Loss: 3.0689191818237305 | KNN Loss: 3.0190072059631348 | CLS Loss: 0.04991201311349869\n",
      "Epoch 16 / 200 | iteration 50 / 171 | Total Loss: 3.0876307487487793 | KNN Loss: 3.037942409515381 | CLS Loss: 0.04968839883804321\n",
      "Epoch 16 / 200 | iteration 60 / 171 | Total Loss: 3.0954501628875732 | KNN Loss: 3.0453689098358154 | CLS Loss: 0.05008115991950035\n",
      "Epoch 16 / 200 | iteration 70 / 171 | Total Loss: 3.101632595062256 | KNN Loss: 3.0581142902374268 | CLS Loss: 0.043518416583538055\n",
      "Epoch 16 / 200 | iteration 80 / 171 | Total Loss: 3.10830020904541 | KNN Loss: 3.0227208137512207 | CLS Loss: 0.08557937294244766\n",
      "Epoch 16 / 200 | iteration 90 / 171 | Total Loss: 3.1165568828582764 | KNN Loss: 3.0335593223571777 | CLS Loss: 0.08299750089645386\n",
      "Epoch 16 / 200 | iteration 100 / 171 | Total Loss: 3.079144239425659 | KNN Loss: 3.047269821166992 | CLS Loss: 0.03187442198395729\n",
      "Epoch 16 / 200 | iteration 110 / 171 | Total Loss: 3.1029999256134033 | KNN Loss: 3.0233542919158936 | CLS Loss: 0.07964574545621872\n",
      "Epoch 16 / 200 | iteration 120 / 171 | Total Loss: 3.130284070968628 | KNN Loss: 3.060150623321533 | CLS Loss: 0.07013344019651413\n",
      "Epoch 16 / 200 | iteration 130 / 171 | Total Loss: 3.079928159713745 | KNN Loss: 3.026204824447632 | CLS Loss: 0.05372324958443642\n",
      "Epoch 16 / 200 | iteration 140 / 171 | Total Loss: 3.1088240146636963 | KNN Loss: 3.053858757019043 | CLS Loss: 0.05496537312865257\n",
      "Epoch 16 / 200 | iteration 150 / 171 | Total Loss: 3.079904794692993 | KNN Loss: 3.035275459289551 | CLS Loss: 0.04462926834821701\n",
      "Epoch 16 / 200 | iteration 160 / 171 | Total Loss: 3.115119457244873 | KNN Loss: 3.036316394805908 | CLS Loss: 0.07880299538373947\n",
      "Epoch 16 / 200 | iteration 170 / 171 | Total Loss: 3.071160078048706 | KNN Loss: 2.9955615997314453 | CLS Loss: 0.07559854537248611\n",
      "Epoch: 016, Loss: 3.1010, Train: 0.9843, Valid: 0.9802, Best: 0.9809\n",
      "Epoch 17 / 200 | iteration 0 / 171 | Total Loss: 3.056960105895996 | KNN Loss: 3.010796546936035 | CLS Loss: 0.046163495630025864\n",
      "Epoch 17 / 200 | iteration 10 / 171 | Total Loss: 3.072692394256592 | KNN Loss: 3.034799575805664 | CLS Loss: 0.03789282590150833\n",
      "Epoch 17 / 200 | iteration 20 / 171 | Total Loss: 3.086209535598755 | KNN Loss: 3.0049376487731934 | CLS Loss: 0.0812719389796257\n",
      "Epoch 17 / 200 | iteration 30 / 171 | Total Loss: 3.096813678741455 | KNN Loss: 3.041672468185425 | CLS Loss: 0.05514112487435341\n",
      "Epoch 17 / 200 | iteration 40 / 171 | Total Loss: 3.109044313430786 | KNN Loss: 3.040708541870117 | CLS Loss: 0.06833569705486298\n",
      "Epoch 17 / 200 | iteration 50 / 171 | Total Loss: 3.088404417037964 | KNN Loss: 3.0400707721710205 | CLS Loss: 0.04833358898758888\n",
      "Epoch 17 / 200 | iteration 60 / 171 | Total Loss: 3.094393491744995 | KNN Loss: 3.0758185386657715 | CLS Loss: 0.01857503317296505\n",
      "Epoch 17 / 200 | iteration 70 / 171 | Total Loss: 3.0949690341949463 | KNN Loss: 3.0186550617218018 | CLS Loss: 0.0763140618801117\n",
      "Epoch 17 / 200 | iteration 80 / 171 | Total Loss: 3.072953701019287 | KNN Loss: 2.9767467975616455 | CLS Loss: 0.09620692580938339\n",
      "Epoch 17 / 200 | iteration 90 / 171 | Total Loss: 3.1139957904815674 | KNN Loss: 3.0903689861297607 | CLS Loss: 0.023626741021871567\n",
      "Epoch 17 / 200 | iteration 100 / 171 | Total Loss: 3.110365390777588 | KNN Loss: 3.0349042415618896 | CLS Loss: 0.07546106725931168\n",
      "Epoch 17 / 200 | iteration 110 / 171 | Total Loss: 3.0991952419281006 | KNN Loss: 3.0526206493377686 | CLS Loss: 0.04657449945807457\n",
      "Epoch 17 / 200 | iteration 120 / 171 | Total Loss: 3.1686408519744873 | KNN Loss: 3.0604333877563477 | CLS Loss: 0.10820737481117249\n",
      "Epoch 17 / 200 | iteration 130 / 171 | Total Loss: 3.117891550064087 | KNN Loss: 3.0662970542907715 | CLS Loss: 0.05159449577331543\n",
      "Epoch 17 / 200 | iteration 140 / 171 | Total Loss: 3.150238513946533 | KNN Loss: 3.1038308143615723 | CLS Loss: 0.04640774056315422\n",
      "Epoch 17 / 200 | iteration 150 / 171 | Total Loss: 3.1035404205322266 | KNN Loss: 3.0306437015533447 | CLS Loss: 0.07289683073759079\n",
      "Epoch 17 / 200 | iteration 160 / 171 | Total Loss: 3.1047487258911133 | KNN Loss: 3.071932792663574 | CLS Loss: 0.03281581401824951\n",
      "Epoch 17 / 200 | iteration 170 / 171 | Total Loss: 3.1103038787841797 | KNN Loss: 3.0741360187530518 | CLS Loss: 0.03616775572299957\n",
      "Epoch: 017, Loss: 3.0979, Train: 0.9848, Valid: 0.9804, Best: 0.9809\n",
      "Epoch 18 / 200 | iteration 0 / 171 | Total Loss: 3.0677380561828613 | KNN Loss: 3.020122528076172 | CLS Loss: 0.047615472227334976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 / 200 | iteration 10 / 171 | Total Loss: 3.047639846801758 | KNN Loss: 3.0063743591308594 | CLS Loss: 0.041265495121479034\n",
      "Epoch 18 / 200 | iteration 20 / 171 | Total Loss: 3.1023342609405518 | KNN Loss: 3.0273938179016113 | CLS Loss: 0.07494048029184341\n",
      "Epoch 18 / 200 | iteration 30 / 171 | Total Loss: 3.078040361404419 | KNN Loss: 3.0472617149353027 | CLS Loss: 0.030778763815760612\n",
      "Epoch 18 / 200 | iteration 40 / 171 | Total Loss: 3.0831592082977295 | KNN Loss: 3.0112509727478027 | CLS Loss: 0.07190817594528198\n",
      "Epoch 18 / 200 | iteration 50 / 171 | Total Loss: 3.077197790145874 | KNN Loss: 3.03450870513916 | CLS Loss: 0.042689040303230286\n",
      "Epoch 18 / 200 | iteration 60 / 171 | Total Loss: 3.102269411087036 | KNN Loss: 3.064713954925537 | CLS Loss: 0.03755553066730499\n",
      "Epoch 18 / 200 | iteration 70 / 171 | Total Loss: 3.0684375762939453 | KNN Loss: 3.0333611965179443 | CLS Loss: 0.03507634997367859\n",
      "Epoch 18 / 200 | iteration 80 / 171 | Total Loss: 3.091987133026123 | KNN Loss: 3.017725944519043 | CLS Loss: 0.07426109164953232\n",
      "Epoch 18 / 200 | iteration 90 / 171 | Total Loss: 3.095184326171875 | KNN Loss: 3.033365249633789 | CLS Loss: 0.061819106340408325\n",
      "Epoch 18 / 200 | iteration 100 / 171 | Total Loss: 3.0691990852355957 | KNN Loss: 3.034970998764038 | CLS Loss: 0.03422797843813896\n",
      "Epoch 18 / 200 | iteration 110 / 171 | Total Loss: 3.1270909309387207 | KNN Loss: 3.0453224182128906 | CLS Loss: 0.08176839351654053\n",
      "Epoch 18 / 200 | iteration 120 / 171 | Total Loss: 3.1222946643829346 | KNN Loss: 3.031639337539673 | CLS Loss: 0.09065522253513336\n",
      "Epoch 18 / 200 | iteration 130 / 171 | Total Loss: 3.0767555236816406 | KNN Loss: 3.0186333656311035 | CLS Loss: 0.058122146874666214\n",
      "Epoch 18 / 200 | iteration 140 / 171 | Total Loss: 3.09616756439209 | KNN Loss: 3.031684160232544 | CLS Loss: 0.06448351591825485\n",
      "Epoch 18 / 200 | iteration 150 / 171 | Total Loss: 3.076066493988037 | KNN Loss: 3.0461196899414062 | CLS Loss: 0.029946882277727127\n",
      "Epoch 18 / 200 | iteration 160 / 171 | Total Loss: 3.0778141021728516 | KNN Loss: 3.0095067024230957 | CLS Loss: 0.06830751895904541\n",
      "Epoch 18 / 200 | iteration 170 / 171 | Total Loss: 3.1382253170013428 | KNN Loss: 3.0542855262756348 | CLS Loss: 0.08393985778093338\n",
      "Epoch: 018, Loss: 3.0942, Train: 0.9840, Valid: 0.9794, Best: 0.9809\n",
      "Epoch 19 / 200 | iteration 0 / 171 | Total Loss: 3.0869383811950684 | KNN Loss: 3.0017812252044678 | CLS Loss: 0.0851571336388588\n",
      "Epoch 19 / 200 | iteration 10 / 171 | Total Loss: 3.073115110397339 | KNN Loss: 3.0317940711975098 | CLS Loss: 0.04132106527686119\n",
      "Epoch 19 / 200 | iteration 20 / 171 | Total Loss: 3.0666797161102295 | KNN Loss: 3.036353349685669 | CLS Loss: 0.03032642975449562\n",
      "Epoch 19 / 200 | iteration 30 / 171 | Total Loss: 3.0847647190093994 | KNN Loss: 3.029197931289673 | CLS Loss: 0.055566880851984024\n",
      "Epoch 19 / 200 | iteration 40 / 171 | Total Loss: 3.0992367267608643 | KNN Loss: 3.0336921215057373 | CLS Loss: 0.06554464250802994\n",
      "Epoch 19 / 200 | iteration 50 / 171 | Total Loss: 3.1367108821868896 | KNN Loss: 3.052464485168457 | CLS Loss: 0.08424628525972366\n",
      "Epoch 19 / 200 | iteration 60 / 171 | Total Loss: 3.076465129852295 | KNN Loss: 3.030874013900757 | CLS Loss: 0.04559112712740898\n",
      "Epoch 19 / 200 | iteration 70 / 171 | Total Loss: 3.1277072429656982 | KNN Loss: 3.037870168685913 | CLS Loss: 0.08983711153268814\n",
      "Epoch 19 / 200 | iteration 80 / 171 | Total Loss: 3.059980630874634 | KNN Loss: 3.0211939811706543 | CLS Loss: 0.03878672420978546\n",
      "Epoch 19 / 200 | iteration 90 / 171 | Total Loss: 3.08622407913208 | KNN Loss: 3.0031402111053467 | CLS Loss: 0.08308390527963638\n",
      "Epoch 19 / 200 | iteration 100 / 171 | Total Loss: 3.0579705238342285 | KNN Loss: 3.003450870513916 | CLS Loss: 0.054519716650247574\n",
      "Epoch 19 / 200 | iteration 110 / 171 | Total Loss: 3.13568115234375 | KNN Loss: 3.0625157356262207 | CLS Loss: 0.07316534221172333\n",
      "Epoch 19 / 200 | iteration 120 / 171 | Total Loss: 3.120404005050659 | KNN Loss: 3.0442872047424316 | CLS Loss: 0.07611680775880814\n",
      "Epoch 19 / 200 | iteration 130 / 171 | Total Loss: 3.1284422874450684 | KNN Loss: 3.0827319622039795 | CLS Loss: 0.04571032524108887\n",
      "Epoch 19 / 200 | iteration 140 / 171 | Total Loss: 3.0565803050994873 | KNN Loss: 3.027597665786743 | CLS Loss: 0.028982754796743393\n",
      "Epoch 19 / 200 | iteration 150 / 171 | Total Loss: 3.0588197708129883 | KNN Loss: 2.997802495956421 | CLS Loss: 0.06101735308766365\n",
      "Epoch 19 / 200 | iteration 160 / 171 | Total Loss: 3.065462112426758 | KNN Loss: 3.0098626613616943 | CLS Loss: 0.05559942498803139\n",
      "Epoch 19 / 200 | iteration 170 / 171 | Total Loss: 3.103851795196533 | KNN Loss: 3.027167320251465 | CLS Loss: 0.07668440043926239\n",
      "Epoch: 019, Loss: 3.0876, Train: 0.9856, Valid: 0.9811, Best: 0.9811\n",
      "Epoch 20 / 200 | iteration 0 / 171 | Total Loss: 3.047030210494995 | KNN Loss: 3.009059190750122 | CLS Loss: 0.03797106072306633\n",
      "Epoch 20 / 200 | iteration 10 / 171 | Total Loss: 3.0957512855529785 | KNN Loss: 3.018632173538208 | CLS Loss: 0.0771191194653511\n",
      "Epoch 20 / 200 | iteration 20 / 171 | Total Loss: 3.088245153427124 | KNN Loss: 3.0167648792266846 | CLS Loss: 0.07148027420043945\n",
      "Epoch 20 / 200 | iteration 30 / 171 | Total Loss: 3.071553945541382 | KNN Loss: 3.039747953414917 | CLS Loss: 0.03180601820349693\n",
      "Epoch 20 / 200 | iteration 40 / 171 | Total Loss: 3.102457046508789 | KNN Loss: 3.0426902770996094 | CLS Loss: 0.05976670980453491\n",
      "Epoch 20 / 200 | iteration 50 / 171 | Total Loss: 3.099879741668701 | KNN Loss: 3.0503103733062744 | CLS Loss: 0.04956943541765213\n",
      "Epoch 20 / 200 | iteration 60 / 171 | Total Loss: 3.066065788269043 | KNN Loss: 3.02236270904541 | CLS Loss: 0.04370315745472908\n",
      "Epoch 20 / 200 | iteration 70 / 171 | Total Loss: 3.0873820781707764 | KNN Loss: 3.033273696899414 | CLS Loss: 0.05410835146903992\n",
      "Epoch 20 / 200 | iteration 80 / 171 | Total Loss: 3.106379508972168 | KNN Loss: 3.068922758102417 | CLS Loss: 0.03745675086975098\n",
      "Epoch 20 / 200 | iteration 90 / 171 | Total Loss: 3.0755536556243896 | KNN Loss: 3.0217978954315186 | CLS Loss: 0.0537557490170002\n",
      "Epoch 20 / 200 | iteration 100 / 171 | Total Loss: 3.1098334789276123 | KNN Loss: 3.0485079288482666 | CLS Loss: 0.061325546354055405\n",
      "Epoch 20 / 200 | iteration 110 / 171 | Total Loss: 3.032602310180664 | KNN Loss: 3.0070371627807617 | CLS Loss: 0.02556513622403145\n",
      "Epoch 20 / 200 | iteration 120 / 171 | Total Loss: 3.066035509109497 | KNN Loss: 3.0314786434173584 | CLS Loss: 0.03455696627497673\n",
      "Epoch 20 / 200 | iteration 130 / 171 | Total Loss: 3.102642774581909 | KNN Loss: 3.053212881088257 | CLS Loss: 0.04942994564771652\n",
      "Epoch 20 / 200 | iteration 140 / 171 | Total Loss: 3.0958120822906494 | KNN Loss: 3.039858102798462 | CLS Loss: 0.05595399811863899\n",
      "Epoch 20 / 200 | iteration 150 / 171 | Total Loss: 3.0777435302734375 | KNN Loss: 3.022198438644409 | CLS Loss: 0.05554509162902832\n",
      "Epoch 20 / 200 | iteration 160 / 171 | Total Loss: 3.0936992168426514 | KNN Loss: 3.021920919418335 | CLS Loss: 0.07177827507257462\n",
      "Epoch 20 / 200 | iteration 170 / 171 | Total Loss: 3.073305130004883 | KNN Loss: 3.0039126873016357 | CLS Loss: 0.06939256191253662\n",
      "Epoch: 020, Loss: 3.0853, Train: 0.9866, Valid: 0.9820, Best: 0.9820\n",
      "Epoch 21 / 200 | iteration 0 / 171 | Total Loss: 3.0514111518859863 | KNN Loss: 3.001554250717163 | CLS Loss: 0.04985693842172623\n",
      "Epoch 21 / 200 | iteration 10 / 171 | Total Loss: 3.0402379035949707 | KNN Loss: 3.0038504600524902 | CLS Loss: 0.03638744726777077\n",
      "Epoch 21 / 200 | iteration 20 / 171 | Total Loss: 3.063084602355957 | KNN Loss: 3.0193278789520264 | CLS Loss: 0.043756745755672455\n",
      "Epoch 21 / 200 | iteration 30 / 171 | Total Loss: 3.1077282428741455 | KNN Loss: 3.0593864917755127 | CLS Loss: 0.048341769725084305\n",
      "Epoch 21 / 200 | iteration 40 / 171 | Total Loss: 3.1167049407958984 | KNN Loss: 3.043930768966675 | CLS Loss: 0.07277411967515945\n",
      "Epoch 21 / 200 | iteration 50 / 171 | Total Loss: 3.074976682662964 | KNN Loss: 3.0302810668945312 | CLS Loss: 0.04469561576843262\n",
      "Epoch 21 / 200 | iteration 60 / 171 | Total Loss: 3.0590474605560303 | KNN Loss: 3.0397868156433105 | CLS Loss: 0.019260665401816368\n",
      "Epoch 21 / 200 | iteration 70 / 171 | Total Loss: 3.0651190280914307 | KNN Loss: 3.019902467727661 | CLS Loss: 0.04521644860506058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 200 | iteration 80 / 171 | Total Loss: 3.064152956008911 | KNN Loss: 3.033048391342163 | CLS Loss: 0.031104493886232376\n",
      "Epoch 21 / 200 | iteration 90 / 171 | Total Loss: 3.0890579223632812 | KNN Loss: 3.062134027481079 | CLS Loss: 0.026923948898911476\n",
      "Epoch 21 / 200 | iteration 100 / 171 | Total Loss: 3.102947950363159 | KNN Loss: 3.0143537521362305 | CLS Loss: 0.08859430253505707\n",
      "Epoch 21 / 200 | iteration 110 / 171 | Total Loss: 3.084491491317749 | KNN Loss: 3.0092668533325195 | CLS Loss: 0.07522459328174591\n",
      "Epoch 21 / 200 | iteration 120 / 171 | Total Loss: 3.1031434535980225 | KNN Loss: 3.0316169261932373 | CLS Loss: 0.0715264230966568\n",
      "Epoch 21 / 200 | iteration 130 / 171 | Total Loss: 3.0821657180786133 | KNN Loss: 3.0285468101501465 | CLS Loss: 0.05361894890666008\n",
      "Epoch 21 / 200 | iteration 140 / 171 | Total Loss: 3.102811813354492 | KNN Loss: 3.0358939170837402 | CLS Loss: 0.06691781431436539\n",
      "Epoch 21 / 200 | iteration 150 / 171 | Total Loss: 3.092658281326294 | KNN Loss: 3.0381996631622314 | CLS Loss: 0.054458506405353546\n",
      "Epoch 21 / 200 | iteration 160 / 171 | Total Loss: 3.1111502647399902 | KNN Loss: 3.052584648132324 | CLS Loss: 0.058565713465213776\n",
      "Epoch 21 / 200 | iteration 170 / 171 | Total Loss: 3.0908727645874023 | KNN Loss: 2.988825559616089 | CLS Loss: 0.10204722732305527\n",
      "Epoch: 021, Loss: 3.0873, Train: 0.9860, Valid: 0.9815, Best: 0.9820\n",
      "Epoch 22 / 200 | iteration 0 / 171 | Total Loss: 3.051548957824707 | KNN Loss: 3.0119235515594482 | CLS Loss: 0.03962548077106476\n",
      "Epoch 22 / 200 | iteration 10 / 171 | Total Loss: 3.0793442726135254 | KNN Loss: 3.038586139678955 | CLS Loss: 0.04075812175869942\n",
      "Epoch 22 / 200 | iteration 20 / 171 | Total Loss: 3.076042652130127 | KNN Loss: 3.0448193550109863 | CLS Loss: 0.031223371624946594\n",
      "Epoch 22 / 200 | iteration 30 / 171 | Total Loss: 3.145764112472534 | KNN Loss: 3.049241781234741 | CLS Loss: 0.09652230888605118\n",
      "Epoch 22 / 200 | iteration 40 / 171 | Total Loss: 3.111617088317871 | KNN Loss: 3.050316095352173 | CLS Loss: 0.06130094453692436\n",
      "Epoch 22 / 200 | iteration 50 / 171 | Total Loss: 3.0741631984710693 | KNN Loss: 3.038013458251953 | CLS Loss: 0.03614979237318039\n",
      "Epoch 22 / 200 | iteration 60 / 171 | Total Loss: 3.0540406703948975 | KNN Loss: 3.0230512619018555 | CLS Loss: 0.03098949044942856\n",
      "Epoch 22 / 200 | iteration 70 / 171 | Total Loss: 3.063913106918335 | KNN Loss: 3.025777816772461 | CLS Loss: 0.038135185837745667\n",
      "Epoch 22 / 200 | iteration 80 / 171 | Total Loss: 3.105684518814087 | KNN Loss: 3.045308828353882 | CLS Loss: 0.060375723987817764\n",
      "Epoch 22 / 200 | iteration 90 / 171 | Total Loss: 3.1092140674591064 | KNN Loss: 3.0773825645446777 | CLS Loss: 0.031831566244363785\n",
      "Epoch 22 / 200 | iteration 100 / 171 | Total Loss: 3.078490972518921 | KNN Loss: 3.000840187072754 | CLS Loss: 0.07765083014965057\n",
      "Epoch 22 / 200 | iteration 110 / 171 | Total Loss: 3.0655136108398438 | KNN Loss: 3.0171735286712646 | CLS Loss: 0.04833997040987015\n",
      "Epoch 22 / 200 | iteration 120 / 171 | Total Loss: 3.0830347537994385 | KNN Loss: 3.0562033653259277 | CLS Loss: 0.026831377297639847\n",
      "Epoch 22 / 200 | iteration 130 / 171 | Total Loss: 3.0787582397460938 | KNN Loss: 3.014840841293335 | CLS Loss: 0.06391751766204834\n",
      "Epoch 22 / 200 | iteration 140 / 171 | Total Loss: 3.0685033798217773 | KNN Loss: 3.0167174339294434 | CLS Loss: 0.05178588628768921\n",
      "Epoch 22 / 200 | iteration 150 / 171 | Total Loss: 3.0925228595733643 | KNN Loss: 3.0236856937408447 | CLS Loss: 0.06883715838193893\n",
      "Epoch 22 / 200 | iteration 160 / 171 | Total Loss: 3.06455659866333 | KNN Loss: 3.014092445373535 | CLS Loss: 0.05046410858631134\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc = 0\n",
    "losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train(model, train_data_iter, optimizer, device)\n",
    "#     print(f\"Loss: {loss} =============================\")\n",
    "    losses.append(loss)\n",
    "    train_acc = test(model, train_data_iter, device)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_acc = test(model, test_data_iter, device)\n",
    "    val_accs.append(valid_acc)\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Train: {train_acc:.4f}, Valid: {valid_acc:.4f}, '\n",
    "              f'Best: {best_valid_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_data_iter, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses, label='train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_accs, label='train accuracy')\n",
    "plt.plot(val_accs, label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = torch.tensor([])\n",
    "projections = torch.tensor([])\n",
    "labels = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(test_data_iter):\n",
    "        test_samples = torch.cat([test_samples, x])\n",
    "        labels = torch.cat([labels, y])\n",
    "        x = x.to(device)\n",
    "        _, interm = model(x, True)\n",
    "        projections = torch.cat([projections, interm.detach().cpu().flatten(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=2, min_samples=10).fit_predict(projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of inliers: {sum(clusters != -1) / len(clusters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = 100\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(test_samples.flatten(1)[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 400\n",
    "log_interval = 100\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=test_samples.shape[2], output_dim=len(set(clusters)) - 1, depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "# plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names = [f\"T_{i}\" for i in range(test_samples.shape[2])]\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
