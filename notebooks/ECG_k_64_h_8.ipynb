{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.mit_bih import MITBIH\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss, ClassificationKNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 64\n",
    "tree_depth = 8\n",
    "batch_size = 512\n",
    "device = 'cuda'\n",
    "train_data_path = r'<>/mitbih_train.csv'  # replace <> with the correct path of the dataset\n",
    "test_data_path = r'<>/mitbih_test.csv'  # replace <> with the correct path of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_iter = torch.utils.data.DataLoader(MITBIH(train_data_path),\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=1,\n",
    "                                             pin_memory=True,\n",
    "                                             drop_last=True)\n",
    "\n",
    "test_data_iter = torch.utils.data.DataLoader(MITBIH(test_data_path),\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=1,\n",
    "                                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(32, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv1d(32, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=5, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.conv1(y)\n",
    "        y = self.relu1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = y + x\n",
    "        y = self.relu2(y)\n",
    "        y = self.pool(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class ECGModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECGModel, self).__init__()\n",
    "        self.conv = nn.Conv1d(1, 32, kernel_size=5, stride=1, padding=1)\n",
    "        self.block1 = ConvBlock()\n",
    "        self.block2 = ConvBlock()\n",
    "        self.block3 = ConvBlock()\n",
    "        self.block4 = ConvBlock()\n",
    "        self.block5 = ConvBlock()\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 5)\n",
    "\n",
    "    def forward(self, x, return_interm=False):\n",
    "        x = self.conv(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        interm = x.flatten(1)\n",
    "        x = self.fc1(interm)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        if return_interm:\n",
    "            return x, interm\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_crt = ClassificationKNNLoss(k=k).to(device)\n",
    "\n",
    "def train(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, interm = model(batch, return_interm=True)\n",
    "        mse_loss = F.cross_entropy(outputs, target)\n",
    "        mse_loss = mse_loss.sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(interm, target)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = torch.tensor(0).to(device)\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0).to(device)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(loader)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | CLS Loss: {mse_loss.item()}\")\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    for iteration, (batch, target) in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        y_pred = model(batch).argmax(dim=-1)\n",
    "        correct += y_pred.eq(target.view(-1).data).sum()\n",
    "    \n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params: 53957\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "lr = 1e-3\n",
    "log_every = 10\n",
    "\n",
    "model = ECGModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "print(f'#Params: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 200 | iteration 0 / 171 | Total Loss: 7.37387752532959 | KNN Loss: 5.745442867279053 | CLS Loss: 1.6284348964691162\n",
      "Epoch 1 / 200 | iteration 10 / 171 | Total Loss: 5.379044532775879 | KNN Loss: 4.717100143432617 | CLS Loss: 0.6619442105293274\n",
      "Epoch 1 / 200 | iteration 20 / 171 | Total Loss: 5.141284465789795 | KNN Loss: 4.541954517364502 | CLS Loss: 0.5993297696113586\n",
      "Epoch 1 / 200 | iteration 30 / 171 | Total Loss: 5.106218338012695 | KNN Loss: 4.494356632232666 | CLS Loss: 0.6118617057800293\n",
      "Epoch 1 / 200 | iteration 40 / 171 | Total Loss: 5.020920753479004 | KNN Loss: 4.459837436676025 | CLS Loss: 0.5610834360122681\n",
      "Epoch 1 / 200 | iteration 50 / 171 | Total Loss: 4.974459171295166 | KNN Loss: 4.389924049377441 | CLS Loss: 0.5845351815223694\n",
      "Epoch 1 / 200 | iteration 60 / 171 | Total Loss: 4.929912567138672 | KNN Loss: 4.4062910079956055 | CLS Loss: 0.5236214995384216\n",
      "Epoch 1 / 200 | iteration 70 / 171 | Total Loss: 4.792877674102783 | KNN Loss: 4.397512912750244 | CLS Loss: 0.3953648805618286\n",
      "Epoch 1 / 200 | iteration 80 / 171 | Total Loss: 4.8632402420043945 | KNN Loss: 4.393400192260742 | CLS Loss: 0.46984025835990906\n",
      "Epoch 1 / 200 | iteration 90 / 171 | Total Loss: 4.91480827331543 | KNN Loss: 4.4272003173828125 | CLS Loss: 0.4876081347465515\n",
      "Epoch 1 / 200 | iteration 100 / 171 | Total Loss: 4.775811672210693 | KNN Loss: 4.403521537780762 | CLS Loss: 0.3722899258136749\n",
      "Epoch 1 / 200 | iteration 110 / 171 | Total Loss: 4.777783393859863 | KNN Loss: 4.364669322967529 | CLS Loss: 0.4131143093109131\n",
      "Epoch 1 / 200 | iteration 120 / 171 | Total Loss: 4.715494632720947 | KNN Loss: 4.341352939605713 | CLS Loss: 0.37414172291755676\n",
      "Epoch 1 / 200 | iteration 130 / 171 | Total Loss: 4.702499866485596 | KNN Loss: 4.334423065185547 | CLS Loss: 0.3680765926837921\n",
      "Epoch 1 / 200 | iteration 140 / 171 | Total Loss: 4.699441432952881 | KNN Loss: 4.353951930999756 | CLS Loss: 0.34548965096473694\n",
      "Epoch 1 / 200 | iteration 150 / 171 | Total Loss: 4.632406711578369 | KNN Loss: 4.328537464141846 | CLS Loss: 0.30386921763420105\n",
      "Epoch 1 / 200 | iteration 160 / 171 | Total Loss: 4.637119770050049 | KNN Loss: 4.348038196563721 | CLS Loss: 0.2890816032886505\n",
      "Epoch 1 / 200 | iteration 170 / 171 | Total Loss: 4.6174492835998535 | KNN Loss: 4.326106071472168 | CLS Loss: 0.2913433611392975\n",
      "Epoch: 001, Loss: 4.9387, Train: 0.9229, Valid: 0.9228, Best: 0.9228\n",
      "Epoch 2 / 200 | iteration 0 / 171 | Total Loss: 4.615243911743164 | KNN Loss: 4.299889087677002 | CLS Loss: 0.3153548240661621\n",
      "Epoch 2 / 200 | iteration 10 / 171 | Total Loss: 4.595103740692139 | KNN Loss: 4.315732479095459 | CLS Loss: 0.279371052980423\n",
      "Epoch 2 / 200 | iteration 20 / 171 | Total Loss: 4.572463035583496 | KNN Loss: 4.331151962280273 | CLS Loss: 0.24131113290786743\n",
      "Epoch 2 / 200 | iteration 30 / 171 | Total Loss: 4.620065689086914 | KNN Loss: 4.310661792755127 | CLS Loss: 0.3094037175178528\n",
      "Epoch 2 / 200 | iteration 40 / 171 | Total Loss: 4.584877014160156 | KNN Loss: 4.339844703674316 | CLS Loss: 0.2450321465730667\n",
      "Epoch 2 / 200 | iteration 50 / 171 | Total Loss: 4.6072587966918945 | KNN Loss: 4.344444751739502 | CLS Loss: 0.2628142237663269\n",
      "Epoch 2 / 200 | iteration 60 / 171 | Total Loss: 4.570510387420654 | KNN Loss: 4.317564964294434 | CLS Loss: 0.25294554233551025\n",
      "Epoch 2 / 200 | iteration 70 / 171 | Total Loss: 4.5804762840271 | KNN Loss: 4.316233158111572 | CLS Loss: 0.26424333453178406\n",
      "Epoch 2 / 200 | iteration 80 / 171 | Total Loss: 4.526775360107422 | KNN Loss: 4.31889533996582 | CLS Loss: 0.2078799605369568\n",
      "Epoch 2 / 200 | iteration 90 / 171 | Total Loss: 4.560619354248047 | KNN Loss: 4.342252731323242 | CLS Loss: 0.21836665272712708\n",
      "Epoch 2 / 200 | iteration 100 / 171 | Total Loss: 4.5824079513549805 | KNN Loss: 4.340768814086914 | CLS Loss: 0.24163903295993805\n",
      "Epoch 2 / 200 | iteration 110 / 171 | Total Loss: 4.521810531616211 | KNN Loss: 4.285041809082031 | CLS Loss: 0.23676879703998566\n",
      "Epoch 2 / 200 | iteration 120 / 171 | Total Loss: 4.569712162017822 | KNN Loss: 4.342443943023682 | CLS Loss: 0.2272680252790451\n",
      "Epoch 2 / 200 | iteration 130 / 171 | Total Loss: 4.479330539703369 | KNN Loss: 4.3276824951171875 | CLS Loss: 0.15164810419082642\n",
      "Epoch 2 / 200 | iteration 140 / 171 | Total Loss: 4.47987174987793 | KNN Loss: 4.278876304626465 | CLS Loss: 0.2009952962398529\n",
      "Epoch 2 / 200 | iteration 150 / 171 | Total Loss: 4.492470741271973 | KNN Loss: 4.295865535736084 | CLS Loss: 0.19660519063472748\n",
      "Epoch 2 / 200 | iteration 160 / 171 | Total Loss: 4.493589401245117 | KNN Loss: 4.270259857177734 | CLS Loss: 0.2233293503522873\n",
      "Epoch 2 / 200 | iteration 170 / 171 | Total Loss: 4.477772235870361 | KNN Loss: 4.301375389099121 | CLS Loss: 0.17639683187007904\n",
      "Epoch: 002, Loss: 4.5559, Train: 0.9609, Valid: 0.9588, Best: 0.9588\n",
      "Epoch 3 / 200 | iteration 0 / 171 | Total Loss: 4.431494235992432 | KNN Loss: 4.294895172119141 | CLS Loss: 0.1365988403558731\n",
      "Epoch 3 / 200 | iteration 10 / 171 | Total Loss: 4.440007209777832 | KNN Loss: 4.28933572769165 | CLS Loss: 0.15067169070243835\n",
      "Epoch 3 / 200 | iteration 20 / 171 | Total Loss: 4.479746341705322 | KNN Loss: 4.330780982971191 | CLS Loss: 0.1489655077457428\n",
      "Epoch 3 / 200 | iteration 30 / 171 | Total Loss: 4.465555191040039 | KNN Loss: 4.321432113647461 | CLS Loss: 0.14412300288677216\n",
      "Epoch 3 / 200 | iteration 40 / 171 | Total Loss: 4.4418416023254395 | KNN Loss: 4.333532333374023 | CLS Loss: 0.10830903798341751\n",
      "Epoch 3 / 200 | iteration 50 / 171 | Total Loss: 4.507842063903809 | KNN Loss: 4.3184638023376465 | CLS Loss: 0.1893784999847412\n",
      "Epoch 3 / 200 | iteration 60 / 171 | Total Loss: 4.439078330993652 | KNN Loss: 4.290783882141113 | CLS Loss: 0.148294597864151\n",
      "Epoch 3 / 200 | iteration 70 / 171 | Total Loss: 4.493706703186035 | KNN Loss: 4.296844005584717 | CLS Loss: 0.19686292111873627\n",
      "Epoch 3 / 200 | iteration 80 / 171 | Total Loss: 4.536137104034424 | KNN Loss: 4.323380470275879 | CLS Loss: 0.21275654435157776\n",
      "Epoch 3 / 200 | iteration 90 / 171 | Total Loss: 4.43812894821167 | KNN Loss: 4.274933338165283 | CLS Loss: 0.16319580376148224\n",
      "Epoch 3 / 200 | iteration 100 / 171 | Total Loss: 4.4826507568359375 | KNN Loss: 4.335111141204834 | CLS Loss: 0.1475396454334259\n",
      "Epoch 3 / 200 | iteration 110 / 171 | Total Loss: 4.468921661376953 | KNN Loss: 4.327493190765381 | CLS Loss: 0.14142833650112152\n",
      "Epoch 3 / 200 | iteration 120 / 171 | Total Loss: 4.431960105895996 | KNN Loss: 4.308795928955078 | CLS Loss: 0.12316427379846573\n",
      "Epoch 3 / 200 | iteration 130 / 171 | Total Loss: 4.511761665344238 | KNN Loss: 4.317503929138184 | CLS Loss: 0.19425784051418304\n",
      "Epoch 3 / 200 | iteration 140 / 171 | Total Loss: 4.508493423461914 | KNN Loss: 4.327471733093262 | CLS Loss: 0.18102145195007324\n",
      "Epoch 3 / 200 | iteration 150 / 171 | Total Loss: 4.4030303955078125 | KNN Loss: 4.2840189933776855 | CLS Loss: 0.1190112978219986\n",
      "Epoch 3 / 200 | iteration 160 / 171 | Total Loss: 4.476074695587158 | KNN Loss: 4.306878089904785 | CLS Loss: 0.16919653117656708\n",
      "Epoch 3 / 200 | iteration 170 / 171 | Total Loss: 4.381821632385254 | KNN Loss: 4.252152919769287 | CLS Loss: 0.1296689212322235\n",
      "Epoch: 003, Loss: 4.4541, Train: 0.9674, Valid: 0.9656, Best: 0.9656\n",
      "Epoch 4 / 200 | iteration 0 / 171 | Total Loss: 4.394035816192627 | KNN Loss: 4.290520668029785 | CLS Loss: 0.10351517051458359\n",
      "Epoch 4 / 200 | iteration 10 / 171 | Total Loss: 4.412328243255615 | KNN Loss: 4.303665637969971 | CLS Loss: 0.1086626797914505\n",
      "Epoch 4 / 200 | iteration 20 / 171 | Total Loss: 4.42047119140625 | KNN Loss: 4.309313774108887 | CLS Loss: 0.11115732789039612\n",
      "Epoch 4 / 200 | iteration 30 / 171 | Total Loss: 4.401581287384033 | KNN Loss: 4.294761657714844 | CLS Loss: 0.10681973397731781\n",
      "Epoch 4 / 200 | iteration 40 / 171 | Total Loss: 4.45839262008667 | KNN Loss: 4.309311389923096 | CLS Loss: 0.14908142387866974\n",
      "Epoch 4 / 200 | iteration 50 / 171 | Total Loss: 4.380283832550049 | KNN Loss: 4.263631820678711 | CLS Loss: 0.11665219068527222\n",
      "Epoch 4 / 200 | iteration 60 / 171 | Total Loss: 4.385569095611572 | KNN Loss: 4.248692035675049 | CLS Loss: 0.13687725365161896\n",
      "Epoch 4 / 200 | iteration 70 / 171 | Total Loss: 4.394540786743164 | KNN Loss: 4.276994705200195 | CLS Loss: 0.11754591763019562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 / 200 | iteration 80 / 171 | Total Loss: 4.4375739097595215 | KNN Loss: 4.3133392333984375 | CLS Loss: 0.12423483282327652\n",
      "Epoch 4 / 200 | iteration 90 / 171 | Total Loss: 4.4066643714904785 | KNN Loss: 4.28235387802124 | CLS Loss: 0.12431031465530396\n",
      "Epoch 4 / 200 | iteration 100 / 171 | Total Loss: 4.430000305175781 | KNN Loss: 4.27595853805542 | CLS Loss: 0.15404152870178223\n",
      "Epoch 4 / 200 | iteration 110 / 171 | Total Loss: 4.413160800933838 | KNN Loss: 4.305107116699219 | CLS Loss: 0.10805387049913406\n",
      "Epoch 4 / 200 | iteration 120 / 171 | Total Loss: 4.368527412414551 | KNN Loss: 4.257289409637451 | CLS Loss: 0.11123808473348618\n",
      "Epoch 4 / 200 | iteration 130 / 171 | Total Loss: 4.407856464385986 | KNN Loss: 4.308313369750977 | CLS Loss: 0.09954296797513962\n",
      "Epoch 4 / 200 | iteration 140 / 171 | Total Loss: 4.343050956726074 | KNN Loss: 4.2276611328125 | CLS Loss: 0.11538973450660706\n",
      "Epoch 4 / 200 | iteration 150 / 171 | Total Loss: 4.367811679840088 | KNN Loss: 4.225940227508545 | CLS Loss: 0.1418714076280594\n",
      "Epoch 4 / 200 | iteration 160 / 171 | Total Loss: 4.469719886779785 | KNN Loss: 4.312989711761475 | CLS Loss: 0.15673020482063293\n",
      "Epoch 4 / 200 | iteration 170 / 171 | Total Loss: 4.35101318359375 | KNN Loss: 4.272481441497803 | CLS Loss: 0.07853177934885025\n",
      "Epoch: 004, Loss: 4.4071, Train: 0.9678, Valid: 0.9652, Best: 0.9656\n",
      "Epoch 5 / 200 | iteration 0 / 171 | Total Loss: 4.350957870483398 | KNN Loss: 4.26069450378418 | CLS Loss: 0.0902634784579277\n",
      "Epoch 5 / 200 | iteration 10 / 171 | Total Loss: 4.344718933105469 | KNN Loss: 4.245005130767822 | CLS Loss: 0.09971366077661514\n",
      "Epoch 5 / 200 | iteration 20 / 171 | Total Loss: 4.372159481048584 | KNN Loss: 4.266826152801514 | CLS Loss: 0.10533318668603897\n",
      "Epoch 5 / 200 | iteration 30 / 171 | Total Loss: 4.384274959564209 | KNN Loss: 4.257660865783691 | CLS Loss: 0.126614049077034\n",
      "Epoch 5 / 200 | iteration 40 / 171 | Total Loss: 4.423039436340332 | KNN Loss: 4.260661602020264 | CLS Loss: 0.16237789392471313\n",
      "Epoch 5 / 200 | iteration 50 / 171 | Total Loss: 4.365090847015381 | KNN Loss: 4.233788013458252 | CLS Loss: 0.1313028335571289\n",
      "Epoch 5 / 200 | iteration 60 / 171 | Total Loss: 4.387551307678223 | KNN Loss: 4.271963119506836 | CLS Loss: 0.11558811366558075\n",
      "Epoch 5 / 200 | iteration 70 / 171 | Total Loss: 4.341432571411133 | KNN Loss: 4.275397300720215 | CLS Loss: 0.06603547930717468\n",
      "Epoch 5 / 200 | iteration 80 / 171 | Total Loss: 4.324753284454346 | KNN Loss: 4.238295078277588 | CLS Loss: 0.08645813167095184\n",
      "Epoch 5 / 200 | iteration 90 / 171 | Total Loss: 4.403773307800293 | KNN Loss: 4.271701812744141 | CLS Loss: 0.13207165896892548\n",
      "Epoch 5 / 200 | iteration 100 / 171 | Total Loss: 4.364169120788574 | KNN Loss: 4.274514198303223 | CLS Loss: 0.08965478837490082\n",
      "Epoch 5 / 200 | iteration 110 / 171 | Total Loss: 4.4104108810424805 | KNN Loss: 4.280707836151123 | CLS Loss: 0.1297028362751007\n",
      "Epoch 5 / 200 | iteration 120 / 171 | Total Loss: 4.38735818862915 | KNN Loss: 4.280214309692383 | CLS Loss: 0.10714402049779892\n",
      "Epoch 5 / 200 | iteration 130 / 171 | Total Loss: 4.352135181427002 | KNN Loss: 4.24864387512207 | CLS Loss: 0.10349135100841522\n",
      "Epoch 5 / 200 | iteration 140 / 171 | Total Loss: 4.374885082244873 | KNN Loss: 4.266843795776367 | CLS Loss: 0.1080411896109581\n",
      "Epoch 5 / 200 | iteration 150 / 171 | Total Loss: 4.351239204406738 | KNN Loss: 4.260447978973389 | CLS Loss: 0.09079145640134811\n",
      "Epoch 5 / 200 | iteration 160 / 171 | Total Loss: 4.408137321472168 | KNN Loss: 4.272711277008057 | CLS Loss: 0.13542625308036804\n",
      "Epoch 5 / 200 | iteration 170 / 171 | Total Loss: 4.40713357925415 | KNN Loss: 4.319931983947754 | CLS Loss: 0.08720140159130096\n",
      "Epoch: 005, Loss: 4.3800, Train: 0.9765, Valid: 0.9734, Best: 0.9734\n",
      "Epoch 6 / 200 | iteration 0 / 171 | Total Loss: 4.38011360168457 | KNN Loss: 4.289751052856445 | CLS Loss: 0.09036243706941605\n",
      "Epoch 6 / 200 | iteration 10 / 171 | Total Loss: 4.372650146484375 | KNN Loss: 4.285350799560547 | CLS Loss: 0.08729948103427887\n",
      "Epoch 6 / 200 | iteration 20 / 171 | Total Loss: 4.341233730316162 | KNN Loss: 4.246156215667725 | CLS Loss: 0.09507741034030914\n",
      "Epoch 6 / 200 | iteration 30 / 171 | Total Loss: 4.357691287994385 | KNN Loss: 4.267147064208984 | CLS Loss: 0.09054405987262726\n",
      "Epoch 6 / 200 | iteration 40 / 171 | Total Loss: 4.431992053985596 | KNN Loss: 4.290605545043945 | CLS Loss: 0.14138631522655487\n",
      "Epoch 6 / 200 | iteration 50 / 171 | Total Loss: 4.324009418487549 | KNN Loss: 4.25486946105957 | CLS Loss: 0.06913994252681732\n",
      "Epoch 6 / 200 | iteration 60 / 171 | Total Loss: 4.400087356567383 | KNN Loss: 4.274851322174072 | CLS Loss: 0.1252361536026001\n",
      "Epoch 6 / 200 | iteration 70 / 171 | Total Loss: 4.332690238952637 | KNN Loss: 4.258789539337158 | CLS Loss: 0.0739007368683815\n",
      "Epoch 6 / 200 | iteration 80 / 171 | Total Loss: 4.373558044433594 | KNN Loss: 4.261382579803467 | CLS Loss: 0.11217538267374039\n",
      "Epoch 6 / 200 | iteration 90 / 171 | Total Loss: 4.353360652923584 | KNN Loss: 4.258780479431152 | CLS Loss: 0.09457997232675552\n",
      "Epoch 6 / 200 | iteration 100 / 171 | Total Loss: 4.362265110015869 | KNN Loss: 4.271859645843506 | CLS Loss: 0.09040532261133194\n",
      "Epoch 6 / 200 | iteration 110 / 171 | Total Loss: 4.4235429763793945 | KNN Loss: 4.326760292053223 | CLS Loss: 0.0967826247215271\n",
      "Epoch 6 / 200 | iteration 120 / 171 | Total Loss: 4.310363292694092 | KNN Loss: 4.23378849029541 | CLS Loss: 0.07657469809055328\n",
      "Epoch 6 / 200 | iteration 130 / 171 | Total Loss: 4.3261919021606445 | KNN Loss: 4.2628655433654785 | CLS Loss: 0.06332644075155258\n",
      "Epoch 6 / 200 | iteration 140 / 171 | Total Loss: 4.355535507202148 | KNN Loss: 4.255228519439697 | CLS Loss: 0.10030709952116013\n",
      "Epoch 6 / 200 | iteration 150 / 171 | Total Loss: 4.3945512771606445 | KNN Loss: 4.275550365447998 | CLS Loss: 0.11900106072425842\n",
      "Epoch 6 / 200 | iteration 160 / 171 | Total Loss: 4.3073506355285645 | KNN Loss: 4.256847381591797 | CLS Loss: 0.05050347372889519\n",
      "Epoch 6 / 200 | iteration 170 / 171 | Total Loss: 4.382436752319336 | KNN Loss: 4.267224311828613 | CLS Loss: 0.11521244794130325\n",
      "Epoch: 006, Loss: 4.3588, Train: 0.9771, Valid: 0.9740, Best: 0.9740\n",
      "Epoch 7 / 200 | iteration 0 / 171 | Total Loss: 4.406524181365967 | KNN Loss: 4.310909271240234 | CLS Loss: 0.09561491012573242\n",
      "Epoch 7 / 200 | iteration 10 / 171 | Total Loss: 4.321418762207031 | KNN Loss: 4.203174591064453 | CLS Loss: 0.11824420839548111\n",
      "Epoch 7 / 200 | iteration 20 / 171 | Total Loss: 4.3647284507751465 | KNN Loss: 4.257623195648193 | CLS Loss: 0.10710505396127701\n",
      "Epoch 7 / 200 | iteration 30 / 171 | Total Loss: 4.319944858551025 | KNN Loss: 4.26423454284668 | CLS Loss: 0.055710550397634506\n",
      "Epoch 7 / 200 | iteration 40 / 171 | Total Loss: 4.344857215881348 | KNN Loss: 4.2540388107299805 | CLS Loss: 0.09081829339265823\n",
      "Epoch 7 / 200 | iteration 50 / 171 | Total Loss: 4.338502407073975 | KNN Loss: 4.2598466873168945 | CLS Loss: 0.07865582406520844\n",
      "Epoch 7 / 200 | iteration 60 / 171 | Total Loss: 4.339374542236328 | KNN Loss: 4.265472888946533 | CLS Loss: 0.0739014595746994\n",
      "Epoch 7 / 200 | iteration 70 / 171 | Total Loss: 4.30608606338501 | KNN Loss: 4.224106788635254 | CLS Loss: 0.08197937905788422\n",
      "Epoch 7 / 200 | iteration 80 / 171 | Total Loss: 4.290319919586182 | KNN Loss: 4.252078533172607 | CLS Loss: 0.0382411926984787\n",
      "Epoch 7 / 200 | iteration 90 / 171 | Total Loss: 4.2956695556640625 | KNN Loss: 4.237730979919434 | CLS Loss: 0.05793871358036995\n",
      "Epoch 7 / 200 | iteration 100 / 171 | Total Loss: 4.330917835235596 | KNN Loss: 4.248663425445557 | CLS Loss: 0.08225453644990921\n",
      "Epoch 7 / 200 | iteration 110 / 171 | Total Loss: 4.337269306182861 | KNN Loss: 4.254854202270508 | CLS Loss: 0.08241516351699829\n",
      "Epoch 7 / 200 | iteration 120 / 171 | Total Loss: 4.372684478759766 | KNN Loss: 4.255638599395752 | CLS Loss: 0.11704599112272263\n",
      "Epoch 7 / 200 | iteration 130 / 171 | Total Loss: 4.376931667327881 | KNN Loss: 4.26558256149292 | CLS Loss: 0.11134904623031616\n",
      "Epoch 7 / 200 | iteration 140 / 171 | Total Loss: 4.323275566101074 | KNN Loss: 4.268829345703125 | CLS Loss: 0.054446056485176086\n",
      "Epoch 7 / 200 | iteration 150 / 171 | Total Loss: 4.31126070022583 | KNN Loss: 4.259411334991455 | CLS Loss: 0.051849424839019775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 / 200 | iteration 160 / 171 | Total Loss: 4.337508201599121 | KNN Loss: 4.26438570022583 | CLS Loss: 0.07312242686748505\n",
      "Epoch 7 / 200 | iteration 170 / 171 | Total Loss: 4.38505744934082 | KNN Loss: 4.29645299911499 | CLS Loss: 0.08860433101654053\n",
      "Epoch: 007, Loss: 4.3477, Train: 0.9780, Valid: 0.9746, Best: 0.9746\n",
      "Epoch 8 / 200 | iteration 0 / 171 | Total Loss: 4.310878753662109 | KNN Loss: 4.237785339355469 | CLS Loss: 0.0730932205915451\n",
      "Epoch 8 / 200 | iteration 10 / 171 | Total Loss: 4.317103385925293 | KNN Loss: 4.243649959564209 | CLS Loss: 0.07345327734947205\n",
      "Epoch 8 / 200 | iteration 20 / 171 | Total Loss: 4.363016128540039 | KNN Loss: 4.258892059326172 | CLS Loss: 0.1041242927312851\n",
      "Epoch 8 / 200 | iteration 30 / 171 | Total Loss: 4.30294132232666 | KNN Loss: 4.233722686767578 | CLS Loss: 0.06921858340501785\n",
      "Epoch 8 / 200 | iteration 40 / 171 | Total Loss: 4.381361484527588 | KNN Loss: 4.2399582862854 | CLS Loss: 0.1414029896259308\n",
      "Epoch 8 / 200 | iteration 50 / 171 | Total Loss: 4.364475250244141 | KNN Loss: 4.276676654815674 | CLS Loss: 0.08779864758253098\n",
      "Epoch 8 / 200 | iteration 60 / 171 | Total Loss: 4.374194145202637 | KNN Loss: 4.2633056640625 | CLS Loss: 0.11088861525058746\n",
      "Epoch 8 / 200 | iteration 70 / 171 | Total Loss: 4.327486991882324 | KNN Loss: 4.236845970153809 | CLS Loss: 0.09064113348722458\n",
      "Epoch 8 / 200 | iteration 80 / 171 | Total Loss: 4.303194999694824 | KNN Loss: 4.245768070220947 | CLS Loss: 0.05742698907852173\n",
      "Epoch 8 / 200 | iteration 90 / 171 | Total Loss: 4.335849761962891 | KNN Loss: 4.258498668670654 | CLS Loss: 0.0773509070277214\n",
      "Epoch 8 / 200 | iteration 100 / 171 | Total Loss: 4.364867687225342 | KNN Loss: 4.261068344116211 | CLS Loss: 0.10379944741725922\n",
      "Epoch 8 / 200 | iteration 110 / 171 | Total Loss: 4.319580078125 | KNN Loss: 4.23183012008667 | CLS Loss: 0.0877501517534256\n",
      "Epoch 8 / 200 | iteration 120 / 171 | Total Loss: 4.395593643188477 | KNN Loss: 4.248302459716797 | CLS Loss: 0.14729124307632446\n",
      "Epoch 8 / 200 | iteration 130 / 171 | Total Loss: 4.360457420349121 | KNN Loss: 4.27067756652832 | CLS Loss: 0.08977968990802765\n",
      "Epoch 8 / 200 | iteration 140 / 171 | Total Loss: 4.304567337036133 | KNN Loss: 4.248203277587891 | CLS Loss: 0.0563640221953392\n",
      "Epoch 8 / 200 | iteration 150 / 171 | Total Loss: 4.322741508483887 | KNN Loss: 4.251701354980469 | CLS Loss: 0.07104022055864334\n",
      "Epoch 8 / 200 | iteration 160 / 171 | Total Loss: 4.302288055419922 | KNN Loss: 4.241859436035156 | CLS Loss: 0.06042862311005592\n",
      "Epoch 8 / 200 | iteration 170 / 171 | Total Loss: 4.313731670379639 | KNN Loss: 4.271801471710205 | CLS Loss: 0.04193012788891792\n",
      "Epoch: 008, Loss: 4.3355, Train: 0.9797, Valid: 0.9776, Best: 0.9776\n",
      "Epoch 9 / 200 | iteration 0 / 171 | Total Loss: 4.289420127868652 | KNN Loss: 4.222919464111328 | CLS Loss: 0.06650055199861526\n",
      "Epoch 9 / 200 | iteration 10 / 171 | Total Loss: 4.270908355712891 | KNN Loss: 4.211365222930908 | CLS Loss: 0.05954323336482048\n",
      "Epoch 9 / 200 | iteration 20 / 171 | Total Loss: 4.283639907836914 | KNN Loss: 4.197442054748535 | CLS Loss: 0.08619806170463562\n",
      "Epoch 9 / 200 | iteration 30 / 171 | Total Loss: 4.273143768310547 | KNN Loss: 4.219418048858643 | CLS Loss: 0.05372581630945206\n",
      "Epoch 9 / 200 | iteration 40 / 171 | Total Loss: 4.293094158172607 | KNN Loss: 4.232988357543945 | CLS Loss: 0.06010602042078972\n",
      "Epoch 9 / 200 | iteration 50 / 171 | Total Loss: 4.3177337646484375 | KNN Loss: 4.223013877868652 | CLS Loss: 0.09471990168094635\n",
      "Epoch 9 / 200 | iteration 60 / 171 | Total Loss: 4.311677932739258 | KNN Loss: 4.234638690948486 | CLS Loss: 0.07703935354948044\n",
      "Epoch 9 / 200 | iteration 70 / 171 | Total Loss: 4.334538459777832 | KNN Loss: 4.266963005065918 | CLS Loss: 0.06757558882236481\n",
      "Epoch 9 / 200 | iteration 80 / 171 | Total Loss: 4.27829647064209 | KNN Loss: 4.201949119567871 | CLS Loss: 0.07634739577770233\n",
      "Epoch 9 / 200 | iteration 90 / 171 | Total Loss: 4.286957740783691 | KNN Loss: 4.231097221374512 | CLS Loss: 0.05586070939898491\n",
      "Epoch 9 / 200 | iteration 100 / 171 | Total Loss: 4.338983058929443 | KNN Loss: 4.272748947143555 | CLS Loss: 0.0662340521812439\n",
      "Epoch 9 / 200 | iteration 110 / 171 | Total Loss: 4.322937965393066 | KNN Loss: 4.232966423034668 | CLS Loss: 0.08997175842523575\n",
      "Epoch 9 / 200 | iteration 120 / 171 | Total Loss: 4.292832374572754 | KNN Loss: 4.232112407684326 | CLS Loss: 0.060720089823007584\n",
      "Epoch 9 / 200 | iteration 130 / 171 | Total Loss: 4.3693013191223145 | KNN Loss: 4.260406017303467 | CLS Loss: 0.10889527946710587\n",
      "Epoch 9 / 200 | iteration 140 / 171 | Total Loss: 4.32344913482666 | KNN Loss: 4.238966941833496 | CLS Loss: 0.08448202162981033\n",
      "Epoch 9 / 200 | iteration 150 / 171 | Total Loss: 4.33995246887207 | KNN Loss: 4.2609710693359375 | CLS Loss: 0.07898160815238953\n",
      "Epoch 9 / 200 | iteration 160 / 171 | Total Loss: 4.275926113128662 | KNN Loss: 4.235982894897461 | CLS Loss: 0.03994300216436386\n",
      "Epoch 9 / 200 | iteration 170 / 171 | Total Loss: 4.302562713623047 | KNN Loss: 4.230199813842773 | CLS Loss: 0.07236310094594955\n",
      "Epoch: 009, Loss: 4.3112, Train: 0.9814, Valid: 0.9782, Best: 0.9782\n",
      "Epoch 10 / 200 | iteration 0 / 171 | Total Loss: 4.293417453765869 | KNN Loss: 4.215635299682617 | CLS Loss: 0.07778225839138031\n",
      "Epoch 10 / 200 | iteration 10 / 171 | Total Loss: 4.318349838256836 | KNN Loss: 4.2177348136901855 | CLS Loss: 0.10061503201723099\n",
      "Epoch 10 / 200 | iteration 20 / 171 | Total Loss: 4.284826755523682 | KNN Loss: 4.216627597808838 | CLS Loss: 0.06819906830787659\n",
      "Epoch 10 / 200 | iteration 30 / 171 | Total Loss: 4.294923782348633 | KNN Loss: 4.255861759185791 | CLS Loss: 0.03906209394335747\n",
      "Epoch 10 / 200 | iteration 40 / 171 | Total Loss: 4.2999796867370605 | KNN Loss: 4.2534685134887695 | CLS Loss: 0.04651094228029251\n",
      "Epoch 10 / 200 | iteration 50 / 171 | Total Loss: 4.312800884246826 | KNN Loss: 4.248522758483887 | CLS Loss: 0.06427821516990662\n",
      "Epoch 10 / 200 | iteration 60 / 171 | Total Loss: 4.253820896148682 | KNN Loss: 4.200533866882324 | CLS Loss: 0.053286854177713394\n",
      "Epoch 10 / 200 | iteration 70 / 171 | Total Loss: 4.32767915725708 | KNN Loss: 4.230801105499268 | CLS Loss: 0.09687824547290802\n",
      "Epoch 10 / 200 | iteration 80 / 171 | Total Loss: 4.299315929412842 | KNN Loss: 4.234528541564941 | CLS Loss: 0.06478746980428696\n",
      "Epoch 10 / 200 | iteration 90 / 171 | Total Loss: 4.354273796081543 | KNN Loss: 4.2492828369140625 | CLS Loss: 0.10499079525470734\n",
      "Epoch 10 / 200 | iteration 100 / 171 | Total Loss: 4.252502918243408 | KNN Loss: 4.191404819488525 | CLS Loss: 0.06109805405139923\n",
      "Epoch 10 / 200 | iteration 110 / 171 | Total Loss: 4.337416172027588 | KNN Loss: 4.2670817375183105 | CLS Loss: 0.07033459842205048\n",
      "Epoch 10 / 200 | iteration 120 / 171 | Total Loss: 4.3887457847595215 | KNN Loss: 4.32735013961792 | CLS Loss: 0.061395835131406784\n",
      "Epoch 10 / 200 | iteration 130 / 171 | Total Loss: 4.317685604095459 | KNN Loss: 4.253875732421875 | CLS Loss: 0.06380972266197205\n",
      "Epoch 10 / 200 | iteration 140 / 171 | Total Loss: 4.295305252075195 | KNN Loss: 4.237608432769775 | CLS Loss: 0.05769667774438858\n",
      "Epoch 10 / 200 | iteration 150 / 171 | Total Loss: 4.324771881103516 | KNN Loss: 4.2645583152771 | CLS Loss: 0.06021363288164139\n",
      "Epoch 10 / 200 | iteration 160 / 171 | Total Loss: 4.279519557952881 | KNN Loss: 4.212782859802246 | CLS Loss: 0.06673680245876312\n",
      "Epoch 10 / 200 | iteration 170 / 171 | Total Loss: 4.325093746185303 | KNN Loss: 4.245652198791504 | CLS Loss: 0.07944175601005554\n",
      "Epoch: 010, Loss: 4.3056, Train: 0.9812, Valid: 0.9763, Best: 0.9782\n",
      "Epoch 11 / 200 | iteration 0 / 171 | Total Loss: 4.365869998931885 | KNN Loss: 4.30316686630249 | CLS Loss: 0.06270329654216766\n",
      "Epoch 11 / 200 | iteration 10 / 171 | Total Loss: 4.280948162078857 | KNN Loss: 4.223047733306885 | CLS Loss: 0.057900600135326385\n",
      "Epoch 11 / 200 | iteration 20 / 171 | Total Loss: 4.3253960609436035 | KNN Loss: 4.2510881423950195 | CLS Loss: 0.07430776953697205\n",
      "Epoch 11 / 200 | iteration 30 / 171 | Total Loss: 4.255065441131592 | KNN Loss: 4.199636459350586 | CLS Loss: 0.05542907491326332\n",
      "Epoch 11 / 200 | iteration 40 / 171 | Total Loss: 4.284408092498779 | KNN Loss: 4.24370002746582 | CLS Loss: 0.04070784151554108\n",
      "Epoch 11 / 200 | iteration 50 / 171 | Total Loss: 4.306509017944336 | KNN Loss: 4.249803066253662 | CLS Loss: 0.056706011295318604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 / 200 | iteration 60 / 171 | Total Loss: 4.265674591064453 | KNN Loss: 4.198460578918457 | CLS Loss: 0.06721407920122147\n",
      "Epoch 11 / 200 | iteration 70 / 171 | Total Loss: 4.292954444885254 | KNN Loss: 4.233286380767822 | CLS Loss: 0.05966807156801224\n",
      "Epoch 11 / 200 | iteration 80 / 171 | Total Loss: 4.345195293426514 | KNN Loss: 4.264439582824707 | CLS Loss: 0.08075588196516037\n",
      "Epoch 11 / 200 | iteration 90 / 171 | Total Loss: 4.255561828613281 | KNN Loss: 4.189026832580566 | CLS Loss: 0.06653519719839096\n",
      "Epoch 11 / 200 | iteration 100 / 171 | Total Loss: 4.286505699157715 | KNN Loss: 4.235074996948242 | CLS Loss: 0.05143052339553833\n",
      "Epoch 11 / 200 | iteration 110 / 171 | Total Loss: 4.33653450012207 | KNN Loss: 4.242529392242432 | CLS Loss: 0.09400498867034912\n",
      "Epoch 11 / 200 | iteration 120 / 171 | Total Loss: 4.2691545486450195 | KNN Loss: 4.221701622009277 | CLS Loss: 0.047452885657548904\n",
      "Epoch 11 / 200 | iteration 130 / 171 | Total Loss: 4.316824913024902 | KNN Loss: 4.262485027313232 | CLS Loss: 0.05433981865644455\n",
      "Epoch 11 / 200 | iteration 140 / 171 | Total Loss: 4.268605709075928 | KNN Loss: 4.211073398590088 | CLS Loss: 0.05753237009048462\n",
      "Epoch 11 / 200 | iteration 150 / 171 | Total Loss: 4.277976989746094 | KNN Loss: 4.230727672576904 | CLS Loss: 0.04724913462996483\n",
      "Epoch 11 / 200 | iteration 160 / 171 | Total Loss: 4.26779317855835 | KNN Loss: 4.222237586975098 | CLS Loss: 0.045555371791124344\n",
      "Epoch 11 / 200 | iteration 170 / 171 | Total Loss: 4.276571273803711 | KNN Loss: 4.210266590118408 | CLS Loss: 0.0663045272231102\n",
      "Epoch: 011, Loss: 4.2955, Train: 0.9840, Valid: 0.9797, Best: 0.9797\n",
      "Epoch 12 / 200 | iteration 0 / 171 | Total Loss: 4.25435733795166 | KNN Loss: 4.2234601974487305 | CLS Loss: 0.03089718520641327\n",
      "Epoch 12 / 200 | iteration 10 / 171 | Total Loss: 4.293512344360352 | KNN Loss: 4.258181571960449 | CLS Loss: 0.03533094748854637\n",
      "Epoch 12 / 200 | iteration 20 / 171 | Total Loss: 4.244518756866455 | KNN Loss: 4.194303035736084 | CLS Loss: 0.050215572118759155\n",
      "Epoch 12 / 200 | iteration 30 / 171 | Total Loss: 4.2742767333984375 | KNN Loss: 4.23439884185791 | CLS Loss: 0.03987805172801018\n",
      "Epoch 12 / 200 | iteration 40 / 171 | Total Loss: 4.319660663604736 | KNN Loss: 4.2337799072265625 | CLS Loss: 0.0858808159828186\n",
      "Epoch 12 / 200 | iteration 50 / 171 | Total Loss: 4.261892795562744 | KNN Loss: 4.220963001251221 | CLS Loss: 0.04092997685074806\n",
      "Epoch 12 / 200 | iteration 60 / 171 | Total Loss: 4.3322367668151855 | KNN Loss: 4.243503093719482 | CLS Loss: 0.08873381465673447\n",
      "Epoch 12 / 200 | iteration 70 / 171 | Total Loss: 4.251483917236328 | KNN Loss: 4.197600841522217 | CLS Loss: 0.053883228451013565\n",
      "Epoch 12 / 200 | iteration 80 / 171 | Total Loss: 4.260575294494629 | KNN Loss: 4.206369876861572 | CLS Loss: 0.054205551743507385\n",
      "Epoch 12 / 200 | iteration 90 / 171 | Total Loss: 4.2789082527160645 | KNN Loss: 4.2038726806640625 | CLS Loss: 0.0750356987118721\n",
      "Epoch 12 / 200 | iteration 100 / 171 | Total Loss: 4.282371997833252 | KNN Loss: 4.211442947387695 | CLS Loss: 0.07092899084091187\n",
      "Epoch 12 / 200 | iteration 110 / 171 | Total Loss: 4.296757698059082 | KNN Loss: 4.228688716888428 | CLS Loss: 0.06806893646717072\n",
      "Epoch 12 / 200 | iteration 120 / 171 | Total Loss: 4.31619119644165 | KNN Loss: 4.242999076843262 | CLS Loss: 0.07319194823503494\n",
      "Epoch 12 / 200 | iteration 130 / 171 | Total Loss: 4.271349906921387 | KNN Loss: 4.207472324371338 | CLS Loss: 0.0638773962855339\n",
      "Epoch 12 / 200 | iteration 140 / 171 | Total Loss: 4.285830020904541 | KNN Loss: 4.229721546173096 | CLS Loss: 0.05610834062099457\n",
      "Epoch 12 / 200 | iteration 150 / 171 | Total Loss: 4.23072624206543 | KNN Loss: 4.187405586242676 | CLS Loss: 0.04332065209746361\n",
      "Epoch 12 / 200 | iteration 160 / 171 | Total Loss: 4.308548927307129 | KNN Loss: 4.262336730957031 | CLS Loss: 0.04621230438351631\n",
      "Epoch 12 / 200 | iteration 170 / 171 | Total Loss: 4.243080139160156 | KNN Loss: 4.176591396331787 | CLS Loss: 0.06648876518011093\n",
      "Epoch: 012, Loss: 4.2862, Train: 0.9836, Valid: 0.9796, Best: 0.9797\n",
      "Epoch 13 / 200 | iteration 0 / 171 | Total Loss: 4.299318313598633 | KNN Loss: 4.234886646270752 | CLS Loss: 0.06443144381046295\n",
      "Epoch 13 / 200 | iteration 10 / 171 | Total Loss: 4.252339839935303 | KNN Loss: 4.217046737670898 | CLS Loss: 0.035292915999889374\n",
      "Epoch 13 / 200 | iteration 20 / 171 | Total Loss: 4.2351555824279785 | KNN Loss: 4.204524993896484 | CLS Loss: 0.030630694702267647\n",
      "Epoch 13 / 200 | iteration 30 / 171 | Total Loss: 4.267147541046143 | KNN Loss: 4.224876880645752 | CLS Loss: 0.04227045178413391\n",
      "Epoch 13 / 200 | iteration 40 / 171 | Total Loss: 4.250247955322266 | KNN Loss: 4.184330940246582 | CLS Loss: 0.06591689586639404\n",
      "Epoch 13 / 200 | iteration 50 / 171 | Total Loss: 4.280864715576172 | KNN Loss: 4.241574287414551 | CLS Loss: 0.03929037228226662\n",
      "Epoch 13 / 200 | iteration 60 / 171 | Total Loss: 4.301718235015869 | KNN Loss: 4.23661470413208 | CLS Loss: 0.06510336697101593\n",
      "Epoch 13 / 200 | iteration 70 / 171 | Total Loss: 4.239783763885498 | KNN Loss: 4.20521879196167 | CLS Loss: 0.03456507623195648\n",
      "Epoch 13 / 200 | iteration 80 / 171 | Total Loss: 4.3272786140441895 | KNN Loss: 4.242532253265381 | CLS Loss: 0.08474631607532501\n",
      "Epoch 13 / 200 | iteration 90 / 171 | Total Loss: 4.2762451171875 | KNN Loss: 4.2239556312561035 | CLS Loss: 0.05228964611887932\n",
      "Epoch 13 / 200 | iteration 100 / 171 | Total Loss: 4.261563301086426 | KNN Loss: 4.214202880859375 | CLS Loss: 0.04736051335930824\n",
      "Epoch 13 / 200 | iteration 110 / 171 | Total Loss: 4.230318069458008 | KNN Loss: 4.203231334686279 | CLS Loss: 0.02708674781024456\n",
      "Epoch 13 / 200 | iteration 120 / 171 | Total Loss: 4.329782009124756 | KNN Loss: 4.243490219116211 | CLS Loss: 0.08629165589809418\n",
      "Epoch 13 / 200 | iteration 130 / 171 | Total Loss: 4.284775257110596 | KNN Loss: 4.235163688659668 | CLS Loss: 0.049611352384090424\n",
      "Epoch 13 / 200 | iteration 140 / 171 | Total Loss: 4.301738262176514 | KNN Loss: 4.242552280426025 | CLS Loss: 0.05918605253100395\n",
      "Epoch 13 / 200 | iteration 150 / 171 | Total Loss: 4.3076491355896 | KNN Loss: 4.204245567321777 | CLS Loss: 0.10340343415737152\n",
      "Epoch 13 / 200 | iteration 160 / 171 | Total Loss: 4.305180072784424 | KNN Loss: 4.257558345794678 | CLS Loss: 0.04762176796793938\n",
      "Epoch 13 / 200 | iteration 170 / 171 | Total Loss: 4.241939544677734 | KNN Loss: 4.208862781524658 | CLS Loss: 0.0330766998231411\n",
      "Epoch: 013, Loss: 4.2795, Train: 0.9849, Valid: 0.9820, Best: 0.9820\n",
      "Epoch 14 / 200 | iteration 0 / 171 | Total Loss: 4.318139553070068 | KNN Loss: 4.23405122756958 | CLS Loss: 0.0840882733464241\n",
      "Epoch 14 / 200 | iteration 10 / 171 | Total Loss: 4.287020683288574 | KNN Loss: 4.227285861968994 | CLS Loss: 0.059734735637903214\n",
      "Epoch 14 / 200 | iteration 20 / 171 | Total Loss: 4.26701545715332 | KNN Loss: 4.206246852874756 | CLS Loss: 0.06076847389340401\n",
      "Epoch 14 / 200 | iteration 30 / 171 | Total Loss: 4.28735876083374 | KNN Loss: 4.219860553741455 | CLS Loss: 0.06749825924634933\n",
      "Epoch 14 / 200 | iteration 40 / 171 | Total Loss: 4.296200752258301 | KNN Loss: 4.216991424560547 | CLS Loss: 0.07920930534601212\n",
      "Epoch 14 / 200 | iteration 50 / 171 | Total Loss: 4.257599830627441 | KNN Loss: 4.222357749938965 | CLS Loss: 0.0352419912815094\n",
      "Epoch 14 / 200 | iteration 60 / 171 | Total Loss: 4.34355354309082 | KNN Loss: 4.264322280883789 | CLS Loss: 0.07923144102096558\n",
      "Epoch 14 / 200 | iteration 70 / 171 | Total Loss: 4.25350284576416 | KNN Loss: 4.208415508270264 | CLS Loss: 0.04508716240525246\n",
      "Epoch 14 / 200 | iteration 80 / 171 | Total Loss: 4.250126838684082 | KNN Loss: 4.210840225219727 | CLS Loss: 0.03928649425506592\n",
      "Epoch 14 / 200 | iteration 90 / 171 | Total Loss: 4.257472515106201 | KNN Loss: 4.200177192687988 | CLS Loss: 0.05729532614350319\n",
      "Epoch 14 / 200 | iteration 100 / 171 | Total Loss: 4.289746284484863 | KNN Loss: 4.198825359344482 | CLS Loss: 0.0909210667014122\n",
      "Epoch 14 / 200 | iteration 110 / 171 | Total Loss: 4.260511875152588 | KNN Loss: 4.2158308029174805 | CLS Loss: 0.04468120262026787\n",
      "Epoch 14 / 200 | iteration 120 / 171 | Total Loss: 4.3362202644348145 | KNN Loss: 4.265183448791504 | CLS Loss: 0.07103681564331055\n",
      "Epoch 14 / 200 | iteration 130 / 171 | Total Loss: 4.289873123168945 | KNN Loss: 4.196323394775391 | CLS Loss: 0.09354956448078156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 / 200 | iteration 140 / 171 | Total Loss: 4.3268327713012695 | KNN Loss: 4.242607116699219 | CLS Loss: 0.08422550559043884\n",
      "Epoch 14 / 200 | iteration 150 / 171 | Total Loss: 4.250906944274902 | KNN Loss: 4.21697473526001 | CLS Loss: 0.033932238817214966\n",
      "Epoch 14 / 200 | iteration 160 / 171 | Total Loss: 4.29229736328125 | KNN Loss: 4.2303314208984375 | CLS Loss: 0.06196609139442444\n",
      "Epoch 14 / 200 | iteration 170 / 171 | Total Loss: 4.245389938354492 | KNN Loss: 4.221269607543945 | CLS Loss: 0.024120260030031204\n",
      "Epoch: 014, Loss: 4.2697, Train: 0.9867, Valid: 0.9825, Best: 0.9825\n",
      "Epoch 15 / 200 | iteration 0 / 171 | Total Loss: 4.3214945793151855 | KNN Loss: 4.242618083953857 | CLS Loss: 0.07887637615203857\n",
      "Epoch 15 / 200 | iteration 10 / 171 | Total Loss: 4.244252681732178 | KNN Loss: 4.169705867767334 | CLS Loss: 0.07454696297645569\n",
      "Epoch 15 / 200 | iteration 20 / 171 | Total Loss: 4.253611087799072 | KNN Loss: 4.202136993408203 | CLS Loss: 0.05147401615977287\n",
      "Epoch 15 / 200 | iteration 30 / 171 | Total Loss: 4.227664947509766 | KNN Loss: 4.179841041564941 | CLS Loss: 0.04782390594482422\n",
      "Epoch 15 / 200 | iteration 40 / 171 | Total Loss: 4.239110946655273 | KNN Loss: 4.204461097717285 | CLS Loss: 0.03464963287115097\n",
      "Epoch 15 / 200 | iteration 50 / 171 | Total Loss: 4.264228820800781 | KNN Loss: 4.230332851409912 | CLS Loss: 0.03389611840248108\n",
      "Epoch 15 / 200 | iteration 60 / 171 | Total Loss: 4.255300998687744 | KNN Loss: 4.192056655883789 | CLS Loss: 0.06324446946382523\n",
      "Epoch 15 / 200 | iteration 70 / 171 | Total Loss: 4.251892566680908 | KNN Loss: 4.2050957679748535 | CLS Loss: 0.04679699242115021\n",
      "Epoch 15 / 200 | iteration 80 / 171 | Total Loss: 4.257505416870117 | KNN Loss: 4.219035625457764 | CLS Loss: 0.038469694554805756\n",
      "Epoch 15 / 200 | iteration 90 / 171 | Total Loss: 4.315532207489014 | KNN Loss: 4.2494378089904785 | CLS Loss: 0.06609457731246948\n",
      "Epoch 15 / 200 | iteration 100 / 171 | Total Loss: 4.235628128051758 | KNN Loss: 4.173252582550049 | CLS Loss: 0.06237533688545227\n",
      "Epoch 15 / 200 | iteration 110 / 171 | Total Loss: 4.230531215667725 | KNN Loss: 4.190695762634277 | CLS Loss: 0.03983531519770622\n",
      "Epoch 15 / 200 | iteration 120 / 171 | Total Loss: 4.2660417556762695 | KNN Loss: 4.230377674102783 | CLS Loss: 0.035664256662130356\n",
      "Epoch 15 / 200 | iteration 130 / 171 | Total Loss: 4.2672576904296875 | KNN Loss: 4.184988498687744 | CLS Loss: 0.08226905018091202\n",
      "Epoch 15 / 200 | iteration 140 / 171 | Total Loss: 4.259302616119385 | KNN Loss: 4.211338043212891 | CLS Loss: 0.04796476662158966\n",
      "Epoch 15 / 200 | iteration 150 / 171 | Total Loss: 4.266141414642334 | KNN Loss: 4.2357916831970215 | CLS Loss: 0.03034972958266735\n",
      "Epoch 15 / 200 | iteration 160 / 171 | Total Loss: 4.225168704986572 | KNN Loss: 4.16676664352417 | CLS Loss: 0.05840194225311279\n",
      "Epoch 15 / 200 | iteration 170 / 171 | Total Loss: 4.280651569366455 | KNN Loss: 4.210289001464844 | CLS Loss: 0.07036234438419342\n",
      "Epoch: 015, Loss: 4.2665, Train: 0.9872, Valid: 0.9834, Best: 0.9834\n",
      "Epoch 16 / 200 | iteration 0 / 171 | Total Loss: 4.252911567687988 | KNN Loss: 4.2195305824279785 | CLS Loss: 0.03338102251291275\n",
      "Epoch 16 / 200 | iteration 10 / 171 | Total Loss: 4.265336513519287 | KNN Loss: 4.224295139312744 | CLS Loss: 0.0410415418446064\n",
      "Epoch 16 / 200 | iteration 20 / 171 | Total Loss: 4.257801055908203 | KNN Loss: 4.221839904785156 | CLS Loss: 0.03596100956201553\n",
      "Epoch 16 / 200 | iteration 30 / 171 | Total Loss: 4.267528057098389 | KNN Loss: 4.210251331329346 | CLS Loss: 0.05727659538388252\n",
      "Epoch 16 / 200 | iteration 40 / 171 | Total Loss: 4.230020046234131 | KNN Loss: 4.185631275177002 | CLS Loss: 0.04438890889286995\n",
      "Epoch 16 / 200 | iteration 50 / 171 | Total Loss: 4.216946601867676 | KNN Loss: 4.1842732429504395 | CLS Loss: 0.03267345204949379\n",
      "Epoch 16 / 200 | iteration 60 / 171 | Total Loss: 4.259037971496582 | KNN Loss: 4.2063446044921875 | CLS Loss: 0.052693285048007965\n",
      "Epoch 16 / 200 | iteration 70 / 171 | Total Loss: 4.258512496948242 | KNN Loss: 4.218114852905273 | CLS Loss: 0.04039764031767845\n",
      "Epoch 16 / 200 | iteration 80 / 171 | Total Loss: 4.263577461242676 | KNN Loss: 4.222288608551025 | CLS Loss: 0.041288699954748154\n",
      "Epoch 16 / 200 | iteration 90 / 171 | Total Loss: 4.262869834899902 | KNN Loss: 4.219605445861816 | CLS Loss: 0.04326438903808594\n",
      "Epoch 16 / 200 | iteration 100 / 171 | Total Loss: 4.2892842292785645 | KNN Loss: 4.236289978027344 | CLS Loss: 0.0529940165579319\n",
      "Epoch 16 / 200 | iteration 110 / 171 | Total Loss: 4.274653911590576 | KNN Loss: 4.196834087371826 | CLS Loss: 0.07781982421875\n",
      "Epoch 16 / 200 | iteration 120 / 171 | Total Loss: 4.210235118865967 | KNN Loss: 4.1627936363220215 | CLS Loss: 0.04744170978665352\n",
      "Epoch 16 / 200 | iteration 130 / 171 | Total Loss: 4.265377521514893 | KNN Loss: 4.215648174285889 | CLS Loss: 0.049729570746421814\n",
      "Epoch 16 / 200 | iteration 140 / 171 | Total Loss: 4.287976264953613 | KNN Loss: 4.233428478240967 | CLS Loss: 0.054547619074583054\n",
      "Epoch 16 / 200 | iteration 150 / 171 | Total Loss: 4.293752193450928 | KNN Loss: 4.216361045837402 | CLS Loss: 0.07739119231700897\n",
      "Epoch 16 / 200 | iteration 160 / 171 | Total Loss: 4.212985515594482 | KNN Loss: 4.185652256011963 | CLS Loss: 0.027333468198776245\n",
      "Epoch 16 / 200 | iteration 170 / 171 | Total Loss: 4.268721103668213 | KNN Loss: 4.238796234130859 | CLS Loss: 0.029924767091870308\n",
      "Epoch: 016, Loss: 4.2615, Train: 0.9878, Valid: 0.9835, Best: 0.9835\n",
      "Epoch 17 / 200 | iteration 0 / 171 | Total Loss: 4.293816566467285 | KNN Loss: 4.199636459350586 | CLS Loss: 0.09418017417192459\n",
      "Epoch 17 / 200 | iteration 10 / 171 | Total Loss: 4.252067565917969 | KNN Loss: 4.193892002105713 | CLS Loss: 0.05817560479044914\n",
      "Epoch 17 / 200 | iteration 20 / 171 | Total Loss: 4.268584251403809 | KNN Loss: 4.221558570861816 | CLS Loss: 0.047025758773088455\n",
      "Epoch 17 / 200 | iteration 30 / 171 | Total Loss: 4.219053745269775 | KNN Loss: 4.180873394012451 | CLS Loss: 0.03818057104945183\n",
      "Epoch 17 / 200 | iteration 40 / 171 | Total Loss: 4.269782543182373 | KNN Loss: 4.203434467315674 | CLS Loss: 0.0663478672504425\n",
      "Epoch 17 / 200 | iteration 50 / 171 | Total Loss: 4.39054536819458 | KNN Loss: 4.340360641479492 | CLS Loss: 0.050184864550828934\n",
      "Epoch 17 / 200 | iteration 60 / 171 | Total Loss: 4.23382043838501 | KNN Loss: 4.201347827911377 | CLS Loss: 0.032472576946020126\n",
      "Epoch 17 / 200 | iteration 70 / 171 | Total Loss: 4.237779140472412 | KNN Loss: 4.189319610595703 | CLS Loss: 0.048459380865097046\n",
      "Epoch 17 / 200 | iteration 80 / 171 | Total Loss: 4.247589111328125 | KNN Loss: 4.218418121337891 | CLS Loss: 0.029170751571655273\n",
      "Epoch 17 / 200 | iteration 90 / 171 | Total Loss: 4.227770805358887 | KNN Loss: 4.201160907745361 | CLS Loss: 0.026609765365719795\n",
      "Epoch 17 / 200 | iteration 100 / 171 | Total Loss: 4.263401985168457 | KNN Loss: 4.223142623901367 | CLS Loss: 0.04025954008102417\n",
      "Epoch 17 / 200 | iteration 110 / 171 | Total Loss: 4.235810279846191 | KNN Loss: 4.199385166168213 | CLS Loss: 0.036425117403268814\n",
      "Epoch 17 / 200 | iteration 120 / 171 | Total Loss: 4.248808860778809 | KNN Loss: 4.188164710998535 | CLS Loss: 0.06064410135149956\n",
      "Epoch 17 / 200 | iteration 130 / 171 | Total Loss: 4.286612510681152 | KNN Loss: 4.216869831085205 | CLS Loss: 0.0697428435087204\n",
      "Epoch 17 / 200 | iteration 140 / 171 | Total Loss: 4.244619846343994 | KNN Loss: 4.214621067047119 | CLS Loss: 0.029998669400811195\n",
      "Epoch 17 / 200 | iteration 150 / 171 | Total Loss: 4.229732036590576 | KNN Loss: 4.1850175857543945 | CLS Loss: 0.04471424221992493\n",
      "Epoch 17 / 200 | iteration 160 / 171 | Total Loss: 4.243332386016846 | KNN Loss: 4.192070484161377 | CLS Loss: 0.051261674612760544\n",
      "Epoch 17 / 200 | iteration 170 / 171 | Total Loss: 4.271347999572754 | KNN Loss: 4.2316813468933105 | CLS Loss: 0.03966687619686127\n",
      "Epoch: 017, Loss: 4.2632, Train: 0.9869, Valid: 0.9826, Best: 0.9835\n",
      "Epoch 18 / 200 | iteration 0 / 171 | Total Loss: 4.258468151092529 | KNN Loss: 4.206067085266113 | CLS Loss: 0.05240114405751228\n",
      "Epoch 18 / 200 | iteration 10 / 171 | Total Loss: 4.239553451538086 | KNN Loss: 4.195770740509033 | CLS Loss: 0.043782707303762436\n",
      "Epoch 18 / 200 | iteration 20 / 171 | Total Loss: 4.291168212890625 | KNN Loss: 4.209646701812744 | CLS Loss: 0.08152170479297638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 / 200 | iteration 30 / 171 | Total Loss: 4.268491744995117 | KNN Loss: 4.179671764373779 | CLS Loss: 0.08882015198469162\n",
      "Epoch 18 / 200 | iteration 40 / 171 | Total Loss: 4.274069309234619 | KNN Loss: 4.242717266082764 | CLS Loss: 0.03135199844837189\n",
      "Epoch 18 / 200 | iteration 50 / 171 | Total Loss: 4.261744976043701 | KNN Loss: 4.19309663772583 | CLS Loss: 0.06864825636148453\n",
      "Epoch 18 / 200 | iteration 60 / 171 | Total Loss: 4.265486717224121 | KNN Loss: 4.206356525421143 | CLS Loss: 0.0591299831867218\n",
      "Epoch 18 / 200 | iteration 70 / 171 | Total Loss: 4.2632646560668945 | KNN Loss: 4.203118801116943 | CLS Loss: 0.06014568358659744\n",
      "Epoch 18 / 200 | iteration 80 / 171 | Total Loss: 4.288754463195801 | KNN Loss: 4.255762100219727 | CLS Loss: 0.03299219161272049\n",
      "Epoch 18 / 200 | iteration 90 / 171 | Total Loss: 4.256245136260986 | KNN Loss: 4.218506336212158 | CLS Loss: 0.037738800048828125\n",
      "Epoch 18 / 200 | iteration 100 / 171 | Total Loss: 4.250636577606201 | KNN Loss: 4.210872650146484 | CLS Loss: 0.03976372629404068\n",
      "Epoch 18 / 200 | iteration 110 / 171 | Total Loss: 4.268925666809082 | KNN Loss: 4.217597961425781 | CLS Loss: 0.05132780224084854\n",
      "Epoch 18 / 200 | iteration 120 / 171 | Total Loss: 4.2299017906188965 | KNN Loss: 4.186652183532715 | CLS Loss: 0.043249599635601044\n",
      "Epoch 18 / 200 | iteration 130 / 171 | Total Loss: 4.251569747924805 | KNN Loss: 4.194309711456299 | CLS Loss: 0.05726000294089317\n",
      "Epoch 18 / 200 | iteration 140 / 171 | Total Loss: 4.2589216232299805 | KNN Loss: 4.214907646179199 | CLS Loss: 0.04401407390832901\n",
      "Epoch 18 / 200 | iteration 150 / 171 | Total Loss: 4.220378875732422 | KNN Loss: 4.177525520324707 | CLS Loss: 0.04285318776965141\n",
      "Epoch 18 / 200 | iteration 160 / 171 | Total Loss: 4.246657848358154 | KNN Loss: 4.190798282623291 | CLS Loss: 0.05585945397615433\n",
      "Epoch 18 / 200 | iteration 170 / 171 | Total Loss: 4.2281575202941895 | KNN Loss: 4.182043552398682 | CLS Loss: 0.04611388221383095\n",
      "Epoch: 018, Loss: 4.2538, Train: 0.9871, Valid: 0.9826, Best: 0.9835\n",
      "Epoch 19 / 200 | iteration 0 / 171 | Total Loss: 4.212430953979492 | KNN Loss: 4.181236743927002 | CLS Loss: 0.031194187700748444\n",
      "Epoch 19 / 200 | iteration 10 / 171 | Total Loss: 4.191596031188965 | KNN Loss: 4.15955114364624 | CLS Loss: 0.03204512596130371\n",
      "Epoch 19 / 200 | iteration 20 / 171 | Total Loss: 4.21631383895874 | KNN Loss: 4.178245544433594 | CLS Loss: 0.03806823864579201\n",
      "Epoch 19 / 200 | iteration 30 / 171 | Total Loss: 4.220544338226318 | KNN Loss: 4.176297187805176 | CLS Loss: 0.04424697533249855\n",
      "Epoch 19 / 200 | iteration 40 / 171 | Total Loss: 4.250326156616211 | KNN Loss: 4.207540512084961 | CLS Loss: 0.04278544709086418\n",
      "Epoch 19 / 200 | iteration 50 / 171 | Total Loss: 4.25237512588501 | KNN Loss: 4.205684185028076 | CLS Loss: 0.046691060066223145\n",
      "Epoch 19 / 200 | iteration 60 / 171 | Total Loss: 4.257746696472168 | KNN Loss: 4.228875637054443 | CLS Loss: 0.02887090854346752\n",
      "Epoch 19 / 200 | iteration 70 / 171 | Total Loss: 4.215400695800781 | KNN Loss: 4.170979976654053 | CLS Loss: 0.044420551508665085\n",
      "Epoch 19 / 200 | iteration 80 / 171 | Total Loss: 4.277261257171631 | KNN Loss: 4.237159729003906 | CLS Loss: 0.04010166972875595\n",
      "Epoch 19 / 200 | iteration 90 / 171 | Total Loss: 4.263474464416504 | KNN Loss: 4.233930587768555 | CLS Loss: 0.029543932527303696\n",
      "Epoch 19 / 200 | iteration 100 / 171 | Total Loss: 4.2464919090271 | KNN Loss: 4.193853855133057 | CLS Loss: 0.05263783037662506\n",
      "Epoch 19 / 200 | iteration 110 / 171 | Total Loss: 4.302638053894043 | KNN Loss: 4.245056629180908 | CLS Loss: 0.05758129432797432\n",
      "Epoch 19 / 200 | iteration 120 / 171 | Total Loss: 4.2620697021484375 | KNN Loss: 4.232309818267822 | CLS Loss: 0.029759664088487625\n",
      "Epoch 19 / 200 | iteration 130 / 171 | Total Loss: 4.251590728759766 | KNN Loss: 4.211536407470703 | CLS Loss: 0.0400543250143528\n",
      "Epoch 19 / 200 | iteration 140 / 171 | Total Loss: 4.2314019203186035 | KNN Loss: 4.205057621002197 | CLS Loss: 0.026344096288084984\n",
      "Epoch 19 / 200 | iteration 150 / 171 | Total Loss: 4.303040027618408 | KNN Loss: 4.220028400421143 | CLS Loss: 0.08301170915365219\n",
      "Epoch 19 / 200 | iteration 160 / 171 | Total Loss: 4.297931671142578 | KNN Loss: 4.2087860107421875 | CLS Loss: 0.08914585411548615\n",
      "Epoch 19 / 200 | iteration 170 / 171 | Total Loss: 4.256463527679443 | KNN Loss: 4.223783493041992 | CLS Loss: 0.03268025070428848\n",
      "Epoch: 019, Loss: 4.2532, Train: 0.9844, Valid: 0.9801, Best: 0.9835\n",
      "Epoch 20 / 200 | iteration 0 / 171 | Total Loss: 4.275423526763916 | KNN Loss: 4.212615966796875 | CLS Loss: 0.06280773133039474\n",
      "Epoch 20 / 200 | iteration 10 / 171 | Total Loss: 4.231461524963379 | KNN Loss: 4.190578937530518 | CLS Loss: 0.04088256508111954\n",
      "Epoch 20 / 200 | iteration 20 / 171 | Total Loss: 4.254298686981201 | KNN Loss: 4.202822208404541 | CLS Loss: 0.051476411521434784\n",
      "Epoch 20 / 200 | iteration 30 / 171 | Total Loss: 4.24111270904541 | KNN Loss: 4.202421188354492 | CLS Loss: 0.038691628724336624\n",
      "Epoch 20 / 200 | iteration 40 / 171 | Total Loss: 4.229434490203857 | KNN Loss: 4.198985576629639 | CLS Loss: 0.03044906258583069\n",
      "Epoch 20 / 200 | iteration 50 / 171 | Total Loss: 4.203097820281982 | KNN Loss: 4.178125381469727 | CLS Loss: 0.024972273036837578\n",
      "Epoch 20 / 200 | iteration 60 / 171 | Total Loss: 4.25726842880249 | KNN Loss: 4.204595565795898 | CLS Loss: 0.05267287790775299\n",
      "Epoch 20 / 200 | iteration 70 / 171 | Total Loss: 4.274423122406006 | KNN Loss: 4.239894866943359 | CLS Loss: 0.03452831879258156\n",
      "Epoch 20 / 200 | iteration 80 / 171 | Total Loss: 4.2907538414001465 | KNN Loss: 4.226683616638184 | CLS Loss: 0.06407026946544647\n",
      "Epoch 20 / 200 | iteration 90 / 171 | Total Loss: 4.230268955230713 | KNN Loss: 4.167557239532471 | CLS Loss: 0.06271179020404816\n",
      "Epoch 20 / 200 | iteration 100 / 171 | Total Loss: 4.269336700439453 | KNN Loss: 4.206605434417725 | CLS Loss: 0.06273143738508224\n",
      "Epoch 20 / 200 | iteration 110 / 171 | Total Loss: 4.329098701477051 | KNN Loss: 4.288898944854736 | CLS Loss: 0.04019951820373535\n",
      "Epoch 20 / 200 | iteration 120 / 171 | Total Loss: 4.235464096069336 | KNN Loss: 4.188323497772217 | CLS Loss: 0.04714047163724899\n",
      "Epoch 20 / 200 | iteration 130 / 171 | Total Loss: 4.229959487915039 | KNN Loss: 4.182732105255127 | CLS Loss: 0.04722742363810539\n",
      "Epoch 20 / 200 | iteration 140 / 171 | Total Loss: 4.243378639221191 | KNN Loss: 4.1973652839660645 | CLS Loss: 0.04601350054144859\n",
      "Epoch 20 / 200 | iteration 150 / 171 | Total Loss: 4.2269206047058105 | KNN Loss: 4.191346168518066 | CLS Loss: 0.035574428737163544\n",
      "Epoch 20 / 200 | iteration 160 / 171 | Total Loss: 4.260495662689209 | KNN Loss: 4.17805290222168 | CLS Loss: 0.08244266360998154\n",
      "Epoch 20 / 200 | iteration 170 / 171 | Total Loss: 4.2231950759887695 | KNN Loss: 4.1972808837890625 | CLS Loss: 0.025913963094353676\n",
      "Epoch: 020, Loss: 4.2499, Train: 0.9883, Valid: 0.9825, Best: 0.9835\n",
      "Epoch 21 / 200 | iteration 0 / 171 | Total Loss: 4.230469703674316 | KNN Loss: 4.197232246398926 | CLS Loss: 0.03323724493384361\n",
      "Epoch 21 / 200 | iteration 10 / 171 | Total Loss: 4.249675273895264 | KNN Loss: 4.203142166137695 | CLS Loss: 0.04653305187821388\n",
      "Epoch 21 / 200 | iteration 20 / 171 | Total Loss: 4.227003574371338 | KNN Loss: 4.2132062911987305 | CLS Loss: 0.013797330670058727\n",
      "Epoch 21 / 200 | iteration 30 / 171 | Total Loss: 4.278366565704346 | KNN Loss: 4.241776943206787 | CLS Loss: 0.03658977150917053\n",
      "Epoch 21 / 200 | iteration 40 / 171 | Total Loss: 4.233912467956543 | KNN Loss: 4.184622764587402 | CLS Loss: 0.04928954690694809\n",
      "Epoch 21 / 200 | iteration 50 / 171 | Total Loss: 4.291474342346191 | KNN Loss: 4.233053684234619 | CLS Loss: 0.05842052400112152\n",
      "Epoch 21 / 200 | iteration 60 / 171 | Total Loss: 4.268459320068359 | KNN Loss: 4.2105231285095215 | CLS Loss: 0.057936206459999084\n",
      "Epoch 21 / 200 | iteration 70 / 171 | Total Loss: 4.235852241516113 | KNN Loss: 4.206136226654053 | CLS Loss: 0.029715998098254204\n",
      "Epoch 21 / 200 | iteration 80 / 171 | Total Loss: 4.247562408447266 | KNN Loss: 4.208461761474609 | CLS Loss: 0.03910065442323685\n",
      "Epoch 21 / 200 | iteration 90 / 171 | Total Loss: 4.231765270233154 | KNN Loss: 4.18391752243042 | CLS Loss: 0.04784760624170303\n",
      "Epoch 21 / 200 | iteration 100 / 171 | Total Loss: 4.231827259063721 | KNN Loss: 4.186589241027832 | CLS Loss: 0.04523792117834091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 200 | iteration 110 / 171 | Total Loss: 4.291011810302734 | KNN Loss: 4.203880786895752 | CLS Loss: 0.08713094890117645\n",
      "Epoch 21 / 200 | iteration 120 / 171 | Total Loss: 4.262795925140381 | KNN Loss: 4.209388732910156 | CLS Loss: 0.053407035768032074\n",
      "Epoch 21 / 200 | iteration 130 / 171 | Total Loss: 4.259098529815674 | KNN Loss: 4.2263593673706055 | CLS Loss: 0.03273935988545418\n",
      "Epoch 21 / 200 | iteration 140 / 171 | Total Loss: 4.1981964111328125 | KNN Loss: 4.181526184082031 | CLS Loss: 0.01667002961039543\n",
      "Epoch 21 / 200 | iteration 150 / 171 | Total Loss: 4.229837417602539 | KNN Loss: 4.208588123321533 | CLS Loss: 0.021249476820230484\n",
      "Epoch 21 / 200 | iteration 160 / 171 | Total Loss: 4.2756195068359375 | KNN Loss: 4.238901138305664 | CLS Loss: 0.03671824559569359\n",
      "Epoch 21 / 200 | iteration 170 / 171 | Total Loss: 4.293495178222656 | KNN Loss: 4.2436604499816895 | CLS Loss: 0.04983476921916008\n",
      "Epoch: 021, Loss: 4.2423, Train: 0.9878, Valid: 0.9839, Best: 0.9839\n",
      "Epoch 22 / 200 | iteration 0 / 171 | Total Loss: 4.271869659423828 | KNN Loss: 4.212596416473389 | CLS Loss: 0.059273410588502884\n",
      "Epoch 22 / 200 | iteration 10 / 171 | Total Loss: 4.2384514808654785 | KNN Loss: 4.221683502197266 | CLS Loss: 0.016768209636211395\n",
      "Epoch 22 / 200 | iteration 20 / 171 | Total Loss: 4.206186294555664 | KNN Loss: 4.1734619140625 | CLS Loss: 0.032724183052778244\n",
      "Epoch 22 / 200 | iteration 30 / 171 | Total Loss: 4.251253604888916 | KNN Loss: 4.204951286315918 | CLS Loss: 0.04630228132009506\n",
      "Epoch 22 / 200 | iteration 40 / 171 | Total Loss: 4.269352436065674 | KNN Loss: 4.219285488128662 | CLS Loss: 0.05006704851984978\n",
      "Epoch 22 / 200 | iteration 50 / 171 | Total Loss: 4.201409816741943 | KNN Loss: 4.160929203033447 | CLS Loss: 0.04048077389597893\n",
      "Epoch 22 / 200 | iteration 60 / 171 | Total Loss: 4.253542423248291 | KNN Loss: 4.212636470794678 | CLS Loss: 0.04090593755245209\n",
      "Epoch 22 / 200 | iteration 70 / 171 | Total Loss: 4.245700359344482 | KNN Loss: 4.197223663330078 | CLS Loss: 0.048476479947566986\n",
      "Epoch 22 / 200 | iteration 80 / 171 | Total Loss: 4.1861796379089355 | KNN Loss: 4.147590637207031 | CLS Loss: 0.03858896344900131\n",
      "Epoch 22 / 200 | iteration 90 / 171 | Total Loss: 4.237889766693115 | KNN Loss: 4.189327239990234 | CLS Loss: 0.048562683165073395\n",
      "Epoch 22 / 200 | iteration 100 / 171 | Total Loss: 4.278805255889893 | KNN Loss: 4.223587512969971 | CLS Loss: 0.05521764978766441\n",
      "Epoch 22 / 200 | iteration 110 / 171 | Total Loss: 4.2400312423706055 | KNN Loss: 4.205427169799805 | CLS Loss: 0.03460385650396347\n",
      "Epoch 22 / 200 | iteration 120 / 171 | Total Loss: 4.261841773986816 | KNN Loss: 4.251091957092285 | CLS Loss: 0.010749953798949718\n",
      "Epoch 22 / 200 | iteration 130 / 171 | Total Loss: 4.218600273132324 | KNN Loss: 4.1843647956848145 | CLS Loss: 0.03423553705215454\n",
      "Epoch 22 / 200 | iteration 140 / 171 | Total Loss: 4.230937480926514 | KNN Loss: 4.199626922607422 | CLS Loss: 0.031310632824897766\n",
      "Epoch 22 / 200 | iteration 150 / 171 | Total Loss: 4.235064506530762 | KNN Loss: 4.203588485717773 | CLS Loss: 0.03147583827376366\n",
      "Epoch 22 / 200 | iteration 160 / 171 | Total Loss: 4.243760585784912 | KNN Loss: 4.183733940124512 | CLS Loss: 0.060026463121175766\n",
      "Epoch 22 / 200 | iteration 170 / 171 | Total Loss: 4.2787580490112305 | KNN Loss: 4.21552848815918 | CLS Loss: 0.06322970986366272\n",
      "Epoch: 022, Loss: 4.2473, Train: 0.9884, Valid: 0.9828, Best: 0.9839\n",
      "Epoch 23 / 200 | iteration 0 / 171 | Total Loss: 4.228292942047119 | KNN Loss: 4.1870293617248535 | CLS Loss: 0.04126379266381264\n",
      "Epoch 23 / 200 | iteration 10 / 171 | Total Loss: 4.264421463012695 | KNN Loss: 4.234859943389893 | CLS Loss: 0.029561549425125122\n",
      "Epoch 23 / 200 | iteration 20 / 171 | Total Loss: 4.216810703277588 | KNN Loss: 4.169726848602295 | CLS Loss: 0.047083817422389984\n",
      "Epoch 23 / 200 | iteration 30 / 171 | Total Loss: 4.199918746948242 | KNN Loss: 4.189722061157227 | CLS Loss: 0.010196484625339508\n",
      "Epoch 23 / 200 | iteration 40 / 171 | Total Loss: 4.248808860778809 | KNN Loss: 4.202858924865723 | CLS Loss: 0.04595000296831131\n",
      "Epoch 23 / 200 | iteration 50 / 171 | Total Loss: 4.268284320831299 | KNN Loss: 4.220329284667969 | CLS Loss: 0.0479552336037159\n",
      "Epoch 23 / 200 | iteration 60 / 171 | Total Loss: 4.28781270980835 | KNN Loss: 4.228225231170654 | CLS Loss: 0.059587713330984116\n",
      "Epoch 23 / 200 | iteration 70 / 171 | Total Loss: 4.228653430938721 | KNN Loss: 4.178499698638916 | CLS Loss: 0.050153885036706924\n",
      "Epoch 23 / 200 | iteration 80 / 171 | Total Loss: 4.2193756103515625 | KNN Loss: 4.178321361541748 | CLS Loss: 0.04105440527200699\n",
      "Epoch 23 / 200 | iteration 90 / 171 | Total Loss: 4.200542449951172 | KNN Loss: 4.177738666534424 | CLS Loss: 0.022803891450166702\n",
      "Epoch 23 / 200 | iteration 100 / 171 | Total Loss: 4.208280086517334 | KNN Loss: 4.150237560272217 | CLS Loss: 0.0580427348613739\n",
      "Epoch 23 / 200 | iteration 110 / 171 | Total Loss: 4.231264114379883 | KNN Loss: 4.205456733703613 | CLS Loss: 0.02580742910504341\n",
      "Epoch 23 / 200 | iteration 120 / 171 | Total Loss: 4.235486030578613 | KNN Loss: 4.220157146453857 | CLS Loss: 0.015328967012465\n",
      "Epoch 23 / 200 | iteration 130 / 171 | Total Loss: 4.268587112426758 | KNN Loss: 4.192869663238525 | CLS Loss: 0.07571741938591003\n",
      "Epoch 23 / 200 | iteration 140 / 171 | Total Loss: 4.227042198181152 | KNN Loss: 4.20418643951416 | CLS Loss: 0.022855741903185844\n",
      "Epoch 23 / 200 | iteration 150 / 171 | Total Loss: 4.209231376647949 | KNN Loss: 4.174124717712402 | CLS Loss: 0.03510643541812897\n",
      "Epoch 23 / 200 | iteration 160 / 171 | Total Loss: 4.20059061050415 | KNN Loss: 4.179471015930176 | CLS Loss: 0.021119412034749985\n",
      "Epoch 23 / 200 | iteration 170 / 171 | Total Loss: 4.2498931884765625 | KNN Loss: 4.193807125091553 | CLS Loss: 0.05608607456088066\n",
      "Epoch: 023, Loss: 4.2345, Train: 0.9905, Valid: 0.9848, Best: 0.9848\n",
      "Epoch 24 / 200 | iteration 0 / 171 | Total Loss: 4.257296562194824 | KNN Loss: 4.235028266906738 | CLS Loss: 0.02226828783750534\n",
      "Epoch 24 / 200 | iteration 10 / 171 | Total Loss: 4.183259963989258 | KNN Loss: 4.170358657836914 | CLS Loss: 0.012901284731924534\n",
      "Epoch 24 / 200 | iteration 20 / 171 | Total Loss: 4.2374067306518555 | KNN Loss: 4.195427894592285 | CLS Loss: 0.04197880998253822\n",
      "Epoch 24 / 200 | iteration 30 / 171 | Total Loss: 4.2192511558532715 | KNN Loss: 4.174923419952393 | CLS Loss: 0.04432767629623413\n",
      "Epoch 24 / 200 | iteration 40 / 171 | Total Loss: 4.201862335205078 | KNN Loss: 4.162540435791016 | CLS Loss: 0.03932208567857742\n",
      "Epoch 24 / 200 | iteration 50 / 171 | Total Loss: 4.23348331451416 | KNN Loss: 4.195044994354248 | CLS Loss: 0.03843813017010689\n",
      "Epoch 24 / 200 | iteration 60 / 171 | Total Loss: 4.209078788757324 | KNN Loss: 4.1876349449157715 | CLS Loss: 0.021444056183099747\n",
      "Epoch 24 / 200 | iteration 70 / 171 | Total Loss: 4.294913291931152 | KNN Loss: 4.224076747894287 | CLS Loss: 0.0708363801240921\n",
      "Epoch 24 / 200 | iteration 80 / 171 | Total Loss: 4.223624229431152 | KNN Loss: 4.189732074737549 | CLS Loss: 0.03389221802353859\n",
      "Epoch 24 / 200 | iteration 90 / 171 | Total Loss: 4.260244846343994 | KNN Loss: 4.209293365478516 | CLS Loss: 0.050951313227415085\n",
      "Epoch 24 / 200 | iteration 100 / 171 | Total Loss: 4.208093643188477 | KNN Loss: 4.18809175491333 | CLS Loss: 0.020002001896500587\n",
      "Epoch 24 / 200 | iteration 110 / 171 | Total Loss: 4.2645368576049805 | KNN Loss: 4.2115702629089355 | CLS Loss: 0.05296681448817253\n",
      "Epoch 24 / 200 | iteration 120 / 171 | Total Loss: 4.2822065353393555 | KNN Loss: 4.227828502655029 | CLS Loss: 0.054377805441617966\n",
      "Epoch 24 / 200 | iteration 130 / 171 | Total Loss: 4.19528341293335 | KNN Loss: 4.17664909362793 | CLS Loss: 0.01863408274948597\n",
      "Epoch 24 / 200 | iteration 140 / 171 | Total Loss: 4.2352519035339355 | KNN Loss: 4.191607475280762 | CLS Loss: 0.04364461824297905\n",
      "Epoch 24 / 200 | iteration 150 / 171 | Total Loss: 4.205048084259033 | KNN Loss: 4.1950836181640625 | CLS Loss: 0.009964540600776672\n",
      "Epoch 24 / 200 | iteration 160 / 171 | Total Loss: 4.1896233558654785 | KNN Loss: 4.176820278167725 | CLS Loss: 0.012802978046238422\n",
      "Epoch 24 / 200 | iteration 170 / 171 | Total Loss: 4.258047580718994 | KNN Loss: 4.201387405395508 | CLS Loss: 0.056660059839487076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 024, Loss: 4.2356, Train: 0.9896, Valid: 0.9845, Best: 0.9848\n",
      "Epoch 25 / 200 | iteration 0 / 171 | Total Loss: 4.186246395111084 | KNN Loss: 4.164303779602051 | CLS Loss: 0.021942852064967155\n",
      "Epoch 25 / 200 | iteration 10 / 171 | Total Loss: 4.168835639953613 | KNN Loss: 4.150075912475586 | CLS Loss: 0.018759753555059433\n",
      "Epoch 25 / 200 | iteration 20 / 171 | Total Loss: 4.218719482421875 | KNN Loss: 4.181945323944092 | CLS Loss: 0.036774229258298874\n",
      "Epoch 25 / 200 | iteration 30 / 171 | Total Loss: 4.247707843780518 | KNN Loss: 4.198183059692383 | CLS Loss: 0.049524638801813126\n",
      "Epoch 25 / 200 | iteration 40 / 171 | Total Loss: 4.2186126708984375 | KNN Loss: 4.18963623046875 | CLS Loss: 0.02897651493549347\n",
      "Epoch 25 / 200 | iteration 50 / 171 | Total Loss: 4.2046709060668945 | KNN Loss: 4.158708095550537 | CLS Loss: 0.04596279561519623\n",
      "Epoch 25 / 200 | iteration 60 / 171 | Total Loss: 4.228909492492676 | KNN Loss: 4.199152946472168 | CLS Loss: 0.029756421223282814\n",
      "Epoch 25 / 200 | iteration 70 / 171 | Total Loss: 4.267114639282227 | KNN Loss: 4.200114727020264 | CLS Loss: 0.06699978560209274\n",
      "Epoch 25 / 200 | iteration 80 / 171 | Total Loss: 4.218707084655762 | KNN Loss: 4.165833473205566 | CLS Loss: 0.05287362262606621\n",
      "Epoch 25 / 200 | iteration 90 / 171 | Total Loss: 4.231142520904541 | KNN Loss: 4.2125444412231445 | CLS Loss: 0.01859830878674984\n",
      "Epoch 25 / 200 | iteration 100 / 171 | Total Loss: 4.265384197235107 | KNN Loss: 4.216921329498291 | CLS Loss: 0.0484628789126873\n",
      "Epoch 25 / 200 | iteration 110 / 171 | Total Loss: 4.24768590927124 | KNN Loss: 4.198509693145752 | CLS Loss: 0.049176447093486786\n",
      "Epoch 25 / 200 | iteration 120 / 171 | Total Loss: 4.214722633361816 | KNN Loss: 4.195936679840088 | CLS Loss: 0.0187857486307621\n",
      "Epoch 25 / 200 | iteration 130 / 171 | Total Loss: 4.218461036682129 | KNN Loss: 4.1929450035095215 | CLS Loss: 0.02551579661667347\n",
      "Epoch 25 / 200 | iteration 140 / 171 | Total Loss: 4.264141082763672 | KNN Loss: 4.2148871421813965 | CLS Loss: 0.04925406351685524\n",
      "Epoch 25 / 200 | iteration 150 / 171 | Total Loss: 4.211785316467285 | KNN Loss: 4.190758228302002 | CLS Loss: 0.021027175709605217\n",
      "Epoch 25 / 200 | iteration 160 / 171 | Total Loss: 4.251651287078857 | KNN Loss: 4.193802833557129 | CLS Loss: 0.05784834176301956\n",
      "Epoch 25 / 200 | iteration 170 / 171 | Total Loss: 4.198709964752197 | KNN Loss: 4.177689075469971 | CLS Loss: 0.02102097123861313\n",
      "Epoch: 025, Loss: 4.2301, Train: 0.9906, Valid: 0.9847, Best: 0.9848\n",
      "Epoch 26 / 200 | iteration 0 / 171 | Total Loss: 4.302457332611084 | KNN Loss: 4.266716003417969 | CLS Loss: 0.03574153780937195\n",
      "Epoch 26 / 200 | iteration 10 / 171 | Total Loss: 4.185756683349609 | KNN Loss: 4.165706157684326 | CLS Loss: 0.020050443708896637\n",
      "Epoch 26 / 200 | iteration 20 / 171 | Total Loss: 4.2287750244140625 | KNN Loss: 4.1898369789123535 | CLS Loss: 0.03893797844648361\n",
      "Epoch 26 / 200 | iteration 30 / 171 | Total Loss: 4.219566345214844 | KNN Loss: 4.185134410858154 | CLS Loss: 0.03443215414881706\n",
      "Epoch 26 / 200 | iteration 40 / 171 | Total Loss: 4.207609176635742 | KNN Loss: 4.184813022613525 | CLS Loss: 0.02279607765376568\n",
      "Epoch 26 / 200 | iteration 50 / 171 | Total Loss: 4.2418413162231445 | KNN Loss: 4.186720371246338 | CLS Loss: 0.05512077361345291\n",
      "Epoch 26 / 200 | iteration 60 / 171 | Total Loss: 4.250234127044678 | KNN Loss: 4.2082839012146 | CLS Loss: 0.04195040091872215\n",
      "Epoch 26 / 200 | iteration 70 / 171 | Total Loss: 4.233475685119629 | KNN Loss: 4.20242166519165 | CLS Loss: 0.03105379454791546\n",
      "Epoch 26 / 200 | iteration 80 / 171 | Total Loss: 4.205605983734131 | KNN Loss: 4.193236351013184 | CLS Loss: 0.01236971840262413\n",
      "Epoch 26 / 200 | iteration 90 / 171 | Total Loss: 4.222916603088379 | KNN Loss: 4.185712814331055 | CLS Loss: 0.037203624844551086\n",
      "Epoch 26 / 200 | iteration 100 / 171 | Total Loss: 4.181321620941162 | KNN Loss: 4.1663055419921875 | CLS Loss: 0.015016142278909683\n",
      "Epoch 26 / 200 | iteration 110 / 171 | Total Loss: 4.2298126220703125 | KNN Loss: 4.195361137390137 | CLS Loss: 0.03445132449269295\n",
      "Epoch 26 / 200 | iteration 120 / 171 | Total Loss: 4.284034252166748 | KNN Loss: 4.249841213226318 | CLS Loss: 0.034193068742752075\n",
      "Epoch 26 / 200 | iteration 130 / 171 | Total Loss: 4.2259202003479 | KNN Loss: 4.190255165100098 | CLS Loss: 0.03566514328122139\n",
      "Epoch 26 / 200 | iteration 140 / 171 | Total Loss: 4.187790870666504 | KNN Loss: 4.168368816375732 | CLS Loss: 0.019421905279159546\n",
      "Epoch 26 / 200 | iteration 150 / 171 | Total Loss: 4.2058634757995605 | KNN Loss: 4.157464981079102 | CLS Loss: 0.048398345708847046\n",
      "Epoch 26 / 200 | iteration 160 / 171 | Total Loss: 4.220637798309326 | KNN Loss: 4.17706298828125 | CLS Loss: 0.04357467219233513\n",
      "Epoch 26 / 200 | iteration 170 / 171 | Total Loss: 4.275946617126465 | KNN Loss: 4.234738349914551 | CLS Loss: 0.04120824858546257\n",
      "Epoch: 026, Loss: 4.2231, Train: 0.9907, Valid: 0.9843, Best: 0.9848\n",
      "Epoch 27 / 200 | iteration 0 / 171 | Total Loss: 4.236422538757324 | KNN Loss: 4.201356410980225 | CLS Loss: 0.03506630286574364\n",
      "Epoch 27 / 200 | iteration 10 / 171 | Total Loss: 4.2126030921936035 | KNN Loss: 4.179192543029785 | CLS Loss: 0.03341038525104523\n",
      "Epoch 27 / 200 | iteration 20 / 171 | Total Loss: 4.190969467163086 | KNN Loss: 4.173110485076904 | CLS Loss: 0.017858965322375298\n",
      "Epoch 27 / 200 | iteration 30 / 171 | Total Loss: 4.245993137359619 | KNN Loss: 4.178781986236572 | CLS Loss: 0.06721098721027374\n",
      "Epoch 27 / 200 | iteration 40 / 171 | Total Loss: 4.245593070983887 | KNN Loss: 4.203004360198975 | CLS Loss: 0.04258850961923599\n",
      "Epoch 27 / 200 | iteration 50 / 171 | Total Loss: 4.232784271240234 | KNN Loss: 4.206089019775391 | CLS Loss: 0.02669517695903778\n",
      "Epoch 27 / 200 | iteration 60 / 171 | Total Loss: 4.2241106033325195 | KNN Loss: 4.200827121734619 | CLS Loss: 0.023283701390028\n",
      "Epoch 27 / 200 | iteration 70 / 171 | Total Loss: 4.208895206451416 | KNN Loss: 4.176421642303467 | CLS Loss: 0.032473329454660416\n",
      "Epoch 27 / 200 | iteration 80 / 171 | Total Loss: 4.2216620445251465 | KNN Loss: 4.174045562744141 | CLS Loss: 0.04761650413274765\n",
      "Epoch 27 / 200 | iteration 90 / 171 | Total Loss: 4.32260274887085 | KNN Loss: 4.261894226074219 | CLS Loss: 0.060708656907081604\n",
      "Epoch 27 / 200 | iteration 100 / 171 | Total Loss: 4.236799240112305 | KNN Loss: 4.1779656410217285 | CLS Loss: 0.05883374065160751\n",
      "Epoch 27 / 200 | iteration 110 / 171 | Total Loss: 4.29868745803833 | KNN Loss: 4.250237464904785 | CLS Loss: 0.04844992607831955\n",
      "Epoch 27 / 200 | iteration 120 / 171 | Total Loss: 4.204205513000488 | KNN Loss: 4.1853227615356445 | CLS Loss: 0.01888282038271427\n",
      "Epoch 27 / 200 | iteration 130 / 171 | Total Loss: 4.2433671951293945 | KNN Loss: 4.195061683654785 | CLS Loss: 0.04830557852983475\n",
      "Epoch 27 / 200 | iteration 140 / 171 | Total Loss: 4.214293479919434 | KNN Loss: 4.168355941772461 | CLS Loss: 0.045937344431877136\n",
      "Epoch 27 / 200 | iteration 150 / 171 | Total Loss: 4.191406726837158 | KNN Loss: 4.171969413757324 | CLS Loss: 0.01943713054060936\n",
      "Epoch 27 / 200 | iteration 160 / 171 | Total Loss: 4.219386577606201 | KNN Loss: 4.17547607421875 | CLS Loss: 0.043910302221775055\n",
      "Epoch 27 / 200 | iteration 170 / 171 | Total Loss: 4.232740879058838 | KNN Loss: 4.203773021697998 | CLS Loss: 0.028967848047614098\n",
      "Epoch: 027, Loss: 4.2227, Train: 0.9888, Valid: 0.9830, Best: 0.9848\n",
      "Epoch 28 / 200 | iteration 0 / 171 | Total Loss: 4.227466583251953 | KNN Loss: 4.1733880043029785 | CLS Loss: 0.054078444838523865\n",
      "Epoch 28 / 200 | iteration 10 / 171 | Total Loss: 4.263567924499512 | KNN Loss: 4.225786209106445 | CLS Loss: 0.037781938910484314\n",
      "Epoch 28 / 200 | iteration 20 / 171 | Total Loss: 4.257557392120361 | KNN Loss: 4.228610992431641 | CLS Loss: 0.028946515172719955\n",
      "Epoch 28 / 200 | iteration 30 / 171 | Total Loss: 4.2201313972473145 | KNN Loss: 4.179624557495117 | CLS Loss: 0.04050663486123085\n",
      "Epoch 28 / 200 | iteration 40 / 171 | Total Loss: 4.244135856628418 | KNN Loss: 4.215415000915527 | CLS Loss: 0.028720756992697716\n",
      "Epoch 28 / 200 | iteration 50 / 171 | Total Loss: 4.193994045257568 | KNN Loss: 4.146109104156494 | CLS Loss: 0.047885097563266754\n",
      "Epoch 28 / 200 | iteration 60 / 171 | Total Loss: 4.223701000213623 | KNN Loss: 4.190580368041992 | CLS Loss: 0.033120863139629364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 / 200 | iteration 70 / 171 | Total Loss: 4.265418529510498 | KNN Loss: 4.1763596534729 | CLS Loss: 0.08905909210443497\n",
      "Epoch 28 / 200 | iteration 80 / 171 | Total Loss: 4.174824237823486 | KNN Loss: 4.154087543487549 | CLS Loss: 0.0207368154078722\n",
      "Epoch 28 / 200 | iteration 90 / 171 | Total Loss: 4.168670654296875 | KNN Loss: 4.1440277099609375 | CLS Loss: 0.02464302070438862\n",
      "Epoch 28 / 200 | iteration 100 / 171 | Total Loss: 4.200687408447266 | KNN Loss: 4.175668239593506 | CLS Loss: 0.02501937374472618\n",
      "Epoch 28 / 200 | iteration 110 / 171 | Total Loss: 4.261422157287598 | KNN Loss: 4.236147403717041 | CLS Loss: 0.025274906307458878\n",
      "Epoch 28 / 200 | iteration 120 / 171 | Total Loss: 4.206640720367432 | KNN Loss: 4.184180736541748 | CLS Loss: 0.022459914907813072\n",
      "Epoch 28 / 200 | iteration 130 / 171 | Total Loss: 4.206204414367676 | KNN Loss: 4.1753339767456055 | CLS Loss: 0.030870487913489342\n",
      "Epoch 28 / 200 | iteration 140 / 171 | Total Loss: 4.25545072555542 | KNN Loss: 4.1910834312438965 | CLS Loss: 0.06436736136674881\n",
      "Epoch 28 / 200 | iteration 150 / 171 | Total Loss: 4.204853057861328 | KNN Loss: 4.154481410980225 | CLS Loss: 0.050371453166007996\n",
      "Epoch 28 / 200 | iteration 160 / 171 | Total Loss: 4.2090253829956055 | KNN Loss: 4.182590484619141 | CLS Loss: 0.026434708386659622\n",
      "Epoch 28 / 200 | iteration 170 / 171 | Total Loss: 4.228757858276367 | KNN Loss: 4.169111728668213 | CLS Loss: 0.05964595451951027\n",
      "Epoch: 028, Loss: 4.2186, Train: 0.9917, Valid: 0.9863, Best: 0.9863\n",
      "Epoch 29 / 200 | iteration 0 / 171 | Total Loss: 4.217614650726318 | KNN Loss: 4.191878795623779 | CLS Loss: 0.02573595941066742\n",
      "Epoch 29 / 200 | iteration 10 / 171 | Total Loss: 4.180454254150391 | KNN Loss: 4.159152030944824 | CLS Loss: 0.02130204066634178\n",
      "Epoch 29 / 200 | iteration 20 / 171 | Total Loss: 4.197924613952637 | KNN Loss: 4.183774471282959 | CLS Loss: 0.014150263741612434\n",
      "Epoch 29 / 200 | iteration 30 / 171 | Total Loss: 4.211000919342041 | KNN Loss: 4.192774295806885 | CLS Loss: 0.018226394429802895\n",
      "Epoch 29 / 200 | iteration 40 / 171 | Total Loss: 4.260822772979736 | KNN Loss: 4.227928161621094 | CLS Loss: 0.03289471194148064\n",
      "Epoch 29 / 200 | iteration 50 / 171 | Total Loss: 4.211563587188721 | KNN Loss: 4.191653728485107 | CLS Loss: 0.019909946247935295\n",
      "Epoch 29 / 200 | iteration 60 / 171 | Total Loss: 4.216964244842529 | KNN Loss: 4.1918864250183105 | CLS Loss: 0.02507764659821987\n",
      "Epoch 29 / 200 | iteration 70 / 171 | Total Loss: 4.246617317199707 | KNN Loss: 4.1884236335754395 | CLS Loss: 0.05819346755743027\n",
      "Epoch 29 / 200 | iteration 80 / 171 | Total Loss: 4.232859134674072 | KNN Loss: 4.197412967681885 | CLS Loss: 0.03544621542096138\n",
      "Epoch 29 / 200 | iteration 90 / 171 | Total Loss: 4.217411041259766 | KNN Loss: 4.1892218589782715 | CLS Loss: 0.028189416974782944\n",
      "Epoch 29 / 200 | iteration 100 / 171 | Total Loss: 4.2259440422058105 | KNN Loss: 4.189023494720459 | CLS Loss: 0.03692077100276947\n",
      "Epoch 29 / 200 | iteration 110 / 171 | Total Loss: 4.206079959869385 | KNN Loss: 4.154835224151611 | CLS Loss: 0.05124484747648239\n",
      "Epoch 29 / 200 | iteration 120 / 171 | Total Loss: 4.204645156860352 | KNN Loss: 4.167200088500977 | CLS Loss: 0.03744497522711754\n",
      "Epoch 29 / 200 | iteration 130 / 171 | Total Loss: 4.258970260620117 | KNN Loss: 4.193614959716797 | CLS Loss: 0.06535545736551285\n",
      "Epoch 29 / 200 | iteration 140 / 171 | Total Loss: 4.181839466094971 | KNN Loss: 4.1621928215026855 | CLS Loss: 0.019646435976028442\n",
      "Epoch 29 / 200 | iteration 150 / 171 | Total Loss: 4.1942925453186035 | KNN Loss: 4.1751275062561035 | CLS Loss: 0.019164975732564926\n",
      "Epoch 29 / 200 | iteration 160 / 171 | Total Loss: 4.247708797454834 | KNN Loss: 4.202509880065918 | CLS Loss: 0.04519902914762497\n",
      "Epoch 29 / 200 | iteration 170 / 171 | Total Loss: 4.23137903213501 | KNN Loss: 4.197306156158447 | CLS Loss: 0.0340728834271431\n",
      "Epoch: 029, Loss: 4.2220, Train: 0.9908, Valid: 0.9847, Best: 0.9863\n",
      "Epoch 30 / 200 | iteration 0 / 171 | Total Loss: 4.274338245391846 | KNN Loss: 4.222378253936768 | CLS Loss: 0.05195993557572365\n",
      "Epoch 30 / 200 | iteration 10 / 171 | Total Loss: 4.208752155303955 | KNN Loss: 4.183093547821045 | CLS Loss: 0.025658655911684036\n",
      "Epoch 30 / 200 | iteration 20 / 171 | Total Loss: 4.188846588134766 | KNN Loss: 4.164700984954834 | CLS Loss: 0.024145808070898056\n",
      "Epoch 30 / 200 | iteration 30 / 171 | Total Loss: 4.196120738983154 | KNN Loss: 4.158688545227051 | CLS Loss: 0.03743201121687889\n",
      "Epoch 30 / 200 | iteration 40 / 171 | Total Loss: 4.205114364624023 | KNN Loss: 4.178445816040039 | CLS Loss: 0.02666866034269333\n",
      "Epoch 30 / 200 | iteration 50 / 171 | Total Loss: 4.201706409454346 | KNN Loss: 4.177705764770508 | CLS Loss: 0.0240008607506752\n",
      "Epoch 30 / 200 | iteration 60 / 171 | Total Loss: 4.216599941253662 | KNN Loss: 4.173336505889893 | CLS Loss: 0.04326338693499565\n",
      "Epoch 30 / 200 | iteration 70 / 171 | Total Loss: 4.210438251495361 | KNN Loss: 4.191323757171631 | CLS Loss: 0.01911439374089241\n",
      "Epoch 30 / 200 | iteration 80 / 171 | Total Loss: 4.263987064361572 | KNN Loss: 4.194844722747803 | CLS Loss: 0.06914228200912476\n",
      "Epoch 30 / 200 | iteration 90 / 171 | Total Loss: 4.267117023468018 | KNN Loss: 4.230071544647217 | CLS Loss: 0.037045545876026154\n",
      "Epoch 30 / 200 | iteration 100 / 171 | Total Loss: 4.197616100311279 | KNN Loss: 4.179239273071289 | CLS Loss: 0.018376676365733147\n",
      "Epoch 30 / 200 | iteration 110 / 171 | Total Loss: 4.221533298492432 | KNN Loss: 4.18157434463501 | CLS Loss: 0.03995904698967934\n",
      "Epoch 30 / 200 | iteration 120 / 171 | Total Loss: 4.1889328956604 | KNN Loss: 4.159835338592529 | CLS Loss: 0.029097722843289375\n",
      "Epoch 30 / 200 | iteration 130 / 171 | Total Loss: 4.175075531005859 | KNN Loss: 4.151686191558838 | CLS Loss: 0.023389557376503944\n",
      "Epoch 30 / 200 | iteration 140 / 171 | Total Loss: 4.2355852127075195 | KNN Loss: 4.19744348526001 | CLS Loss: 0.038141652941703796\n",
      "Epoch 30 / 200 | iteration 150 / 171 | Total Loss: 4.206459045410156 | KNN Loss: 4.177746295928955 | CLS Loss: 0.028712604194879532\n",
      "Epoch 30 / 200 | iteration 160 / 171 | Total Loss: 4.247436046600342 | KNN Loss: 4.222029685974121 | CLS Loss: 0.025406159460544586\n",
      "Epoch 30 / 200 | iteration 170 / 171 | Total Loss: 4.212482929229736 | KNN Loss: 4.175247669219971 | CLS Loss: 0.0372353196144104\n",
      "Epoch: 030, Loss: 4.2161, Train: 0.9916, Valid: 0.9851, Best: 0.9863\n",
      "Epoch 31 / 200 | iteration 0 / 171 | Total Loss: 4.187206268310547 | KNN Loss: 4.175556182861328 | CLS Loss: 0.011650098487734795\n",
      "Epoch 31 / 200 | iteration 10 / 171 | Total Loss: 4.216040134429932 | KNN Loss: 4.175443172454834 | CLS Loss: 0.04059680178761482\n",
      "Epoch 31 / 200 | iteration 20 / 171 | Total Loss: 4.232752323150635 | KNN Loss: 4.183751583099365 | CLS Loss: 0.049000512808561325\n",
      "Epoch 31 / 200 | iteration 30 / 171 | Total Loss: 4.212329864501953 | KNN Loss: 4.177006721496582 | CLS Loss: 0.03532326966524124\n",
      "Epoch 31 / 200 | iteration 40 / 171 | Total Loss: 4.184340476989746 | KNN Loss: 4.145936012268066 | CLS Loss: 0.038404542952775955\n",
      "Epoch 31 / 200 | iteration 50 / 171 | Total Loss: 4.246650218963623 | KNN Loss: 4.197251796722412 | CLS Loss: 0.049398455768823624\n",
      "Epoch 31 / 200 | iteration 60 / 171 | Total Loss: 4.232106685638428 | KNN Loss: 4.204120635986328 | CLS Loss: 0.027985993772745132\n",
      "Epoch 31 / 200 | iteration 70 / 171 | Total Loss: 4.1908488273620605 | KNN Loss: 4.165933132171631 | CLS Loss: 0.024915721267461777\n",
      "Epoch 31 / 200 | iteration 80 / 171 | Total Loss: 4.198773384094238 | KNN Loss: 4.185338497161865 | CLS Loss: 0.013434709049761295\n",
      "Epoch 31 / 200 | iteration 90 / 171 | Total Loss: 4.22929048538208 | KNN Loss: 4.204381465911865 | CLS Loss: 0.024909090250730515\n",
      "Epoch 31 / 200 | iteration 100 / 171 | Total Loss: 4.211946964263916 | KNN Loss: 4.163717746734619 | CLS Loss: 0.0482291579246521\n",
      "Epoch 31 / 200 | iteration 110 / 171 | Total Loss: 4.220419883728027 | KNN Loss: 4.173670291900635 | CLS Loss: 0.046749357134103775\n",
      "Epoch 31 / 200 | iteration 120 / 171 | Total Loss: 4.2228264808654785 | KNN Loss: 4.189223289489746 | CLS Loss: 0.033603109419345856\n",
      "Epoch 31 / 200 | iteration 130 / 171 | Total Loss: 4.25077486038208 | KNN Loss: 4.210563659667969 | CLS Loss: 0.0402112677693367\n",
      "Epoch 31 / 200 | iteration 140 / 171 | Total Loss: 4.212437152862549 | KNN Loss: 4.170116424560547 | CLS Loss: 0.04232050105929375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 / 200 | iteration 150 / 171 | Total Loss: 4.227519512176514 | KNN Loss: 4.202352523803711 | CLS Loss: 0.025166820734739304\n",
      "Epoch 31 / 200 | iteration 160 / 171 | Total Loss: 4.203866004943848 | KNN Loss: 4.190859317779541 | CLS Loss: 0.013006502762436867\n",
      "Epoch 31 / 200 | iteration 170 / 171 | Total Loss: 4.2616729736328125 | KNN Loss: 4.1755523681640625 | CLS Loss: 0.08612081408500671\n",
      "Epoch: 031, Loss: 4.2113, Train: 0.9911, Valid: 0.9845, Best: 0.9863\n",
      "Epoch 32 / 200 | iteration 0 / 171 | Total Loss: 4.196518898010254 | KNN Loss: 4.182875156402588 | CLS Loss: 0.013643544167280197\n",
      "Epoch 32 / 200 | iteration 10 / 171 | Total Loss: 4.194110870361328 | KNN Loss: 4.180301189422607 | CLS Loss: 0.013809784315526485\n",
      "Epoch 32 / 200 | iteration 20 / 171 | Total Loss: 4.194121360778809 | KNN Loss: 4.173984050750732 | CLS Loss: 0.020137427374720573\n",
      "Epoch 32 / 200 | iteration 30 / 171 | Total Loss: 4.182323932647705 | KNN Loss: 4.1590962409973145 | CLS Loss: 0.023227810859680176\n",
      "Epoch 32 / 200 | iteration 40 / 171 | Total Loss: 4.192208766937256 | KNN Loss: 4.183322429656982 | CLS Loss: 0.008886569179594517\n",
      "Epoch 32 / 200 | iteration 50 / 171 | Total Loss: 4.186977386474609 | KNN Loss: 4.159976482391357 | CLS Loss: 0.027000941336154938\n",
      "Epoch 32 / 200 | iteration 60 / 171 | Total Loss: 4.167655944824219 | KNN Loss: 4.160104751586914 | CLS Loss: 0.007551314774900675\n",
      "Epoch 32 / 200 | iteration 70 / 171 | Total Loss: 4.2540764808654785 | KNN Loss: 4.220188140869141 | CLS Loss: 0.03388827666640282\n",
      "Epoch 32 / 200 | iteration 80 / 171 | Total Loss: 4.210441589355469 | KNN Loss: 4.173086166381836 | CLS Loss: 0.03735538572072983\n",
      "Epoch 32 / 200 | iteration 90 / 171 | Total Loss: 4.214787006378174 | KNN Loss: 4.1794352531433105 | CLS Loss: 0.035351965576410294\n",
      "Epoch 32 / 200 | iteration 100 / 171 | Total Loss: 4.254446983337402 | KNN Loss: 4.198347091674805 | CLS Loss: 0.05609966069459915\n",
      "Epoch 32 / 200 | iteration 110 / 171 | Total Loss: 4.233305931091309 | KNN Loss: 4.1785359382629395 | CLS Loss: 0.054770104587078094\n",
      "Epoch 32 / 200 | iteration 120 / 171 | Total Loss: 4.1929731369018555 | KNN Loss: 4.154877662658691 | CLS Loss: 0.03809548541903496\n",
      "Epoch 32 / 200 | iteration 130 / 171 | Total Loss: 4.2170867919921875 | KNN Loss: 4.183830261230469 | CLS Loss: 0.03325638920068741\n",
      "Epoch 32 / 200 | iteration 140 / 171 | Total Loss: 4.189782619476318 | KNN Loss: 4.169287204742432 | CLS Loss: 0.020495513454079628\n",
      "Epoch 32 / 200 | iteration 150 / 171 | Total Loss: 4.226241588592529 | KNN Loss: 4.182651519775391 | CLS Loss: 0.043590255081653595\n",
      "Epoch 32 / 200 | iteration 160 / 171 | Total Loss: 4.223937034606934 | KNN Loss: 4.179983615875244 | CLS Loss: 0.043953508138656616\n",
      "Epoch 32 / 200 | iteration 170 / 171 | Total Loss: 4.23263692855835 | KNN Loss: 4.204848766326904 | CLS Loss: 0.02778799645602703\n",
      "Epoch: 032, Loss: 4.2095, Train: 0.9924, Valid: 0.9860, Best: 0.9863\n",
      "Epoch 33 / 200 | iteration 0 / 171 | Total Loss: 4.226873874664307 | KNN Loss: 4.186567306518555 | CLS Loss: 0.040306802839040756\n",
      "Epoch 33 / 200 | iteration 10 / 171 | Total Loss: 4.202853202819824 | KNN Loss: 4.187313556671143 | CLS Loss: 0.015539762564003468\n",
      "Epoch 33 / 200 | iteration 20 / 171 | Total Loss: 4.232848167419434 | KNN Loss: 4.195864677429199 | CLS Loss: 0.03698350489139557\n",
      "Epoch 33 / 200 | iteration 30 / 171 | Total Loss: 4.181724548339844 | KNN Loss: 4.125834941864014 | CLS Loss: 0.05588983744382858\n",
      "Epoch 33 / 200 | iteration 40 / 171 | Total Loss: 4.188305854797363 | KNN Loss: 4.1454854011535645 | CLS Loss: 0.042820315808057785\n",
      "Epoch 33 / 200 | iteration 50 / 171 | Total Loss: 4.19662618637085 | KNN Loss: 4.175125598907471 | CLS Loss: 0.02150050736963749\n",
      "Epoch 33 / 200 | iteration 60 / 171 | Total Loss: 4.24338960647583 | KNN Loss: 4.1971917152404785 | CLS Loss: 0.0461978018283844\n",
      "Epoch 33 / 200 | iteration 70 / 171 | Total Loss: 4.2942304611206055 | KNN Loss: 4.2167439460754395 | CLS Loss: 0.07748662680387497\n",
      "Epoch 33 / 200 | iteration 80 / 171 | Total Loss: 4.191924095153809 | KNN Loss: 4.171802997589111 | CLS Loss: 0.020121298730373383\n",
      "Epoch 33 / 200 | iteration 90 / 171 | Total Loss: 4.169264316558838 | KNN Loss: 4.137515544891357 | CLS Loss: 0.03174898773431778\n",
      "Epoch 33 / 200 | iteration 100 / 171 | Total Loss: 4.223875045776367 | KNN Loss: 4.211087226867676 | CLS Loss: 0.012787946499884129\n",
      "Epoch 33 / 200 | iteration 110 / 171 | Total Loss: 4.174917221069336 | KNN Loss: 4.156181335449219 | CLS Loss: 0.018735935911536217\n",
      "Epoch 33 / 200 | iteration 120 / 171 | Total Loss: 4.215048313140869 | KNN Loss: 4.167220115661621 | CLS Loss: 0.04782838374376297\n",
      "Epoch 33 / 200 | iteration 130 / 171 | Total Loss: 4.19223690032959 | KNN Loss: 4.166493892669678 | CLS Loss: 0.025742819532752037\n",
      "Epoch 33 / 200 | iteration 140 / 171 | Total Loss: 4.205329895019531 | KNN Loss: 4.187183856964111 | CLS Loss: 0.01814580149948597\n",
      "Epoch 33 / 200 | iteration 150 / 171 | Total Loss: 4.224084854125977 | KNN Loss: 4.2002058029174805 | CLS Loss: 0.023878933861851692\n",
      "Epoch 33 / 200 | iteration 160 / 171 | Total Loss: 4.231428623199463 | KNN Loss: 4.183189868927002 | CLS Loss: 0.04823876544833183\n",
      "Epoch 33 / 200 | iteration 170 / 171 | Total Loss: 4.2150492668151855 | KNN Loss: 4.184259414672852 | CLS Loss: 0.030790064483880997\n",
      "Epoch: 033, Loss: 4.2037, Train: 0.9926, Valid: 0.9857, Best: 0.9863\n",
      "Epoch 34 / 200 | iteration 0 / 171 | Total Loss: 4.1649346351623535 | KNN Loss: 4.157765865325928 | CLS Loss: 0.007168825715780258\n",
      "Epoch 34 / 200 | iteration 10 / 171 | Total Loss: 4.246085166931152 | KNN Loss: 4.2121405601501465 | CLS Loss: 0.033944517374038696\n",
      "Epoch 34 / 200 | iteration 20 / 171 | Total Loss: 4.14766263961792 | KNN Loss: 4.131436347961426 | CLS Loss: 0.016226405277848244\n",
      "Epoch 34 / 200 | iteration 30 / 171 | Total Loss: 4.169247627258301 | KNN Loss: 4.146948337554932 | CLS Loss: 0.02229924499988556\n",
      "Epoch 34 / 200 | iteration 40 / 171 | Total Loss: 4.1963629722595215 | KNN Loss: 4.174574851989746 | CLS Loss: 0.021788161247968674\n",
      "Epoch 34 / 200 | iteration 50 / 171 | Total Loss: 4.220948219299316 | KNN Loss: 4.19666862487793 | CLS Loss: 0.024279575794935226\n",
      "Epoch 34 / 200 | iteration 60 / 171 | Total Loss: 4.19356107711792 | KNN Loss: 4.146885871887207 | CLS Loss: 0.046675194054841995\n",
      "Epoch 34 / 200 | iteration 70 / 171 | Total Loss: 4.186760425567627 | KNN Loss: 4.152364253997803 | CLS Loss: 0.034396298229694366\n",
      "Epoch 34 / 200 | iteration 80 / 171 | Total Loss: 4.239462375640869 | KNN Loss: 4.200924873352051 | CLS Loss: 0.038537316024303436\n",
      "Epoch 34 / 200 | iteration 90 / 171 | Total Loss: 4.216185092926025 | KNN Loss: 4.158797264099121 | CLS Loss: 0.05738767236471176\n",
      "Epoch 34 / 200 | iteration 100 / 171 | Total Loss: 4.211185455322266 | KNN Loss: 4.182808876037598 | CLS Loss: 0.028376808390021324\n",
      "Epoch 34 / 200 | iteration 110 / 171 | Total Loss: 4.191173553466797 | KNN Loss: 4.174645900726318 | CLS Loss: 0.01652783341705799\n",
      "Epoch 34 / 200 | iteration 120 / 171 | Total Loss: 4.212031364440918 | KNN Loss: 4.186476707458496 | CLS Loss: 0.025554705411195755\n",
      "Epoch 34 / 200 | iteration 130 / 171 | Total Loss: 4.2149457931518555 | KNN Loss: 4.181981563568115 | CLS Loss: 0.03296400606632233\n",
      "Epoch 34 / 200 | iteration 140 / 171 | Total Loss: 4.208532333374023 | KNN Loss: 4.19165563583374 | CLS Loss: 0.016876650974154472\n",
      "Epoch 34 / 200 | iteration 150 / 171 | Total Loss: 4.183060169219971 | KNN Loss: 4.143744945526123 | CLS Loss: 0.039315398782491684\n",
      "Epoch 34 / 200 | iteration 160 / 171 | Total Loss: 4.192739963531494 | KNN Loss: 4.1631879806518555 | CLS Loss: 0.029551967978477478\n",
      "Epoch 34 / 200 | iteration 170 / 171 | Total Loss: 4.200911998748779 | KNN Loss: 4.181518077850342 | CLS Loss: 0.019394122064113617\n",
      "Epoch: 034, Loss: 4.2060, Train: 0.9900, Valid: 0.9836, Best: 0.9863\n",
      "Epoch 35 / 200 | iteration 0 / 171 | Total Loss: 4.170731544494629 | KNN Loss: 4.151316165924072 | CLS Loss: 0.01941530965268612\n",
      "Epoch 35 / 200 | iteration 10 / 171 | Total Loss: 4.19909143447876 | KNN Loss: 4.15028715133667 | CLS Loss: 0.04880417138338089\n",
      "Epoch 35 / 200 | iteration 20 / 171 | Total Loss: 4.205293655395508 | KNN Loss: 4.173388957977295 | CLS Loss: 0.031904738396406174\n",
      "Epoch 35 / 200 | iteration 30 / 171 | Total Loss: 4.220497131347656 | KNN Loss: 4.157583713531494 | CLS Loss: 0.06291333585977554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 / 200 | iteration 40 / 171 | Total Loss: 4.200634479522705 | KNN Loss: 4.173842906951904 | CLS Loss: 0.026791518554091454\n",
      "Epoch 35 / 200 | iteration 50 / 171 | Total Loss: 4.175117492675781 | KNN Loss: 4.145471096038818 | CLS Loss: 0.029646478593349457\n",
      "Epoch 35 / 200 | iteration 60 / 171 | Total Loss: 4.202883720397949 | KNN Loss: 4.1836724281311035 | CLS Loss: 0.019211314618587494\n",
      "Epoch 35 / 200 | iteration 70 / 171 | Total Loss: 4.230337142944336 | KNN Loss: 4.192715167999268 | CLS Loss: 0.03762213513255119\n",
      "Epoch 35 / 200 | iteration 80 / 171 | Total Loss: 4.137632846832275 | KNN Loss: 4.116912841796875 | CLS Loss: 0.02072017453610897\n",
      "Epoch 35 / 200 | iteration 90 / 171 | Total Loss: 4.1775054931640625 | KNN Loss: 4.134088039398193 | CLS Loss: 0.043417561799287796\n",
      "Epoch 35 / 200 | iteration 100 / 171 | Total Loss: 4.188200950622559 | KNN Loss: 4.172223091125488 | CLS Loss: 0.01597805880010128\n",
      "Epoch 35 / 200 | iteration 110 / 171 | Total Loss: 4.187870979309082 | KNN Loss: 4.163764476776123 | CLS Loss: 0.024106329306960106\n",
      "Epoch 35 / 200 | iteration 120 / 171 | Total Loss: 4.233448028564453 | KNN Loss: 4.1841840744018555 | CLS Loss: 0.049263834953308105\n",
      "Epoch 35 / 200 | iteration 130 / 171 | Total Loss: 4.229525566101074 | KNN Loss: 4.1796159744262695 | CLS Loss: 0.04990971460938454\n",
      "Epoch 35 / 200 | iteration 140 / 171 | Total Loss: 4.2033467292785645 | KNN Loss: 4.16751766204834 | CLS Loss: 0.035829126834869385\n",
      "Epoch 35 / 200 | iteration 150 / 171 | Total Loss: 4.241809368133545 | KNN Loss: 4.2052202224731445 | CLS Loss: 0.036589205265045166\n",
      "Epoch 35 / 200 | iteration 160 / 171 | Total Loss: 4.178224563598633 | KNN Loss: 4.1501078605651855 | CLS Loss: 0.028116827830672264\n",
      "Epoch 35 / 200 | iteration 170 / 171 | Total Loss: 4.207797050476074 | KNN Loss: 4.189032554626465 | CLS Loss: 0.018764406442642212\n",
      "Epoch: 035, Loss: 4.1996, Train: 0.9927, Valid: 0.9863, Best: 0.9863\n",
      "Epoch 36 / 200 | iteration 0 / 171 | Total Loss: 4.185796737670898 | KNN Loss: 4.1656694412231445 | CLS Loss: 0.020127521827816963\n",
      "Epoch 36 / 200 | iteration 10 / 171 | Total Loss: 4.2359137535095215 | KNN Loss: 4.1913251876831055 | CLS Loss: 0.04458845779299736\n",
      "Epoch 36 / 200 | iteration 20 / 171 | Total Loss: 4.150506496429443 | KNN Loss: 4.120045185089111 | CLS Loss: 0.030461527407169342\n",
      "Epoch 36 / 200 | iteration 30 / 171 | Total Loss: 4.167301177978516 | KNN Loss: 4.152476787567139 | CLS Loss: 0.014824306592345238\n",
      "Epoch 36 / 200 | iteration 40 / 171 | Total Loss: 4.23049783706665 | KNN Loss: 4.1894145011901855 | CLS Loss: 0.041083551943302155\n",
      "Epoch 36 / 200 | iteration 50 / 171 | Total Loss: 4.183121681213379 | KNN Loss: 4.125475883483887 | CLS Loss: 0.05764567106962204\n",
      "Epoch 36 / 200 | iteration 60 / 171 | Total Loss: 4.235300064086914 | KNN Loss: 4.184823036193848 | CLS Loss: 0.050476834177970886\n",
      "Epoch 36 / 200 | iteration 70 / 171 | Total Loss: 4.248565673828125 | KNN Loss: 4.190802574157715 | CLS Loss: 0.057763244956731796\n",
      "Epoch 36 / 200 | iteration 80 / 171 | Total Loss: 4.257837772369385 | KNN Loss: 4.198415756225586 | CLS Loss: 0.059421997517347336\n",
      "Epoch 36 / 200 | iteration 90 / 171 | Total Loss: 4.163161277770996 | KNN Loss: 4.150293350219727 | CLS Loss: 0.01286790519952774\n",
      "Epoch 36 / 200 | iteration 100 / 171 | Total Loss: 4.204708099365234 | KNN Loss: 4.185433864593506 | CLS Loss: 0.019274329766631126\n",
      "Epoch 36 / 200 | iteration 110 / 171 | Total Loss: 4.1995015144348145 | KNN Loss: 4.1601786613464355 | CLS Loss: 0.03932306542992592\n",
      "Epoch 36 / 200 | iteration 120 / 171 | Total Loss: 4.231518268585205 | KNN Loss: 4.187424659729004 | CLS Loss: 0.04409347102046013\n",
      "Epoch 36 / 200 | iteration 130 / 171 | Total Loss: 4.228084087371826 | KNN Loss: 4.1968278884887695 | CLS Loss: 0.031256042420864105\n",
      "Epoch 36 / 200 | iteration 140 / 171 | Total Loss: 4.17852258682251 | KNN Loss: 4.172041416168213 | CLS Loss: 0.00648127356544137\n",
      "Epoch 36 / 200 | iteration 150 / 171 | Total Loss: 4.179417610168457 | KNN Loss: 4.156491756439209 | CLS Loss: 0.022925714030861855\n",
      "Epoch 36 / 200 | iteration 160 / 171 | Total Loss: 4.19038200378418 | KNN Loss: 4.1660308837890625 | CLS Loss: 0.024351069703698158\n",
      "Epoch 36 / 200 | iteration 170 / 171 | Total Loss: 4.202930927276611 | KNN Loss: 4.162879467010498 | CLS Loss: 0.04005131125450134\n",
      "Epoch: 036, Loss: 4.2000, Train: 0.9926, Valid: 0.9857, Best: 0.9863\n",
      "Epoch 37 / 200 | iteration 0 / 171 | Total Loss: 4.207867622375488 | KNN Loss: 4.175538063049316 | CLS Loss: 0.032329559326171875\n",
      "Epoch 37 / 200 | iteration 10 / 171 | Total Loss: 4.197328567504883 | KNN Loss: 4.169179916381836 | CLS Loss: 0.0281484667211771\n",
      "Epoch 37 / 200 | iteration 20 / 171 | Total Loss: 4.167744159698486 | KNN Loss: 4.144469738006592 | CLS Loss: 0.023274270817637444\n",
      "Epoch 37 / 200 | iteration 30 / 171 | Total Loss: 4.237979412078857 | KNN Loss: 4.190643787384033 | CLS Loss: 0.04733550548553467\n",
      "Epoch 37 / 200 | iteration 40 / 171 | Total Loss: 4.163331031799316 | KNN Loss: 4.129373550415039 | CLS Loss: 0.03395726904273033\n",
      "Epoch 37 / 200 | iteration 50 / 171 | Total Loss: 4.190285682678223 | KNN Loss: 4.164670944213867 | CLS Loss: 0.025614745914936066\n",
      "Epoch 37 / 200 | iteration 60 / 171 | Total Loss: 4.2167558670043945 | KNN Loss: 4.179593086242676 | CLS Loss: 0.03716255724430084\n",
      "Epoch 37 / 200 | iteration 70 / 171 | Total Loss: 4.161565780639648 | KNN Loss: 4.136868953704834 | CLS Loss: 0.024696731939911842\n",
      "Epoch 37 / 200 | iteration 80 / 171 | Total Loss: 4.206971168518066 | KNN Loss: 4.176526069641113 | CLS Loss: 0.030444880947470665\n",
      "Epoch 37 / 200 | iteration 90 / 171 | Total Loss: 4.18196964263916 | KNN Loss: 4.168262481689453 | CLS Loss: 0.013707146979868412\n",
      "Epoch 37 / 200 | iteration 100 / 171 | Total Loss: 4.203946590423584 | KNN Loss: 4.188916206359863 | CLS Loss: 0.015030577778816223\n",
      "Epoch 37 / 200 | iteration 110 / 171 | Total Loss: 4.202055931091309 | KNN Loss: 4.183502197265625 | CLS Loss: 0.01855379529297352\n",
      "Epoch 37 / 200 | iteration 120 / 171 | Total Loss: 4.227783203125 | KNN Loss: 4.21711540222168 | CLS Loss: 0.010667629539966583\n",
      "Epoch 37 / 200 | iteration 130 / 171 | Total Loss: 4.197638988494873 | KNN Loss: 4.18375825881958 | CLS Loss: 0.013880825601518154\n",
      "Epoch 37 / 200 | iteration 140 / 171 | Total Loss: 4.214198112487793 | KNN Loss: 4.1568169593811035 | CLS Loss: 0.05738098919391632\n",
      "Epoch 37 / 200 | iteration 150 / 171 | Total Loss: 4.194282531738281 | KNN Loss: 4.165291786193848 | CLS Loss: 0.028990918770432472\n",
      "Epoch 37 / 200 | iteration 160 / 171 | Total Loss: 4.206955432891846 | KNN Loss: 4.171192646026611 | CLS Loss: 0.03576264902949333\n",
      "Epoch 37 / 200 | iteration 170 / 171 | Total Loss: 4.220808506011963 | KNN Loss: 4.18070125579834 | CLS Loss: 0.040107473731040955\n",
      "Epoch: 037, Loss: 4.2073, Train: 0.9929, Valid: 0.9863, Best: 0.9863\n",
      "Epoch 38 / 200 | iteration 0 / 171 | Total Loss: 4.208611488342285 | KNN Loss: 4.16379451751709 | CLS Loss: 0.04481705278158188\n",
      "Epoch 38 / 200 | iteration 10 / 171 | Total Loss: 4.182830810546875 | KNN Loss: 4.167691230773926 | CLS Loss: 0.015139710158109665\n",
      "Epoch 38 / 200 | iteration 20 / 171 | Total Loss: 4.279665946960449 | KNN Loss: 4.2159423828125 | CLS Loss: 0.0637236088514328\n",
      "Epoch 38 / 200 | iteration 30 / 171 | Total Loss: 4.191321849822998 | KNN Loss: 4.176777362823486 | CLS Loss: 0.01454458199441433\n",
      "Epoch 38 / 200 | iteration 40 / 171 | Total Loss: 4.216787815093994 | KNN Loss: 4.1917290687561035 | CLS Loss: 0.025058655068278313\n",
      "Epoch 38 / 200 | iteration 50 / 171 | Total Loss: 4.201713562011719 | KNN Loss: 4.1741461753845215 | CLS Loss: 0.02756735496222973\n",
      "Epoch 38 / 200 | iteration 60 / 171 | Total Loss: 4.255205154418945 | KNN Loss: 4.228277206420898 | CLS Loss: 0.02692796289920807\n",
      "Epoch 38 / 200 | iteration 70 / 171 | Total Loss: 4.206427097320557 | KNN Loss: 4.188709259033203 | CLS Loss: 0.017717629671096802\n",
      "Epoch 38 / 200 | iteration 80 / 171 | Total Loss: 4.233249187469482 | KNN Loss: 4.206186294555664 | CLS Loss: 0.02706289291381836\n",
      "Epoch 38 / 200 | iteration 90 / 171 | Total Loss: 4.1933817863464355 | KNN Loss: 4.179609775543213 | CLS Loss: 0.01377181801944971\n",
      "Epoch 38 / 200 | iteration 100 / 171 | Total Loss: 4.144865989685059 | KNN Loss: 4.118258953094482 | CLS Loss: 0.026606881991028786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 / 200 | iteration 110 / 171 | Total Loss: 4.208645343780518 | KNN Loss: 4.191484451293945 | CLS Loss: 0.01716066710650921\n",
      "Epoch 38 / 200 | iteration 120 / 171 | Total Loss: 4.175397872924805 | KNN Loss: 4.131710529327393 | CLS Loss: 0.04368741437792778\n",
      "Epoch 38 / 200 | iteration 130 / 171 | Total Loss: 4.2022786140441895 | KNN Loss: 4.165480613708496 | CLS Loss: 0.03679820895195007\n",
      "Epoch 38 / 200 | iteration 140 / 171 | Total Loss: 4.199082851409912 | KNN Loss: 4.186020374298096 | CLS Loss: 0.013062701560556889\n",
      "Epoch 38 / 200 | iteration 150 / 171 | Total Loss: 4.1941046714782715 | KNN Loss: 4.159545421600342 | CLS Loss: 0.03455926850438118\n",
      "Epoch 38 / 200 | iteration 160 / 171 | Total Loss: 4.205271244049072 | KNN Loss: 4.168568134307861 | CLS Loss: 0.036703210324048996\n",
      "Epoch 38 / 200 | iteration 170 / 171 | Total Loss: 4.1857008934021 | KNN Loss: 4.1565446853637695 | CLS Loss: 0.02915632352232933\n",
      "Epoch: 038, Loss: 4.1978, Train: 0.9924, Valid: 0.9856, Best: 0.9863\n",
      "Epoch 39 / 200 | iteration 0 / 171 | Total Loss: 4.264132499694824 | KNN Loss: 4.217284679412842 | CLS Loss: 0.046848006546497345\n",
      "Epoch 39 / 200 | iteration 10 / 171 | Total Loss: 4.187596797943115 | KNN Loss: 4.172346591949463 | CLS Loss: 0.015250062569975853\n",
      "Epoch 39 / 200 | iteration 20 / 171 | Total Loss: 4.210948467254639 | KNN Loss: 4.177291393280029 | CLS Loss: 0.033656854182481766\n",
      "Epoch 39 / 200 | iteration 30 / 171 | Total Loss: 4.1582865715026855 | KNN Loss: 4.14158296585083 | CLS Loss: 0.016703713685274124\n",
      "Epoch 39 / 200 | iteration 40 / 171 | Total Loss: 4.175783157348633 | KNN Loss: 4.161140441894531 | CLS Loss: 0.014642877504229546\n",
      "Epoch 39 / 200 | iteration 50 / 171 | Total Loss: 4.173635959625244 | KNN Loss: 4.156213760375977 | CLS Loss: 0.017422089353203773\n",
      "Epoch 39 / 200 | iteration 60 / 171 | Total Loss: 4.1786346435546875 | KNN Loss: 4.148740768432617 | CLS Loss: 0.029893744736909866\n",
      "Epoch 39 / 200 | iteration 70 / 171 | Total Loss: 4.196127414703369 | KNN Loss: 4.1796674728393555 | CLS Loss: 0.016459761187434196\n",
      "Epoch 39 / 200 | iteration 80 / 171 | Total Loss: 4.189862251281738 | KNN Loss: 4.15706205368042 | CLS Loss: 0.03280000761151314\n",
      "Epoch 39 / 200 | iteration 90 / 171 | Total Loss: 4.232663154602051 | KNN Loss: 4.1827545166015625 | CLS Loss: 0.04990878701210022\n",
      "Epoch 39 / 200 | iteration 100 / 171 | Total Loss: 4.21150541305542 | KNN Loss: 4.19799280166626 | CLS Loss: 0.01351277157664299\n",
      "Epoch 39 / 200 | iteration 110 / 171 | Total Loss: 4.225856304168701 | KNN Loss: 4.180126190185547 | CLS Loss: 0.04573029652237892\n",
      "Epoch 39 / 200 | iteration 120 / 171 | Total Loss: 4.167003631591797 | KNN Loss: 4.154314994812012 | CLS Loss: 0.012688707560300827\n",
      "Epoch 39 / 200 | iteration 130 / 171 | Total Loss: 4.263718128204346 | KNN Loss: 4.202862739562988 | CLS Loss: 0.06085527315735817\n",
      "Epoch 39 / 200 | iteration 140 / 171 | Total Loss: 4.188926696777344 | KNN Loss: 4.174449920654297 | CLS Loss: 0.014476997777819633\n",
      "Epoch 39 / 200 | iteration 150 / 171 | Total Loss: 4.200733184814453 | KNN Loss: 4.145316123962402 | CLS Loss: 0.055416930466890335\n",
      "Epoch 39 / 200 | iteration 160 / 171 | Total Loss: 4.193273067474365 | KNN Loss: 4.1820759773254395 | CLS Loss: 0.011197022162377834\n",
      "Epoch 39 / 200 | iteration 170 / 171 | Total Loss: 4.222395896911621 | KNN Loss: 4.196588516235352 | CLS Loss: 0.025807566940784454\n",
      "Epoch: 039, Loss: 4.2051, Train: 0.9933, Valid: 0.9860, Best: 0.9863\n",
      "Epoch 40 / 200 | iteration 0 / 171 | Total Loss: 4.232599258422852 | KNN Loss: 4.206260681152344 | CLS Loss: 0.02633853629231453\n",
      "Epoch 40 / 200 | iteration 10 / 171 | Total Loss: 4.167422771453857 | KNN Loss: 4.153818130493164 | CLS Loss: 0.013604707084596157\n",
      "Epoch 40 / 200 | iteration 20 / 171 | Total Loss: 4.186550140380859 | KNN Loss: 4.156761646270752 | CLS Loss: 0.02978837862610817\n",
      "Epoch 40 / 200 | iteration 30 / 171 | Total Loss: 4.161850452423096 | KNN Loss: 4.154801368713379 | CLS Loss: 0.007048959378153086\n",
      "Epoch 40 / 200 | iteration 40 / 171 | Total Loss: 4.215174198150635 | KNN Loss: 4.16652250289917 | CLS Loss: 0.048651572316884995\n",
      "Epoch 40 / 200 | iteration 50 / 171 | Total Loss: 4.185629367828369 | KNN Loss: 4.165614128112793 | CLS Loss: 0.020015032961964607\n",
      "Epoch 40 / 200 | iteration 60 / 171 | Total Loss: 4.194599628448486 | KNN Loss: 4.168214797973633 | CLS Loss: 0.026384882628917694\n",
      "Epoch 40 / 200 | iteration 70 / 171 | Total Loss: 4.2212066650390625 | KNN Loss: 4.183238983154297 | CLS Loss: 0.037967484444379807\n",
      "Epoch 40 / 200 | iteration 80 / 171 | Total Loss: 4.210599422454834 | KNN Loss: 4.174444198608398 | CLS Loss: 0.03615514561533928\n",
      "Epoch 40 / 200 | iteration 90 / 171 | Total Loss: 4.205888271331787 | KNN Loss: 4.198062419891357 | CLS Loss: 0.007825881242752075\n",
      "Epoch 40 / 200 | iteration 100 / 171 | Total Loss: 4.185197830200195 | KNN Loss: 4.163111686706543 | CLS Loss: 0.02208610251545906\n",
      "Epoch 40 / 200 | iteration 110 / 171 | Total Loss: 4.212003707885742 | KNN Loss: 4.1913862228393555 | CLS Loss: 0.020617477595806122\n",
      "Epoch 40 / 200 | iteration 120 / 171 | Total Loss: 4.183345317840576 | KNN Loss: 4.169750690460205 | CLS Loss: 0.013594798743724823\n",
      "Epoch 40 / 200 | iteration 130 / 171 | Total Loss: 4.2100114822387695 | KNN Loss: 4.178858280181885 | CLS Loss: 0.031153278425335884\n",
      "Epoch 40 / 200 | iteration 140 / 171 | Total Loss: 4.204318523406982 | KNN Loss: 4.175039768218994 | CLS Loss: 0.029278896749019623\n",
      "Epoch 40 / 200 | iteration 150 / 171 | Total Loss: 4.176120758056641 | KNN Loss: 4.156066417694092 | CLS Loss: 0.020054427906870842\n",
      "Epoch 40 / 200 | iteration 160 / 171 | Total Loss: 4.213305950164795 | KNN Loss: 4.18438720703125 | CLS Loss: 0.028918687254190445\n",
      "Epoch 40 / 200 | iteration 170 / 171 | Total Loss: 4.166323661804199 | KNN Loss: 4.1368513107299805 | CLS Loss: 0.029472488909959793\n",
      "Epoch: 040, Loss: 4.2019, Train: 0.9914, Valid: 0.9830, Best: 0.9863\n",
      "Epoch 41 / 200 | iteration 0 / 171 | Total Loss: 4.14557409286499 | KNN Loss: 4.137094020843506 | CLS Loss: 0.008480256423354149\n",
      "Epoch 41 / 200 | iteration 10 / 171 | Total Loss: 4.1898393630981445 | KNN Loss: 4.150234699249268 | CLS Loss: 0.039604492485523224\n",
      "Epoch 41 / 200 | iteration 20 / 171 | Total Loss: 4.204212665557861 | KNN Loss: 4.148906230926514 | CLS Loss: 0.05530635267496109\n",
      "Epoch 41 / 200 | iteration 30 / 171 | Total Loss: 4.164146423339844 | KNN Loss: 4.134415626525879 | CLS Loss: 0.02973090298473835\n",
      "Epoch 41 / 200 | iteration 40 / 171 | Total Loss: 4.18310546875 | KNN Loss: 4.1708855628967285 | CLS Loss: 0.012219986878335476\n",
      "Epoch 41 / 200 | iteration 50 / 171 | Total Loss: 4.239486217498779 | KNN Loss: 4.208433628082275 | CLS Loss: 0.031052714213728905\n",
      "Epoch 41 / 200 | iteration 60 / 171 | Total Loss: 4.235515117645264 | KNN Loss: 4.210971832275391 | CLS Loss: 0.024543318897485733\n",
      "Epoch 41 / 200 | iteration 70 / 171 | Total Loss: 4.192260265350342 | KNN Loss: 4.17580509185791 | CLS Loss: 0.016455136239528656\n",
      "Epoch 41 / 200 | iteration 80 / 171 | Total Loss: 4.201440334320068 | KNN Loss: 4.186159610748291 | CLS Loss: 0.01528064627200365\n",
      "Epoch 41 / 200 | iteration 90 / 171 | Total Loss: 4.203640460968018 | KNN Loss: 4.1727166175842285 | CLS Loss: 0.03092382289469242\n",
      "Epoch 41 / 200 | iteration 100 / 171 | Total Loss: 4.186347484588623 | KNN Loss: 4.162843704223633 | CLS Loss: 0.023503975942730904\n",
      "Epoch 41 / 200 | iteration 110 / 171 | Total Loss: 4.214685440063477 | KNN Loss: 4.176156520843506 | CLS Loss: 0.03852885589003563\n",
      "Epoch 41 / 200 | iteration 120 / 171 | Total Loss: 4.1807050704956055 | KNN Loss: 4.153277397155762 | CLS Loss: 0.027427617460489273\n",
      "Epoch 41 / 200 | iteration 130 / 171 | Total Loss: 4.17042350769043 | KNN Loss: 4.141764163970947 | CLS Loss: 0.02865947037935257\n",
      "Epoch 41 / 200 | iteration 140 / 171 | Total Loss: 4.174163341522217 | KNN Loss: 4.1463823318481445 | CLS Loss: 0.027780912816524506\n",
      "Epoch 41 / 200 | iteration 150 / 171 | Total Loss: 4.2000346183776855 | KNN Loss: 4.168883323669434 | CLS Loss: 0.031151093542575836\n",
      "Epoch 41 / 200 | iteration 160 / 171 | Total Loss: 4.220268249511719 | KNN Loss: 4.162039756774902 | CLS Loss: 0.058228522539138794\n",
      "Epoch 41 / 200 | iteration 170 / 171 | Total Loss: 4.177037239074707 | KNN Loss: 4.145581245422363 | CLS Loss: 0.03145614266395569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 041, Loss: 4.1919, Train: 0.9928, Valid: 0.9869, Best: 0.9869\n",
      "Epoch 42 / 200 | iteration 0 / 171 | Total Loss: 4.199117183685303 | KNN Loss: 4.171749114990234 | CLS Loss: 0.027368221431970596\n",
      "Epoch 42 / 200 | iteration 10 / 171 | Total Loss: 4.185390472412109 | KNN Loss: 4.147750377655029 | CLS Loss: 0.03764018043875694\n",
      "Epoch 42 / 200 | iteration 20 / 171 | Total Loss: 4.162098407745361 | KNN Loss: 4.134032249450684 | CLS Loss: 0.0280663650482893\n",
      "Epoch 42 / 200 | iteration 30 / 171 | Total Loss: 4.173219680786133 | KNN Loss: 4.157772541046143 | CLS Loss: 0.015447165817022324\n",
      "Epoch 42 / 200 | iteration 40 / 171 | Total Loss: 4.1575236320495605 | KNN Loss: 4.150005340576172 | CLS Loss: 0.0075180851854383945\n",
      "Epoch 42 / 200 | iteration 50 / 171 | Total Loss: 4.194015026092529 | KNN Loss: 4.1768622398376465 | CLS Loss: 0.017152631655335426\n",
      "Epoch 42 / 200 | iteration 60 / 171 | Total Loss: 4.164169788360596 | KNN Loss: 4.13985013961792 | CLS Loss: 0.024319753050804138\n",
      "Epoch 42 / 200 | iteration 70 / 171 | Total Loss: 4.154576778411865 | KNN Loss: 4.131935119628906 | CLS Loss: 0.022641705349087715\n",
      "Epoch 42 / 200 | iteration 80 / 171 | Total Loss: 4.222629070281982 | KNN Loss: 4.189620018005371 | CLS Loss: 0.03300919383764267\n",
      "Epoch 42 / 200 | iteration 90 / 171 | Total Loss: 4.186686992645264 | KNN Loss: 4.171690464019775 | CLS Loss: 0.014996507205069065\n",
      "Epoch 42 / 200 | iteration 100 / 171 | Total Loss: 4.184521675109863 | KNN Loss: 4.149031162261963 | CLS Loss: 0.03549060598015785\n",
      "Epoch 42 / 200 | iteration 110 / 171 | Total Loss: 4.171992301940918 | KNN Loss: 4.151497840881348 | CLS Loss: 0.020494677126407623\n",
      "Epoch 42 / 200 | iteration 120 / 171 | Total Loss: 4.171506881713867 | KNN Loss: 4.158774375915527 | CLS Loss: 0.012732530944049358\n",
      "Epoch 42 / 200 | iteration 130 / 171 | Total Loss: 4.200977802276611 | KNN Loss: 4.170892238616943 | CLS Loss: 0.030085718259215355\n",
      "Epoch 42 / 200 | iteration 140 / 171 | Total Loss: 4.192624092102051 | KNN Loss: 4.1780104637146 | CLS Loss: 0.014613516628742218\n",
      "Epoch 42 / 200 | iteration 150 / 171 | Total Loss: 4.154743194580078 | KNN Loss: 4.132584571838379 | CLS Loss: 0.022158794105052948\n",
      "Epoch 42 / 200 | iteration 160 / 171 | Total Loss: 4.207382678985596 | KNN Loss: 4.182785511016846 | CLS Loss: 0.024596942588686943\n",
      "Epoch 42 / 200 | iteration 170 / 171 | Total Loss: 4.173749923706055 | KNN Loss: 4.14774751663208 | CLS Loss: 0.026002440601587296\n",
      "Epoch: 042, Loss: 4.1901, Train: 0.9934, Valid: 0.9838, Best: 0.9869\n",
      "Epoch 43 / 200 | iteration 0 / 171 | Total Loss: 4.212220668792725 | KNN Loss: 4.167837142944336 | CLS Loss: 0.044383492320775986\n",
      "Epoch 43 / 200 | iteration 10 / 171 | Total Loss: 4.177839756011963 | KNN Loss: 4.155166149139404 | CLS Loss: 0.022673409432172775\n",
      "Epoch 43 / 200 | iteration 20 / 171 | Total Loss: 4.1680684089660645 | KNN Loss: 4.132837772369385 | CLS Loss: 0.03523040562868118\n",
      "Epoch 43 / 200 | iteration 30 / 171 | Total Loss: 4.197604179382324 | KNN Loss: 4.162893295288086 | CLS Loss: 0.034711021929979324\n",
      "Epoch 43 / 200 | iteration 40 / 171 | Total Loss: 4.208145618438721 | KNN Loss: 4.174200057983398 | CLS Loss: 0.03394579514861107\n",
      "Epoch 43 / 200 | iteration 50 / 171 | Total Loss: 4.162674903869629 | KNN Loss: 4.140161037445068 | CLS Loss: 0.02251390554010868\n",
      "Epoch 43 / 200 | iteration 60 / 171 | Total Loss: 4.194649696350098 | KNN Loss: 4.186278343200684 | CLS Loss: 0.008371252566576004\n",
      "Epoch 43 / 200 | iteration 70 / 171 | Total Loss: 4.185153007507324 | KNN Loss: 4.164622783660889 | CLS Loss: 0.020530162379145622\n",
      "Epoch 43 / 200 | iteration 80 / 171 | Total Loss: 4.157729625701904 | KNN Loss: 4.129108905792236 | CLS Loss: 0.028620801866054535\n",
      "Epoch 43 / 200 | iteration 90 / 171 | Total Loss: 4.186491966247559 | KNN Loss: 4.170522212982178 | CLS Loss: 0.015969792380928993\n",
      "Epoch 43 / 200 | iteration 100 / 171 | Total Loss: 4.150211334228516 | KNN Loss: 4.1352458000183105 | CLS Loss: 0.014965301379561424\n",
      "Epoch 43 / 200 | iteration 110 / 171 | Total Loss: 4.175470352172852 | KNN Loss: 4.142271995544434 | CLS Loss: 0.033198293298482895\n",
      "Epoch 43 / 200 | iteration 120 / 171 | Total Loss: 4.199481010437012 | KNN Loss: 4.183254718780518 | CLS Loss: 0.016226468607783318\n",
      "Epoch 43 / 200 | iteration 130 / 171 | Total Loss: 4.204375743865967 | KNN Loss: 4.171466827392578 | CLS Loss: 0.03290894255042076\n",
      "Epoch 43 / 200 | iteration 140 / 171 | Total Loss: 4.17156457901001 | KNN Loss: 4.153877258300781 | CLS Loss: 0.017687339335680008\n",
      "Epoch 43 / 200 | iteration 150 / 171 | Total Loss: 4.160247325897217 | KNN Loss: 4.139317035675049 | CLS Loss: 0.020930299535393715\n",
      "Epoch 43 / 200 | iteration 160 / 171 | Total Loss: 4.135735034942627 | KNN Loss: 4.116235733032227 | CLS Loss: 0.019499504938721657\n",
      "Epoch 43 / 200 | iteration 170 / 171 | Total Loss: 4.1988348960876465 | KNN Loss: 4.161912441253662 | CLS Loss: 0.03692246228456497\n",
      "Epoch: 043, Loss: 4.1880, Train: 0.9943, Valid: 0.9868, Best: 0.9869\n",
      "Epoch 44 / 200 | iteration 0 / 171 | Total Loss: 4.1824421882629395 | KNN Loss: 4.165726661682129 | CLS Loss: 0.016715500503778458\n",
      "Epoch 44 / 200 | iteration 10 / 171 | Total Loss: 4.181501865386963 | KNN Loss: 4.151459217071533 | CLS Loss: 0.03004283644258976\n",
      "Epoch 44 / 200 | iteration 20 / 171 | Total Loss: 4.195664405822754 | KNN Loss: 4.169403553009033 | CLS Loss: 0.02626067027449608\n",
      "Epoch 44 / 200 | iteration 30 / 171 | Total Loss: 4.161630153656006 | KNN Loss: 4.154004096984863 | CLS Loss: 0.007625872269272804\n",
      "Epoch 44 / 200 | iteration 40 / 171 | Total Loss: 4.196094989776611 | KNN Loss: 4.19033145904541 | CLS Loss: 0.0057633197866380215\n",
      "Epoch 44 / 200 | iteration 50 / 171 | Total Loss: 4.160236835479736 | KNN Loss: 4.126731872558594 | CLS Loss: 0.033505119383335114\n",
      "Epoch 44 / 200 | iteration 60 / 171 | Total Loss: 4.169076919555664 | KNN Loss: 4.12954044342041 | CLS Loss: 0.039536479860544205\n",
      "Epoch 44 / 200 | iteration 70 / 171 | Total Loss: 4.213842391967773 | KNN Loss: 4.174604892730713 | CLS Loss: 0.03923738747835159\n",
      "Epoch 44 / 200 | iteration 80 / 171 | Total Loss: 4.231874465942383 | KNN Loss: 4.202582836151123 | CLS Loss: 0.02929162047803402\n",
      "Epoch 44 / 200 | iteration 90 / 171 | Total Loss: 4.180691719055176 | KNN Loss: 4.157924652099609 | CLS Loss: 0.022767117246985435\n",
      "Epoch 44 / 200 | iteration 100 / 171 | Total Loss: 4.1595258712768555 | KNN Loss: 4.139039993286133 | CLS Loss: 0.0204860121011734\n",
      "Epoch 44 / 200 | iteration 110 / 171 | Total Loss: 4.259854316711426 | KNN Loss: 4.242851257324219 | CLS Loss: 0.017003243789076805\n",
      "Epoch 44 / 200 | iteration 120 / 171 | Total Loss: 4.157859802246094 | KNN Loss: 4.137811660766602 | CLS Loss: 0.020048074424266815\n",
      "Epoch 44 / 200 | iteration 130 / 171 | Total Loss: 4.149188041687012 | KNN Loss: 4.137484073638916 | CLS Loss: 0.011703941971063614\n",
      "Epoch 44 / 200 | iteration 140 / 171 | Total Loss: 4.1852707862854 | KNN Loss: 4.168262004852295 | CLS Loss: 0.01700856350362301\n",
      "Epoch 44 / 200 | iteration 150 / 171 | Total Loss: 4.181777000427246 | KNN Loss: 4.166788101196289 | CLS Loss: 0.014989121817052364\n",
      "Epoch 44 / 200 | iteration 160 / 171 | Total Loss: 4.192111492156982 | KNN Loss: 4.166682720184326 | CLS Loss: 0.025428667664527893\n",
      "Epoch 44 / 200 | iteration 170 / 171 | Total Loss: 4.166464328765869 | KNN Loss: 4.154243469238281 | CLS Loss: 0.012220948934555054\n",
      "Epoch: 044, Loss: 4.1893, Train: 0.9942, Valid: 0.9857, Best: 0.9869\n",
      "Epoch 45 / 200 | iteration 0 / 171 | Total Loss: 4.218568801879883 | KNN Loss: 4.200416564941406 | CLS Loss: 0.018152253702282906\n",
      "Epoch 45 / 200 | iteration 10 / 171 | Total Loss: 4.187920570373535 | KNN Loss: 4.171788215637207 | CLS Loss: 0.016132498160004616\n",
      "Epoch 45 / 200 | iteration 20 / 171 | Total Loss: 4.199020862579346 | KNN Loss: 4.164453983306885 | CLS Loss: 0.034566812217235565\n",
      "Epoch 45 / 200 | iteration 30 / 171 | Total Loss: 4.178621292114258 | KNN Loss: 4.151681423187256 | CLS Loss: 0.02693997323513031\n",
      "Epoch 45 / 200 | iteration 40 / 171 | Total Loss: 4.17929744720459 | KNN Loss: 4.15711784362793 | CLS Loss: 0.022179655730724335\n",
      "Epoch 45 / 200 | iteration 50 / 171 | Total Loss: 4.194371223449707 | KNN Loss: 4.182852268218994 | CLS Loss: 0.011518855579197407\n",
      "Epoch 45 / 200 | iteration 60 / 171 | Total Loss: 4.167019844055176 | KNN Loss: 4.143369197845459 | CLS Loss: 0.023650789633393288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 / 200 | iteration 70 / 171 | Total Loss: 4.195741176605225 | KNN Loss: 4.174771308898926 | CLS Loss: 0.02096995897591114\n",
      "Epoch 45 / 200 | iteration 80 / 171 | Total Loss: 4.240171909332275 | KNN Loss: 4.206181049346924 | CLS Loss: 0.03399086743593216\n",
      "Epoch 45 / 200 | iteration 90 / 171 | Total Loss: 4.183694362640381 | KNN Loss: 4.151634216308594 | CLS Loss: 0.032060373574495316\n",
      "Epoch 45 / 200 | iteration 100 / 171 | Total Loss: 4.1404709815979 | KNN Loss: 4.127050876617432 | CLS Loss: 0.01341994572430849\n",
      "Epoch 45 / 200 | iteration 110 / 171 | Total Loss: 4.148603916168213 | KNN Loss: 4.131430625915527 | CLS Loss: 0.01717349700629711\n",
      "Epoch 45 / 200 | iteration 120 / 171 | Total Loss: 4.173666000366211 | KNN Loss: 4.138299942016602 | CLS Loss: 0.03536606580018997\n",
      "Epoch 45 / 200 | iteration 130 / 171 | Total Loss: 4.18219518661499 | KNN Loss: 4.1590447425842285 | CLS Loss: 0.02315058745443821\n",
      "Epoch 45 / 200 | iteration 140 / 171 | Total Loss: 4.176926612854004 | KNN Loss: 4.157628536224365 | CLS Loss: 0.019298173487186432\n",
      "Epoch 45 / 200 | iteration 150 / 171 | Total Loss: 4.205225944519043 | KNN Loss: 4.16831111907959 | CLS Loss: 0.03691466525197029\n",
      "Epoch 45 / 200 | iteration 160 / 171 | Total Loss: 4.190412998199463 | KNN Loss: 4.166947364807129 | CLS Loss: 0.023465516045689583\n",
      "Epoch 45 / 200 | iteration 170 / 171 | Total Loss: 4.241772651672363 | KNN Loss: 4.217240333557129 | CLS Loss: 0.024532556533813477\n",
      "Epoch: 045, Loss: 4.1849, Train: 0.9929, Valid: 0.9842, Best: 0.9869\n",
      "Epoch 46 / 200 | iteration 0 / 171 | Total Loss: 4.206986427307129 | KNN Loss: 4.1742634773254395 | CLS Loss: 0.03272312507033348\n",
      "Epoch 46 / 200 | iteration 10 / 171 | Total Loss: 4.178654670715332 | KNN Loss: 4.16899299621582 | CLS Loss: 0.009661522693932056\n",
      "Epoch 46 / 200 | iteration 20 / 171 | Total Loss: 4.166003227233887 | KNN Loss: 4.159623622894287 | CLS Loss: 0.0063798376359045506\n",
      "Epoch 46 / 200 | iteration 30 / 171 | Total Loss: 4.159989833831787 | KNN Loss: 4.131414890289307 | CLS Loss: 0.02857494354248047\n",
      "Epoch 46 / 200 | iteration 40 / 171 | Total Loss: 4.162365436553955 | KNN Loss: 4.141339302062988 | CLS Loss: 0.02102619595825672\n",
      "Epoch 46 / 200 | iteration 50 / 171 | Total Loss: 4.171641826629639 | KNN Loss: 4.144331932067871 | CLS Loss: 0.027309870347380638\n",
      "Epoch 46 / 200 | iteration 60 / 171 | Total Loss: 4.231989860534668 | KNN Loss: 4.212548732757568 | CLS Loss: 0.01944112405180931\n",
      "Epoch 46 / 200 | iteration 70 / 171 | Total Loss: 4.153334140777588 | KNN Loss: 4.132957458496094 | CLS Loss: 0.02037656120955944\n",
      "Epoch 46 / 200 | iteration 80 / 171 | Total Loss: 4.172967433929443 | KNN Loss: 4.156750202178955 | CLS Loss: 0.016217030584812164\n",
      "Epoch 46 / 200 | iteration 90 / 171 | Total Loss: 4.202919960021973 | KNN Loss: 4.178420543670654 | CLS Loss: 0.024499423801898956\n",
      "Epoch 46 / 200 | iteration 100 / 171 | Total Loss: 4.214462757110596 | KNN Loss: 4.182762145996094 | CLS Loss: 0.03170064091682434\n",
      "Epoch 46 / 200 | iteration 110 / 171 | Total Loss: 4.171608924865723 | KNN Loss: 4.159438133239746 | CLS Loss: 0.012170816771686077\n",
      "Epoch 46 / 200 | iteration 120 / 171 | Total Loss: 4.2074198722839355 | KNN Loss: 4.183202743530273 | CLS Loss: 0.024217363446950912\n",
      "Epoch 46 / 200 | iteration 130 / 171 | Total Loss: 4.198323726654053 | KNN Loss: 4.150477409362793 | CLS Loss: 0.04784620180726051\n",
      "Epoch 46 / 200 | iteration 140 / 171 | Total Loss: 4.200870037078857 | KNN Loss: 4.185047626495361 | CLS Loss: 0.015822535380721092\n",
      "Epoch 46 / 200 | iteration 150 / 171 | Total Loss: 4.160701751708984 | KNN Loss: 4.141043186187744 | CLS Loss: 0.01965850405395031\n",
      "Epoch 46 / 200 | iteration 160 / 171 | Total Loss: 4.153773784637451 | KNN Loss: 4.1151018142700195 | CLS Loss: 0.0386720634996891\n",
      "Epoch 46 / 200 | iteration 170 / 171 | Total Loss: 4.172321796417236 | KNN Loss: 4.157434940338135 | CLS Loss: 0.014886637218296528\n",
      "Epoch: 046, Loss: 4.1827, Train: 0.9936, Valid: 0.9847, Best: 0.9869\n",
      "Epoch 47 / 200 | iteration 0 / 171 | Total Loss: 4.152312278747559 | KNN Loss: 4.138391494750977 | CLS Loss: 0.013920960016548634\n",
      "Epoch 47 / 200 | iteration 10 / 171 | Total Loss: 4.164380073547363 | KNN Loss: 4.149077415466309 | CLS Loss: 0.015302576124668121\n",
      "Epoch 47 / 200 | iteration 20 / 171 | Total Loss: 4.157990455627441 | KNN Loss: 4.132402420043945 | CLS Loss: 0.02558823861181736\n",
      "Epoch 47 / 200 | iteration 30 / 171 | Total Loss: 4.142974376678467 | KNN Loss: 4.140350818634033 | CLS Loss: 0.0026235743425786495\n",
      "Epoch 47 / 200 | iteration 40 / 171 | Total Loss: 4.118011951446533 | KNN Loss: 4.107868194580078 | CLS Loss: 0.010143876075744629\n",
      "Epoch 47 / 200 | iteration 50 / 171 | Total Loss: 4.185304164886475 | KNN Loss: 4.155724048614502 | CLS Loss: 0.02957998588681221\n",
      "Epoch 47 / 200 | iteration 60 / 171 | Total Loss: 4.177102088928223 | KNN Loss: 4.155084609985352 | CLS Loss: 0.022017648443579674\n",
      "Epoch 47 / 200 | iteration 70 / 171 | Total Loss: 4.138558864593506 | KNN Loss: 4.131978511810303 | CLS Loss: 0.006580205634236336\n",
      "Epoch 47 / 200 | iteration 80 / 171 | Total Loss: 4.175713062286377 | KNN Loss: 4.136351585388184 | CLS Loss: 0.03936124965548515\n",
      "Epoch 47 / 200 | iteration 90 / 171 | Total Loss: 4.207940578460693 | KNN Loss: 4.193548679351807 | CLS Loss: 0.01439205277711153\n",
      "Epoch 47 / 200 | iteration 100 / 171 | Total Loss: 4.200638294219971 | KNN Loss: 4.192154884338379 | CLS Loss: 0.008483187295496464\n",
      "Epoch 47 / 200 | iteration 110 / 171 | Total Loss: 4.197047233581543 | KNN Loss: 4.1861467361450195 | CLS Loss: 0.010900648310780525\n",
      "Epoch 47 / 200 | iteration 120 / 171 | Total Loss: 4.203630447387695 | KNN Loss: 4.1807732582092285 | CLS Loss: 0.022856971248984337\n",
      "Epoch 47 / 200 | iteration 130 / 171 | Total Loss: 4.150449752807617 | KNN Loss: 4.1437668800354 | CLS Loss: 0.006682966835796833\n",
      "Epoch 47 / 200 | iteration 140 / 171 | Total Loss: 4.178615093231201 | KNN Loss: 4.151616096496582 | CLS Loss: 0.0269988551735878\n",
      "Epoch 47 / 200 | iteration 150 / 171 | Total Loss: 4.176559925079346 | KNN Loss: 4.159012317657471 | CLS Loss: 0.01754756271839142\n",
      "Epoch 47 / 200 | iteration 160 / 171 | Total Loss: 4.206552028656006 | KNN Loss: 4.160094261169434 | CLS Loss: 0.04645780846476555\n",
      "Epoch 47 / 200 | iteration 170 / 171 | Total Loss: 4.216213226318359 | KNN Loss: 4.190026760101318 | CLS Loss: 0.026186378672719002\n",
      "Epoch: 047, Loss: 4.1805, Train: 0.9938, Valid: 0.9856, Best: 0.9869\n",
      "Epoch 48 / 200 | iteration 0 / 171 | Total Loss: 4.173818111419678 | KNN Loss: 4.160917282104492 | CLS Loss: 0.012901020236313343\n",
      "Epoch 48 / 200 | iteration 10 / 171 | Total Loss: 4.146569728851318 | KNN Loss: 4.137794494628906 | CLS Loss: 0.008775169029831886\n",
      "Epoch 48 / 200 | iteration 20 / 171 | Total Loss: 4.205378532409668 | KNN Loss: 4.1713666915893555 | CLS Loss: 0.03401179984211922\n",
      "Epoch 48 / 200 | iteration 30 / 171 | Total Loss: 4.1720871925354 | KNN Loss: 4.152460098266602 | CLS Loss: 0.019627001136541367\n",
      "Epoch 48 / 200 | iteration 40 / 171 | Total Loss: 4.151091575622559 | KNN Loss: 4.142049312591553 | CLS Loss: 0.009042210876941681\n",
      "Epoch 48 / 200 | iteration 50 / 171 | Total Loss: 4.175813674926758 | KNN Loss: 4.140964984893799 | CLS Loss: 0.03484846651554108\n",
      "Epoch 48 / 200 | iteration 60 / 171 | Total Loss: 4.171291351318359 | KNN Loss: 4.143951416015625 | CLS Loss: 0.027340088039636612\n",
      "Epoch 48 / 200 | iteration 70 / 171 | Total Loss: 4.205560684204102 | KNN Loss: 4.1746110916137695 | CLS Loss: 0.030949542298913002\n",
      "Epoch 48 / 200 | iteration 80 / 171 | Total Loss: 4.179346084594727 | KNN Loss: 4.156589508056641 | CLS Loss: 0.02275652065873146\n",
      "Epoch 48 / 200 | iteration 90 / 171 | Total Loss: 4.148623943328857 | KNN Loss: 4.125861167907715 | CLS Loss: 0.022762715816497803\n",
      "Epoch 48 / 200 | iteration 100 / 171 | Total Loss: 4.1630754470825195 | KNN Loss: 4.152616024017334 | CLS Loss: 0.010459613986313343\n",
      "Epoch 48 / 200 | iteration 110 / 171 | Total Loss: 4.175959587097168 | KNN Loss: 4.144796848297119 | CLS Loss: 0.031162599101662636\n",
      "Epoch 48 / 200 | iteration 120 / 171 | Total Loss: 4.1922502517700195 | KNN Loss: 4.177489757537842 | CLS Loss: 0.014760619960725307\n",
      "Epoch 48 / 200 | iteration 130 / 171 | Total Loss: 4.165680408477783 | KNN Loss: 4.14879035949707 | CLS Loss: 0.016890155151486397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 / 200 | iteration 140 / 171 | Total Loss: 4.214667320251465 | KNN Loss: 4.183526515960693 | CLS Loss: 0.031140761449933052\n",
      "Epoch 48 / 200 | iteration 150 / 171 | Total Loss: 4.174677848815918 | KNN Loss: 4.163699626922607 | CLS Loss: 0.010978439822793007\n",
      "Epoch 48 / 200 | iteration 160 / 171 | Total Loss: 4.226525783538818 | KNN Loss: 4.206968784332275 | CLS Loss: 0.019557001069188118\n",
      "Epoch 48 / 200 | iteration 170 / 171 | Total Loss: 4.168240070343018 | KNN Loss: 4.135058879852295 | CLS Loss: 0.03318142145872116\n",
      "Epoch: 048, Loss: 4.1807, Train: 0.9944, Valid: 0.9862, Best: 0.9869\n",
      "Epoch 49 / 200 | iteration 0 / 171 | Total Loss: 4.194820880889893 | KNN Loss: 4.185375690460205 | CLS Loss: 0.009445064701139927\n",
      "Epoch 49 / 200 | iteration 10 / 171 | Total Loss: 4.1533589363098145 | KNN Loss: 4.146146297454834 | CLS Loss: 0.0072127413004636765\n",
      "Epoch 49 / 200 | iteration 20 / 171 | Total Loss: 4.1549530029296875 | KNN Loss: 4.149294853210449 | CLS Loss: 0.005658128298819065\n",
      "Epoch 49 / 200 | iteration 30 / 171 | Total Loss: 4.153327465057373 | KNN Loss: 4.1479902267456055 | CLS Loss: 0.005337131675332785\n",
      "Epoch 49 / 200 | iteration 40 / 171 | Total Loss: 4.218434810638428 | KNN Loss: 4.194079399108887 | CLS Loss: 0.024355288594961166\n",
      "Epoch 49 / 200 | iteration 50 / 171 | Total Loss: 4.233047008514404 | KNN Loss: 4.2009735107421875 | CLS Loss: 0.03207331523299217\n",
      "Epoch 49 / 200 | iteration 60 / 171 | Total Loss: 4.182542324066162 | KNN Loss: 4.167263507843018 | CLS Loss: 0.015279003418982029\n",
      "Epoch 49 / 200 | iteration 70 / 171 | Total Loss: 4.132935523986816 | KNN Loss: 4.1246209144592285 | CLS Loss: 0.00831473246216774\n",
      "Epoch 49 / 200 | iteration 80 / 171 | Total Loss: 4.1997880935668945 | KNN Loss: 4.170846939086914 | CLS Loss: 0.028941312804818153\n",
      "Epoch 49 / 200 | iteration 90 / 171 | Total Loss: 4.177127361297607 | KNN Loss: 4.162280082702637 | CLS Loss: 0.014847458340227604\n",
      "Epoch 49 / 200 | iteration 100 / 171 | Total Loss: 4.22697639465332 | KNN Loss: 4.211129665374756 | CLS Loss: 0.015846818685531616\n",
      "Epoch 49 / 200 | iteration 110 / 171 | Total Loss: 4.194857597351074 | KNN Loss: 4.175017833709717 | CLS Loss: 0.019839564338326454\n",
      "Epoch 49 / 200 | iteration 120 / 171 | Total Loss: 4.201467514038086 | KNN Loss: 4.184844017028809 | CLS Loss: 0.01662333309650421\n",
      "Epoch 49 / 200 | iteration 130 / 171 | Total Loss: 4.169223785400391 | KNN Loss: 4.149656772613525 | CLS Loss: 0.01956699788570404\n",
      "Epoch 49 / 200 | iteration 140 / 171 | Total Loss: 4.170867919921875 | KNN Loss: 4.158531188964844 | CLS Loss: 0.012336662039160728\n",
      "Epoch 49 / 200 | iteration 150 / 171 | Total Loss: 4.181974411010742 | KNN Loss: 4.170276641845703 | CLS Loss: 0.011697685346007347\n",
      "Epoch 49 / 200 | iteration 160 / 171 | Total Loss: 4.181889057159424 | KNN Loss: 4.179360389709473 | CLS Loss: 0.002528686774894595\n",
      "Epoch 49 / 200 | iteration 170 / 171 | Total Loss: 4.151723861694336 | KNN Loss: 4.103626728057861 | CLS Loss: 0.0480971597135067\n",
      "Epoch: 049, Loss: 4.1845, Train: 0.9948, Valid: 0.9865, Best: 0.9869\n",
      "Epoch 50 / 200 | iteration 0 / 171 | Total Loss: 4.144927978515625 | KNN Loss: 4.138754367828369 | CLS Loss: 0.00617349985986948\n",
      "Epoch 50 / 200 | iteration 10 / 171 | Total Loss: 4.157255172729492 | KNN Loss: 4.144450664520264 | CLS Loss: 0.012804428115487099\n",
      "Epoch 50 / 200 | iteration 20 / 171 | Total Loss: 4.1676788330078125 | KNN Loss: 4.134097099304199 | CLS Loss: 0.03358183428645134\n",
      "Epoch 50 / 200 | iteration 30 / 171 | Total Loss: 4.12171745300293 | KNN Loss: 4.109660625457764 | CLS Loss: 0.01205690298229456\n",
      "Epoch 50 / 200 | iteration 40 / 171 | Total Loss: 4.201110363006592 | KNN Loss: 4.1722564697265625 | CLS Loss: 0.028854088857769966\n",
      "Epoch 50 / 200 | iteration 50 / 171 | Total Loss: 4.151308536529541 | KNN Loss: 4.130652904510498 | CLS Loss: 0.02065560407936573\n",
      "Epoch 50 / 200 | iteration 60 / 171 | Total Loss: 4.1880364418029785 | KNN Loss: 4.172643184661865 | CLS Loss: 0.015393064357340336\n",
      "Epoch 50 / 200 | iteration 70 / 171 | Total Loss: 4.134039878845215 | KNN Loss: 4.115500450134277 | CLS Loss: 0.018539508804678917\n",
      "Epoch 50 / 200 | iteration 80 / 171 | Total Loss: 4.178426742553711 | KNN Loss: 4.167114734649658 | CLS Loss: 0.01131217461079359\n",
      "Epoch 50 / 200 | iteration 90 / 171 | Total Loss: 4.157895088195801 | KNN Loss: 4.1312665939331055 | CLS Loss: 0.02662843093276024\n",
      "Epoch 50 / 200 | iteration 100 / 171 | Total Loss: 4.174672603607178 | KNN Loss: 4.138710975646973 | CLS Loss: 0.035961832851171494\n",
      "Epoch 50 / 200 | iteration 110 / 171 | Total Loss: 4.1421098709106445 | KNN Loss: 4.130669593811035 | CLS Loss: 0.011440430767834187\n",
      "Epoch 50 / 200 | iteration 120 / 171 | Total Loss: 4.159200191497803 | KNN Loss: 4.141115188598633 | CLS Loss: 0.018085027113556862\n",
      "Epoch 50 / 200 | iteration 130 / 171 | Total Loss: 4.225790977478027 | KNN Loss: 4.218682289123535 | CLS Loss: 0.007108451798558235\n",
      "Epoch 50 / 200 | iteration 140 / 171 | Total Loss: 4.16135835647583 | KNN Loss: 4.142757892608643 | CLS Loss: 0.018600549548864365\n",
      "Epoch 50 / 200 | iteration 150 / 171 | Total Loss: 4.168811798095703 | KNN Loss: 4.13613224029541 | CLS Loss: 0.032679494470357895\n",
      "Epoch 50 / 200 | iteration 160 / 171 | Total Loss: 4.168045520782471 | KNN Loss: 4.158508777618408 | CLS Loss: 0.009536804631352425\n",
      "Epoch 50 / 200 | iteration 170 / 171 | Total Loss: 4.177448272705078 | KNN Loss: 4.166260719299316 | CLS Loss: 0.011187460273504257\n",
      "Epoch: 050, Loss: 4.1806, Train: 0.9943, Valid: 0.9856, Best: 0.9869\n",
      "Epoch 51 / 200 | iteration 0 / 171 | Total Loss: 4.186857223510742 | KNN Loss: 4.161900520324707 | CLS Loss: 0.024956533685326576\n",
      "Epoch 51 / 200 | iteration 10 / 171 | Total Loss: 4.177258491516113 | KNN Loss: 4.1676836013793945 | CLS Loss: 0.009574669413268566\n",
      "Epoch 51 / 200 | iteration 20 / 171 | Total Loss: 4.193510055541992 | KNN Loss: 4.175403594970703 | CLS Loss: 0.018106509000062943\n",
      "Epoch 51 / 200 | iteration 30 / 171 | Total Loss: 4.159551620483398 | KNN Loss: 4.147755146026611 | CLS Loss: 0.01179662998765707\n",
      "Epoch 51 / 200 | iteration 40 / 171 | Total Loss: 4.149046421051025 | KNN Loss: 4.130553245544434 | CLS Loss: 0.018493298441171646\n",
      "Epoch 51 / 200 | iteration 50 / 171 | Total Loss: 4.17924165725708 | KNN Loss: 4.154641151428223 | CLS Loss: 0.024600615724921227\n",
      "Epoch 51 / 200 | iteration 60 / 171 | Total Loss: 4.20616340637207 | KNN Loss: 4.182391166687012 | CLS Loss: 0.02377208322286606\n",
      "Epoch 51 / 200 | iteration 70 / 171 | Total Loss: 4.15989875793457 | KNN Loss: 4.145449161529541 | CLS Loss: 0.014449714682996273\n",
      "Epoch 51 / 200 | iteration 80 / 171 | Total Loss: 4.165592193603516 | KNN Loss: 4.134407043457031 | CLS Loss: 0.031185153871774673\n",
      "Epoch 51 / 200 | iteration 90 / 171 | Total Loss: 4.128880500793457 | KNN Loss: 4.121212482452393 | CLS Loss: 0.007668009959161282\n",
      "Epoch 51 / 200 | iteration 100 / 171 | Total Loss: 4.190061092376709 | KNN Loss: 4.150694847106934 | CLS Loss: 0.03936624526977539\n",
      "Epoch 51 / 200 | iteration 110 / 171 | Total Loss: 4.175424575805664 | KNN Loss: 4.172450542449951 | CLS Loss: 0.002974145347252488\n",
      "Epoch 51 / 200 | iteration 120 / 171 | Total Loss: 4.1952362060546875 | KNN Loss: 4.1751179695129395 | CLS Loss: 0.020118456333875656\n",
      "Epoch 51 / 200 | iteration 130 / 171 | Total Loss: 4.177804946899414 | KNN Loss: 4.145933628082275 | CLS Loss: 0.031871140003204346\n",
      "Epoch 51 / 200 | iteration 140 / 171 | Total Loss: 4.152557373046875 | KNN Loss: 4.1397786140441895 | CLS Loss: 0.012778819538652897\n",
      "Epoch 51 / 200 | iteration 150 / 171 | Total Loss: 4.137996673583984 | KNN Loss: 4.133556842803955 | CLS Loss: 0.004439644515514374\n",
      "Epoch 51 / 200 | iteration 160 / 171 | Total Loss: 4.172208786010742 | KNN Loss: 4.156416416168213 | CLS Loss: 0.01579238846898079\n",
      "Epoch 51 / 200 | iteration 170 / 171 | Total Loss: 4.218341827392578 | KNN Loss: 4.192066192626953 | CLS Loss: 0.02627567946910858\n",
      "Epoch: 051, Loss: 4.1812, Train: 0.9935, Valid: 0.9845, Best: 0.9869\n",
      "Epoch 52 / 200 | iteration 0 / 171 | Total Loss: 4.1912031173706055 | KNN Loss: 4.153928279876709 | CLS Loss: 0.037274960428476334\n",
      "Epoch 52 / 200 | iteration 10 / 171 | Total Loss: 4.192597389221191 | KNN Loss: 4.173953533172607 | CLS Loss: 0.018643943592905998\n",
      "Epoch 52 / 200 | iteration 20 / 171 | Total Loss: 4.1821160316467285 | KNN Loss: 4.164933681488037 | CLS Loss: 0.017182176932692528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 / 200 | iteration 30 / 171 | Total Loss: 4.196008682250977 | KNN Loss: 4.175873279571533 | CLS Loss: 0.020135505124926567\n",
      "Epoch 52 / 200 | iteration 40 / 171 | Total Loss: 4.173285007476807 | KNN Loss: 4.135589122772217 | CLS Loss: 0.03769611194729805\n",
      "Epoch 52 / 200 | iteration 50 / 171 | Total Loss: 4.125834941864014 | KNN Loss: 4.106276988983154 | CLS Loss: 0.019558018073439598\n",
      "Epoch 52 / 200 | iteration 60 / 171 | Total Loss: 4.18631649017334 | KNN Loss: 4.166380405426025 | CLS Loss: 0.019936062395572662\n",
      "Epoch 52 / 200 | iteration 70 / 171 | Total Loss: 4.168275833129883 | KNN Loss: 4.138324737548828 | CLS Loss: 0.029951177537441254\n",
      "Epoch 52 / 200 | iteration 80 / 171 | Total Loss: 4.1514482498168945 | KNN Loss: 4.132675647735596 | CLS Loss: 0.018772650510072708\n",
      "Epoch 52 / 200 | iteration 90 / 171 | Total Loss: 4.154571056365967 | KNN Loss: 4.1483845710754395 | CLS Loss: 0.006186395883560181\n",
      "Epoch 52 / 200 | iteration 100 / 171 | Total Loss: 4.1694231033325195 | KNN Loss: 4.157458782196045 | CLS Loss: 0.01196453720331192\n",
      "Epoch 52 / 200 | iteration 110 / 171 | Total Loss: 4.16619348526001 | KNN Loss: 4.157819747924805 | CLS Loss: 0.008373824879527092\n",
      "Epoch 52 / 200 | iteration 120 / 171 | Total Loss: 4.171753406524658 | KNN Loss: 4.162494659423828 | CLS Loss: 0.00925868097692728\n",
      "Epoch 52 / 200 | iteration 130 / 171 | Total Loss: 4.198199272155762 | KNN Loss: 4.145252227783203 | CLS Loss: 0.052947036921978\n",
      "Epoch 52 / 200 | iteration 140 / 171 | Total Loss: 4.210477352142334 | KNN Loss: 4.172622203826904 | CLS Loss: 0.03785520792007446\n",
      "Epoch 52 / 200 | iteration 150 / 171 | Total Loss: 4.204717636108398 | KNN Loss: 4.18671989440918 | CLS Loss: 0.017997587099671364\n",
      "Epoch 52 / 200 | iteration 160 / 171 | Total Loss: 4.228555679321289 | KNN Loss: 4.200377941131592 | CLS Loss: 0.028177713975310326\n",
      "Epoch 52 / 200 | iteration 170 / 171 | Total Loss: 4.209482669830322 | KNN Loss: 4.195586681365967 | CLS Loss: 0.013895793817937374\n",
      "Epoch: 052, Loss: 4.1762, Train: 0.9941, Valid: 0.9856, Best: 0.9869\n",
      "Epoch 53 / 200 | iteration 0 / 171 | Total Loss: 4.188708305358887 | KNN Loss: 4.173538684844971 | CLS Loss: 0.015169648453593254\n",
      "Epoch 53 / 200 | iteration 10 / 171 | Total Loss: 4.1752800941467285 | KNN Loss: 4.1386399269104 | CLS Loss: 0.036639969795942307\n",
      "Epoch 53 / 200 | iteration 20 / 171 | Total Loss: 4.164917945861816 | KNN Loss: 4.151796340942383 | CLS Loss: 0.013121458701789379\n",
      "Epoch 53 / 200 | iteration 30 / 171 | Total Loss: 4.168979167938232 | KNN Loss: 4.146099090576172 | CLS Loss: 0.022880222648382187\n",
      "Epoch 53 / 200 | iteration 40 / 171 | Total Loss: 4.1678385734558105 | KNN Loss: 4.1476664543151855 | CLS Loss: 0.020171908661723137\n",
      "Epoch 53 / 200 | iteration 50 / 171 | Total Loss: 4.169544696807861 | KNN Loss: 4.1465253829956055 | CLS Loss: 0.023019136860966682\n",
      "Epoch 53 / 200 | iteration 60 / 171 | Total Loss: 4.160120010375977 | KNN Loss: 4.137967109680176 | CLS Loss: 0.022152826189994812\n",
      "Epoch 53 / 200 | iteration 70 / 171 | Total Loss: 4.18768310546875 | KNN Loss: 4.1746745109558105 | CLS Loss: 0.01300857774913311\n",
      "Epoch 53 / 200 | iteration 80 / 171 | Total Loss: 4.1582746505737305 | KNN Loss: 4.14668607711792 | CLS Loss: 0.011588783003389835\n",
      "Epoch 53 / 200 | iteration 90 / 171 | Total Loss: 4.1834306716918945 | KNN Loss: 4.158520221710205 | CLS Loss: 0.02491052821278572\n",
      "Epoch 53 / 200 | iteration 100 / 171 | Total Loss: 4.138836860656738 | KNN Loss: 4.134079933166504 | CLS Loss: 0.004756806883960962\n",
      "Epoch 53 / 200 | iteration 110 / 171 | Total Loss: 4.124344348907471 | KNN Loss: 4.1128129959106445 | CLS Loss: 0.011531475931406021\n",
      "Epoch 53 / 200 | iteration 120 / 171 | Total Loss: 4.1775593757629395 | KNN Loss: 4.1653547286987305 | CLS Loss: 0.012204534374177456\n",
      "Epoch 53 / 200 | iteration 130 / 171 | Total Loss: 4.135251045227051 | KNN Loss: 4.123647689819336 | CLS Loss: 0.011603275313973427\n",
      "Epoch 53 / 200 | iteration 140 / 171 | Total Loss: 4.156622409820557 | KNN Loss: 4.143849849700928 | CLS Loss: 0.01277238316833973\n",
      "Epoch 53 / 200 | iteration 150 / 171 | Total Loss: 4.199941158294678 | KNN Loss: 4.149692535400391 | CLS Loss: 0.05024845153093338\n",
      "Epoch 53 / 200 | iteration 160 / 171 | Total Loss: 4.196700572967529 | KNN Loss: 4.179189205169678 | CLS Loss: 0.01751125231385231\n",
      "Epoch 53 / 200 | iteration 170 / 171 | Total Loss: 4.201996326446533 | KNN Loss: 4.182542324066162 | CLS Loss: 0.019454121589660645\n",
      "Epoch: 053, Loss: 4.1752, Train: 0.9939, Valid: 0.9861, Best: 0.9869\n",
      "Epoch 54 / 200 | iteration 0 / 171 | Total Loss: 4.193446159362793 | KNN Loss: 4.181465148925781 | CLS Loss: 0.01198099460452795\n",
      "Epoch 54 / 200 | iteration 10 / 171 | Total Loss: 4.153772354125977 | KNN Loss: 4.1459221839904785 | CLS Loss: 0.007849984802305698\n",
      "Epoch 54 / 200 | iteration 20 / 171 | Total Loss: 4.199959754943848 | KNN Loss: 4.163753986358643 | CLS Loss: 0.03620591759681702\n",
      "Epoch 54 / 200 | iteration 30 / 171 | Total Loss: 4.203466892242432 | KNN Loss: 4.189945220947266 | CLS Loss: 0.013521777465939522\n",
      "Epoch 54 / 200 | iteration 40 / 171 | Total Loss: 4.158238410949707 | KNN Loss: 4.13318395614624 | CLS Loss: 0.025054482743144035\n",
      "Epoch 54 / 200 | iteration 50 / 171 | Total Loss: 4.180072784423828 | KNN Loss: 4.176421165466309 | CLS Loss: 0.003651578212156892\n",
      "Epoch 54 / 200 | iteration 60 / 171 | Total Loss: 4.181909084320068 | KNN Loss: 4.163552761077881 | CLS Loss: 0.01835615746676922\n",
      "Epoch 54 / 200 | iteration 70 / 171 | Total Loss: 4.192832946777344 | KNN Loss: 4.182882308959961 | CLS Loss: 0.00995070580393076\n",
      "Epoch 54 / 200 | iteration 80 / 171 | Total Loss: 4.188053607940674 | KNN Loss: 4.161900997161865 | CLS Loss: 0.026152612641453743\n",
      "Epoch 54 / 200 | iteration 90 / 171 | Total Loss: 4.141499996185303 | KNN Loss: 4.131531715393066 | CLS Loss: 0.009968332946300507\n",
      "Epoch 54 / 200 | iteration 100 / 171 | Total Loss: 4.138243675231934 | KNN Loss: 4.124261379241943 | CLS Loss: 0.01398212555795908\n",
      "Epoch 54 / 200 | iteration 110 / 171 | Total Loss: 4.167334079742432 | KNN Loss: 4.1576313972473145 | CLS Loss: 0.009702697396278381\n",
      "Epoch 54 / 200 | iteration 120 / 171 | Total Loss: 4.153873443603516 | KNN Loss: 4.141204833984375 | CLS Loss: 0.012668661773204803\n",
      "Epoch 54 / 200 | iteration 130 / 171 | Total Loss: 4.196255207061768 | KNN Loss: 4.1704535484313965 | CLS Loss: 0.025801721960306168\n",
      "Epoch 54 / 200 | iteration 140 / 171 | Total Loss: 4.157251358032227 | KNN Loss: 4.126399517059326 | CLS Loss: 0.0308518148958683\n",
      "Epoch 54 / 200 | iteration 150 / 171 | Total Loss: 4.176234245300293 | KNN Loss: 4.1492815017700195 | CLS Loss: 0.026952750980854034\n",
      "Epoch 54 / 200 | iteration 160 / 171 | Total Loss: 4.160950183868408 | KNN Loss: 4.147830963134766 | CLS Loss: 0.013119193725287914\n",
      "Epoch 54 / 200 | iteration 170 / 171 | Total Loss: 4.166153907775879 | KNN Loss: 4.142470359802246 | CLS Loss: 0.023683562874794006\n",
      "Epoch: 054, Loss: 4.1752, Train: 0.9946, Valid: 0.9865, Best: 0.9869\n",
      "Epoch 55 / 200 | iteration 0 / 171 | Total Loss: 4.1539764404296875 | KNN Loss: 4.13947868347168 | CLS Loss: 0.014497658237814903\n",
      "Epoch 55 / 200 | iteration 10 / 171 | Total Loss: 4.17717981338501 | KNN Loss: 4.1691107749938965 | CLS Loss: 0.008069038391113281\n",
      "Epoch 55 / 200 | iteration 20 / 171 | Total Loss: 4.129822731018066 | KNN Loss: 4.114773273468018 | CLS Loss: 0.015049505047500134\n",
      "Epoch 55 / 200 | iteration 30 / 171 | Total Loss: 4.164877414703369 | KNN Loss: 4.15741491317749 | CLS Loss: 0.007462379522621632\n",
      "Epoch 55 / 200 | iteration 40 / 171 | Total Loss: 4.2320027351379395 | KNN Loss: 4.186606407165527 | CLS Loss: 0.04539650306105614\n",
      "Epoch 55 / 200 | iteration 50 / 171 | Total Loss: 4.16838264465332 | KNN Loss: 4.145656585693359 | CLS Loss: 0.02272624522447586\n",
      "Epoch 55 / 200 | iteration 60 / 171 | Total Loss: 4.187923431396484 | KNN Loss: 4.1663970947265625 | CLS Loss: 0.021526526659727097\n",
      "Epoch 55 / 200 | iteration 70 / 171 | Total Loss: 4.221569538116455 | KNN Loss: 4.179684162139893 | CLS Loss: 0.04188532382249832\n",
      "Epoch 55 / 200 | iteration 80 / 171 | Total Loss: 4.172496318817139 | KNN Loss: 4.156726837158203 | CLS Loss: 0.015769466757774353\n",
      "Epoch 55 / 200 | iteration 90 / 171 | Total Loss: 4.149304389953613 | KNN Loss: 4.135127067565918 | CLS Loss: 0.014177286066114902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 / 200 | iteration 100 / 171 | Total Loss: 4.155078411102295 | KNN Loss: 4.135499000549316 | CLS Loss: 0.01957947388291359\n",
      "Epoch 55 / 200 | iteration 110 / 171 | Total Loss: 4.178250312805176 | KNN Loss: 4.141901016235352 | CLS Loss: 0.036349281668663025\n",
      "Epoch 55 / 200 | iteration 120 / 171 | Total Loss: 4.185174465179443 | KNN Loss: 4.161858558654785 | CLS Loss: 0.02331603690981865\n",
      "Epoch 55 / 200 | iteration 130 / 171 | Total Loss: 4.171797275543213 | KNN Loss: 4.151957988739014 | CLS Loss: 0.019839385524392128\n",
      "Epoch 55 / 200 | iteration 140 / 171 | Total Loss: 4.169252872467041 | KNN Loss: 4.16044807434082 | CLS Loss: 0.008804881945252419\n",
      "Epoch 55 / 200 | iteration 150 / 171 | Total Loss: 4.143223285675049 | KNN Loss: 4.126646041870117 | CLS Loss: 0.016577070578932762\n",
      "Epoch 55 / 200 | iteration 160 / 171 | Total Loss: 4.174678325653076 | KNN Loss: 4.156030654907227 | CLS Loss: 0.018647542223334312\n",
      "Epoch 55 / 200 | iteration 170 / 171 | Total Loss: 4.200728416442871 | KNN Loss: 4.174258708953857 | CLS Loss: 0.026469910517334938\n",
      "Epoch: 055, Loss: 4.1781, Train: 0.9947, Valid: 0.9865, Best: 0.9869\n",
      "Epoch 56 / 200 | iteration 0 / 171 | Total Loss: 4.157804489135742 | KNN Loss: 4.13008975982666 | CLS Loss: 0.02771490253508091\n",
      "Epoch 56 / 200 | iteration 10 / 171 | Total Loss: 4.164241790771484 | KNN Loss: 4.148580551147461 | CLS Loss: 0.01566142588853836\n",
      "Epoch 56 / 200 | iteration 20 / 171 | Total Loss: 4.156006336212158 | KNN Loss: 4.146033763885498 | CLS Loss: 0.009972618892788887\n",
      "Epoch 56 / 200 | iteration 30 / 171 | Total Loss: 4.173539638519287 | KNN Loss: 4.156283378601074 | CLS Loss: 0.017256231978535652\n",
      "Epoch 56 / 200 | iteration 40 / 171 | Total Loss: 4.132979393005371 | KNN Loss: 4.122448444366455 | CLS Loss: 0.010530781000852585\n",
      "Epoch 56 / 200 | iteration 50 / 171 | Total Loss: 4.174023628234863 | KNN Loss: 4.152644157409668 | CLS Loss: 0.021379321813583374\n",
      "Epoch 56 / 200 | iteration 60 / 171 | Total Loss: 4.1711039543151855 | KNN Loss: 4.151286602020264 | CLS Loss: 0.019817303866147995\n",
      "Epoch 56 / 200 | iteration 70 / 171 | Total Loss: 4.187925815582275 | KNN Loss: 4.163095474243164 | CLS Loss: 0.02483023889362812\n",
      "Epoch 56 / 200 | iteration 80 / 171 | Total Loss: 4.204334259033203 | KNN Loss: 4.190853595733643 | CLS Loss: 0.013480495661497116\n",
      "Epoch 56 / 200 | iteration 90 / 171 | Total Loss: 4.163393974304199 | KNN Loss: 4.143846035003662 | CLS Loss: 0.019548064097762108\n",
      "Epoch 56 / 200 | iteration 100 / 171 | Total Loss: 4.1447625160217285 | KNN Loss: 4.126067161560059 | CLS Loss: 0.018695557489991188\n",
      "Epoch 56 / 200 | iteration 110 / 171 | Total Loss: 4.152906894683838 | KNN Loss: 4.1249589920043945 | CLS Loss: 0.02794796973466873\n",
      "Epoch 56 / 200 | iteration 120 / 171 | Total Loss: 4.168119430541992 | KNN Loss: 4.161501884460449 | CLS Loss: 0.006617673672735691\n",
      "Epoch 56 / 200 | iteration 130 / 171 | Total Loss: 4.182096004486084 | KNN Loss: 4.161988735198975 | CLS Loss: 0.020107164978981018\n",
      "Epoch 56 / 200 | iteration 140 / 171 | Total Loss: 4.159930229187012 | KNN Loss: 4.146395206451416 | CLS Loss: 0.013534801080822945\n",
      "Epoch 56 / 200 | iteration 150 / 171 | Total Loss: 4.165772438049316 | KNN Loss: 4.141181945800781 | CLS Loss: 0.024590320885181427\n",
      "Epoch 56 / 200 | iteration 160 / 171 | Total Loss: 4.192439079284668 | KNN Loss: 4.190606117248535 | CLS Loss: 0.0018327557481825352\n",
      "Epoch 56 / 200 | iteration 170 / 171 | Total Loss: 4.154411315917969 | KNN Loss: 4.1343512535095215 | CLS Loss: 0.020060192793607712\n",
      "Epoch: 056, Loss: 4.1743, Train: 0.9951, Valid: 0.9869, Best: 0.9869\n",
      "Epoch 57 / 200 | iteration 0 / 171 | Total Loss: 4.175217628479004 | KNN Loss: 4.159811973571777 | CLS Loss: 0.015405561774969101\n",
      "Epoch 57 / 200 | iteration 10 / 171 | Total Loss: 4.2305908203125 | KNN Loss: 4.216131210327148 | CLS Loss: 0.014459701254963875\n",
      "Epoch 57 / 200 | iteration 20 / 171 | Total Loss: 4.14900541305542 | KNN Loss: 4.125496864318848 | CLS Loss: 0.023508569225668907\n",
      "Epoch 57 / 200 | iteration 30 / 171 | Total Loss: 4.212385654449463 | KNN Loss: 4.190843105316162 | CLS Loss: 0.021542495116591454\n",
      "Epoch 57 / 200 | iteration 40 / 171 | Total Loss: 4.140057563781738 | KNN Loss: 4.124361991882324 | CLS Loss: 0.015695424750447273\n",
      "Epoch 57 / 200 | iteration 50 / 171 | Total Loss: 4.175288677215576 | KNN Loss: 4.163247585296631 | CLS Loss: 0.012041104026138783\n",
      "Epoch 57 / 200 | iteration 60 / 171 | Total Loss: 4.226044178009033 | KNN Loss: 4.195067882537842 | CLS Loss: 0.030976464971899986\n",
      "Epoch 57 / 200 | iteration 70 / 171 | Total Loss: 4.143311977386475 | KNN Loss: 4.135762691497803 | CLS Loss: 0.0075491066090762615\n",
      "Epoch 57 / 200 | iteration 80 / 171 | Total Loss: 4.166877269744873 | KNN Loss: 4.142991065979004 | CLS Loss: 0.02388617768883705\n",
      "Epoch 57 / 200 | iteration 90 / 171 | Total Loss: 4.2036542892456055 | KNN Loss: 4.198919773101807 | CLS Loss: 0.0047342851758003235\n",
      "Epoch 57 / 200 | iteration 100 / 171 | Total Loss: 4.125919342041016 | KNN Loss: 4.1218695640563965 | CLS Loss: 0.00404970021918416\n",
      "Epoch 57 / 200 | iteration 110 / 171 | Total Loss: 4.184054851531982 | KNN Loss: 4.180276393890381 | CLS Loss: 0.003778689308091998\n",
      "Epoch 57 / 200 | iteration 120 / 171 | Total Loss: 4.174131393432617 | KNN Loss: 4.1533203125 | CLS Loss: 0.02081097476184368\n",
      "Epoch 57 / 200 | iteration 130 / 171 | Total Loss: 4.177642345428467 | KNN Loss: 4.168231964111328 | CLS Loss: 0.009410172700881958\n",
      "Epoch 57 / 200 | iteration 140 / 171 | Total Loss: 4.161807537078857 | KNN Loss: 4.1527252197265625 | CLS Loss: 0.009082307107746601\n",
      "Epoch 57 / 200 | iteration 150 / 171 | Total Loss: 4.163287162780762 | KNN Loss: 4.147339820861816 | CLS Loss: 0.0159471333026886\n",
      "Epoch 57 / 200 | iteration 160 / 171 | Total Loss: 4.202430248260498 | KNN Loss: 4.164807319641113 | CLS Loss: 0.03762282803654671\n",
      "Epoch 57 / 200 | iteration 170 / 171 | Total Loss: 4.150741100311279 | KNN Loss: 4.133981227874756 | CLS Loss: 0.01675969548523426\n",
      "Epoch: 057, Loss: 4.1732, Train: 0.9952, Valid: 0.9866, Best: 0.9869\n",
      "Epoch 58 / 200 | iteration 0 / 171 | Total Loss: 4.176004886627197 | KNN Loss: 4.162073612213135 | CLS Loss: 0.013931462541222572\n",
      "Epoch 58 / 200 | iteration 10 / 171 | Total Loss: 4.16412878036499 | KNN Loss: 4.144784450531006 | CLS Loss: 0.019344544038176537\n",
      "Epoch 58 / 200 | iteration 20 / 171 | Total Loss: 4.103842735290527 | KNN Loss: 4.098686695098877 | CLS Loss: 0.005156093742698431\n",
      "Epoch 58 / 200 | iteration 30 / 171 | Total Loss: 4.169100284576416 | KNN Loss: 4.159172058105469 | CLS Loss: 0.009928187355399132\n",
      "Epoch 58 / 200 | iteration 40 / 171 | Total Loss: 4.204327583312988 | KNN Loss: 4.1626081466674805 | CLS Loss: 0.04171949252486229\n",
      "Epoch 58 / 200 | iteration 50 / 171 | Total Loss: 4.165009498596191 | KNN Loss: 4.150618553161621 | CLS Loss: 0.014390879310667515\n",
      "Epoch 58 / 200 | iteration 60 / 171 | Total Loss: 4.133999824523926 | KNN Loss: 4.128791809082031 | CLS Loss: 0.005207820329815149\n",
      "Epoch 58 / 200 | iteration 70 / 171 | Total Loss: 4.170565605163574 | KNN Loss: 4.162936687469482 | CLS Loss: 0.007628708146512508\n",
      "Epoch 58 / 200 | iteration 80 / 171 | Total Loss: 4.249900817871094 | KNN Loss: 4.2271409034729 | CLS Loss: 0.022760070860385895\n",
      "Epoch 58 / 200 | iteration 90 / 171 | Total Loss: 4.206695079803467 | KNN Loss: 4.169635772705078 | CLS Loss: 0.03705921396613121\n",
      "Epoch 58 / 200 | iteration 100 / 171 | Total Loss: 4.1550164222717285 | KNN Loss: 4.123245716094971 | CLS Loss: 0.03177059069275856\n",
      "Epoch 58 / 200 | iteration 110 / 171 | Total Loss: 4.186889171600342 | KNN Loss: 4.146402359008789 | CLS Loss: 0.040486738085746765\n",
      "Epoch 58 / 200 | iteration 120 / 171 | Total Loss: 4.129106044769287 | KNN Loss: 4.120144367218018 | CLS Loss: 0.008961769752204418\n",
      "Epoch 58 / 200 | iteration 130 / 171 | Total Loss: 4.186734676361084 | KNN Loss: 4.176161289215088 | CLS Loss: 0.010573492385447025\n",
      "Epoch 58 / 200 | iteration 140 / 171 | Total Loss: 4.156392574310303 | KNN Loss: 4.133727550506592 | CLS Loss: 0.022664830088615417\n",
      "Epoch 58 / 200 | iteration 150 / 171 | Total Loss: 4.168537616729736 | KNN Loss: 4.151352882385254 | CLS Loss: 0.017184698954224586\n",
      "Epoch 58 / 200 | iteration 160 / 171 | Total Loss: 4.142664909362793 | KNN Loss: 4.137765884399414 | CLS Loss: 0.004898799117654562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 / 200 | iteration 170 / 171 | Total Loss: 4.156519412994385 | KNN Loss: 4.12684440612793 | CLS Loss: 0.029674824327230453\n",
      "Epoch: 058, Loss: 4.1752, Train: 0.9953, Valid: 0.9870, Best: 0.9870\n",
      "Epoch 59 / 200 | iteration 0 / 171 | Total Loss: 4.228031635284424 | KNN Loss: 4.2226080894470215 | CLS Loss: 0.005423477850854397\n",
      "Epoch 59 / 200 | iteration 10 / 171 | Total Loss: 4.157278060913086 | KNN Loss: 4.120929718017578 | CLS Loss: 0.03634849563241005\n",
      "Epoch 59 / 200 | iteration 20 / 171 | Total Loss: 4.184452056884766 | KNN Loss: 4.171756267547607 | CLS Loss: 0.012695957906544209\n",
      "Epoch 59 / 200 | iteration 30 / 171 | Total Loss: 4.147996425628662 | KNN Loss: 4.115864276885986 | CLS Loss: 0.032132092863321304\n",
      "Epoch 59 / 200 | iteration 40 / 171 | Total Loss: 4.15817928314209 | KNN Loss: 4.138386249542236 | CLS Loss: 0.01979309320449829\n",
      "Epoch 59 / 200 | iteration 50 / 171 | Total Loss: 4.180298328399658 | KNN Loss: 4.169809341430664 | CLS Loss: 0.010488756000995636\n",
      "Epoch 59 / 200 | iteration 60 / 171 | Total Loss: 4.175069808959961 | KNN Loss: 4.13596773147583 | CLS Loss: 0.03910229355096817\n",
      "Epoch 59 / 200 | iteration 70 / 171 | Total Loss: 4.156937599182129 | KNN Loss: 4.122160911560059 | CLS Loss: 0.03477649390697479\n",
      "Epoch 59 / 200 | iteration 80 / 171 | Total Loss: 4.178062915802002 | KNN Loss: 4.1714043617248535 | CLS Loss: 0.006658473052084446\n",
      "Epoch 59 / 200 | iteration 90 / 171 | Total Loss: 4.179328918457031 | KNN Loss: 4.1615777015686035 | CLS Loss: 0.01775105483829975\n",
      "Epoch 59 / 200 | iteration 100 / 171 | Total Loss: 4.182901859283447 | KNN Loss: 4.153692722320557 | CLS Loss: 0.029209228232502937\n",
      "Epoch 59 / 200 | iteration 110 / 171 | Total Loss: 4.184607982635498 | KNN Loss: 4.137737274169922 | CLS Loss: 0.046870552003383636\n",
      "Epoch 59 / 200 | iteration 120 / 171 | Total Loss: 4.2017107009887695 | KNN Loss: 4.17905855178833 | CLS Loss: 0.022652214393019676\n",
      "Epoch 59 / 200 | iteration 130 / 171 | Total Loss: 4.182961940765381 | KNN Loss: 4.1576948165893555 | CLS Loss: 0.025267118588089943\n",
      "Epoch 59 / 200 | iteration 140 / 171 | Total Loss: 4.2024993896484375 | KNN Loss: 4.1839599609375 | CLS Loss: 0.01853954792022705\n",
      "Epoch 59 / 200 | iteration 150 / 171 | Total Loss: 4.144589900970459 | KNN Loss: 4.1347784996032715 | CLS Loss: 0.009811356663703918\n",
      "Epoch 59 / 200 | iteration 160 / 171 | Total Loss: 4.187954902648926 | KNN Loss: 4.141598224639893 | CLS Loss: 0.046356718987226486\n",
      "Epoch 59 / 200 | iteration 170 / 171 | Total Loss: 4.210069179534912 | KNN Loss: 4.16976261138916 | CLS Loss: 0.040306456387043\n",
      "Epoch: 059, Loss: 4.1754, Train: 0.9951, Valid: 0.9864, Best: 0.9870\n",
      "Epoch 60 / 200 | iteration 0 / 171 | Total Loss: 4.143639087677002 | KNN Loss: 4.132877826690674 | CLS Loss: 0.01076105423271656\n",
      "Epoch 60 / 200 | iteration 10 / 171 | Total Loss: 4.167516708374023 | KNN Loss: 4.148383617401123 | CLS Loss: 0.01913309469819069\n",
      "Epoch 60 / 200 | iteration 20 / 171 | Total Loss: 4.167579174041748 | KNN Loss: 4.155388832092285 | CLS Loss: 0.012190336361527443\n",
      "Epoch 60 / 200 | iteration 30 / 171 | Total Loss: 4.140740871429443 | KNN Loss: 4.130341529846191 | CLS Loss: 0.010399292223155499\n",
      "Epoch 60 / 200 | iteration 40 / 171 | Total Loss: 4.160304546356201 | KNN Loss: 4.1398420333862305 | CLS Loss: 0.02046274021267891\n",
      "Epoch 60 / 200 | iteration 50 / 171 | Total Loss: 4.162814140319824 | KNN Loss: 4.151426792144775 | CLS Loss: 0.011387265287339687\n",
      "Epoch 60 / 200 | iteration 60 / 171 | Total Loss: 4.240706443786621 | KNN Loss: 4.206847667694092 | CLS Loss: 0.03385885804891586\n",
      "Epoch 60 / 200 | iteration 70 / 171 | Total Loss: 4.161718368530273 | KNN Loss: 4.1367926597595215 | CLS Loss: 0.02492578513920307\n",
      "Epoch 60 / 200 | iteration 80 / 171 | Total Loss: 4.17081356048584 | KNN Loss: 4.156630039215088 | CLS Loss: 0.014183327555656433\n",
      "Epoch 60 / 200 | iteration 90 / 171 | Total Loss: 4.150981426239014 | KNN Loss: 4.143370151519775 | CLS Loss: 0.007611430715769529\n",
      "Epoch 60 / 200 | iteration 100 / 171 | Total Loss: 4.220433235168457 | KNN Loss: 4.207586288452148 | CLS Loss: 0.01284694578498602\n",
      "Epoch 60 / 200 | iteration 110 / 171 | Total Loss: 4.1624298095703125 | KNN Loss: 4.15514612197876 | CLS Loss: 0.007283821236342192\n",
      "Epoch 60 / 200 | iteration 120 / 171 | Total Loss: 4.126367092132568 | KNN Loss: 4.1150078773498535 | CLS Loss: 0.011359219439327717\n",
      "Epoch 60 / 200 | iteration 130 / 171 | Total Loss: 4.115117073059082 | KNN Loss: 4.110458850860596 | CLS Loss: 0.004658452235162258\n",
      "Epoch 60 / 200 | iteration 140 / 171 | Total Loss: 4.173962593078613 | KNN Loss: 4.166317939758301 | CLS Loss: 0.007644427474588156\n",
      "Epoch 60 / 200 | iteration 150 / 171 | Total Loss: 4.20488977432251 | KNN Loss: 4.1764020919799805 | CLS Loss: 0.028487548232078552\n",
      "Epoch 60 / 200 | iteration 160 / 171 | Total Loss: 4.208680152893066 | KNN Loss: 4.1985697746276855 | CLS Loss: 0.010110564529895782\n",
      "Epoch 60 / 200 | iteration 170 / 171 | Total Loss: 4.194886684417725 | KNN Loss: 4.17942476272583 | CLS Loss: 0.015461795963346958\n",
      "Epoch: 060, Loss: 4.1776, Train: 0.9898, Valid: 0.9794, Best: 0.9870\n",
      "Epoch 61 / 200 | iteration 0 / 171 | Total Loss: 4.2214789390563965 | KNN Loss: 4.198244571685791 | CLS Loss: 0.023234263062477112\n",
      "Epoch 61 / 200 | iteration 10 / 171 | Total Loss: 4.1649322509765625 | KNN Loss: 4.15048360824585 | CLS Loss: 0.014448861591517925\n",
      "Epoch 61 / 200 | iteration 20 / 171 | Total Loss: 4.155425071716309 | KNN Loss: 4.143435955047607 | CLS Loss: 0.011989206075668335\n",
      "Epoch 61 / 200 | iteration 30 / 171 | Total Loss: 4.1745123863220215 | KNN Loss: 4.161903381347656 | CLS Loss: 0.012608969584107399\n",
      "Epoch 61 / 200 | iteration 40 / 171 | Total Loss: 4.19273042678833 | KNN Loss: 4.166557312011719 | CLS Loss: 0.026173068210482597\n",
      "Epoch 61 / 200 | iteration 50 / 171 | Total Loss: 4.134157180786133 | KNN Loss: 4.12688684463501 | CLS Loss: 0.00727044278755784\n",
      "Epoch 61 / 200 | iteration 60 / 171 | Total Loss: 4.145838737487793 | KNN Loss: 4.12672233581543 | CLS Loss: 0.019116465002298355\n",
      "Epoch 61 / 200 | iteration 70 / 171 | Total Loss: 4.170391082763672 | KNN Loss: 4.149775981903076 | CLS Loss: 0.020615149289369583\n",
      "Epoch 61 / 200 | iteration 80 / 171 | Total Loss: 4.141470909118652 | KNN Loss: 4.138844013214111 | CLS Loss: 0.00262704910710454\n",
      "Epoch 61 / 200 | iteration 90 / 171 | Total Loss: 4.156806468963623 | KNN Loss: 4.146173477172852 | CLS Loss: 0.010632991790771484\n",
      "Epoch 61 / 200 | iteration 100 / 171 | Total Loss: 4.190384864807129 | KNN Loss: 4.15408182144165 | CLS Loss: 0.036302898079156876\n",
      "Epoch 61 / 200 | iteration 110 / 171 | Total Loss: 4.153487205505371 | KNN Loss: 4.137486457824707 | CLS Loss: 0.016000540927052498\n",
      "Epoch 61 / 200 | iteration 120 / 171 | Total Loss: 4.235722064971924 | KNN Loss: 4.214022159576416 | CLS Loss: 0.021700037643313408\n",
      "Epoch 61 / 200 | iteration 130 / 171 | Total Loss: 4.180362701416016 | KNN Loss: 4.169378280639648 | CLS Loss: 0.010984391905367374\n",
      "Epoch 61 / 200 | iteration 140 / 171 | Total Loss: 4.167073726654053 | KNN Loss: 4.150214195251465 | CLS Loss: 0.016859551891684532\n",
      "Epoch 61 / 200 | iteration 150 / 171 | Total Loss: 4.198515892028809 | KNN Loss: 4.1499409675598145 | CLS Loss: 0.048574961721897125\n",
      "Epoch 61 / 200 | iteration 160 / 171 | Total Loss: 4.176751613616943 | KNN Loss: 4.155857563018799 | CLS Loss: 0.020893849432468414\n",
      "Epoch 61 / 200 | iteration 170 / 171 | Total Loss: 4.155048847198486 | KNN Loss: 4.142940998077393 | CLS Loss: 0.012108057737350464\n",
      "Epoch: 061, Loss: 4.1737, Train: 0.9952, Valid: 0.9871, Best: 0.9871\n",
      "Epoch 62 / 200 | iteration 0 / 171 | Total Loss: 4.1695556640625 | KNN Loss: 4.153504848480225 | CLS Loss: 0.016050904989242554\n",
      "Epoch 62 / 200 | iteration 10 / 171 | Total Loss: 4.193118095397949 | KNN Loss: 4.18263053894043 | CLS Loss: 0.01048773992806673\n",
      "Epoch 62 / 200 | iteration 20 / 171 | Total Loss: 4.162054538726807 | KNN Loss: 4.14447021484375 | CLS Loss: 0.017584366723895073\n",
      "Epoch 62 / 200 | iteration 30 / 171 | Total Loss: 4.1822285652160645 | KNN Loss: 4.159558296203613 | CLS Loss: 0.022670090198516846\n",
      "Epoch 62 / 200 | iteration 40 / 171 | Total Loss: 4.203390598297119 | KNN Loss: 4.18133544921875 | CLS Loss: 0.022055286914110184\n",
      "Epoch 62 / 200 | iteration 50 / 171 | Total Loss: 4.195376873016357 | KNN Loss: 4.154242038726807 | CLS Loss: 0.041134826838970184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 / 200 | iteration 60 / 171 | Total Loss: 4.244478225708008 | KNN Loss: 4.214363098144531 | CLS Loss: 0.030115094035863876\n",
      "Epoch 62 / 200 | iteration 70 / 171 | Total Loss: 4.1530656814575195 | KNN Loss: 4.145619869232178 | CLS Loss: 0.007445921655744314\n",
      "Epoch 62 / 200 | iteration 80 / 171 | Total Loss: 4.192887783050537 | KNN Loss: 4.178734302520752 | CLS Loss: 0.014153496362268925\n",
      "Epoch 62 / 200 | iteration 90 / 171 | Total Loss: 4.157037734985352 | KNN Loss: 4.148182392120361 | CLS Loss: 0.008855515159666538\n",
      "Epoch 62 / 200 | iteration 100 / 171 | Total Loss: 4.22615909576416 | KNN Loss: 4.188061714172363 | CLS Loss: 0.03809729963541031\n",
      "Epoch 62 / 200 | iteration 110 / 171 | Total Loss: 4.161397457122803 | KNN Loss: 4.142932891845703 | CLS Loss: 0.018464647233486176\n",
      "Epoch 62 / 200 | iteration 120 / 171 | Total Loss: 4.121703624725342 | KNN Loss: 4.11791467666626 | CLS Loss: 0.0037890055682510138\n",
      "Epoch 62 / 200 | iteration 130 / 171 | Total Loss: 4.186787128448486 | KNN Loss: 4.176943302154541 | CLS Loss: 0.009843741543591022\n",
      "Epoch 62 / 200 | iteration 140 / 171 | Total Loss: 4.175090789794922 | KNN Loss: 4.151646137237549 | CLS Loss: 0.023444855585694313\n",
      "Epoch 62 / 200 | iteration 150 / 171 | Total Loss: 4.174793243408203 | KNN Loss: 4.147317886352539 | CLS Loss: 0.02747531607747078\n",
      "Epoch 62 / 200 | iteration 160 / 171 | Total Loss: 4.13676643371582 | KNN Loss: 4.128789901733398 | CLS Loss: 0.007976437918841839\n",
      "Epoch 62 / 200 | iteration 170 / 171 | Total Loss: 4.133983612060547 | KNN Loss: 4.128279685974121 | CLS Loss: 0.005703945644199848\n",
      "Epoch: 062, Loss: 4.1752, Train: 0.9956, Valid: 0.9868, Best: 0.9871\n",
      "Epoch 63 / 200 | iteration 0 / 171 | Total Loss: 4.175708770751953 | KNN Loss: 4.168926239013672 | CLS Loss: 0.006782663054764271\n",
      "Epoch 63 / 200 | iteration 10 / 171 | Total Loss: 4.137622356414795 | KNN Loss: 4.130616664886475 | CLS Loss: 0.0070057213306427\n",
      "Epoch 63 / 200 | iteration 20 / 171 | Total Loss: 4.144903182983398 | KNN Loss: 4.123470783233643 | CLS Loss: 0.021432358771562576\n",
      "Epoch 63 / 200 | iteration 30 / 171 | Total Loss: 4.173662185668945 | KNN Loss: 4.143884181976318 | CLS Loss: 0.029778094962239265\n",
      "Epoch 63 / 200 | iteration 40 / 171 | Total Loss: 4.145147323608398 | KNN Loss: 4.136221408843994 | CLS Loss: 0.008925873786211014\n",
      "Epoch 63 / 200 | iteration 50 / 171 | Total Loss: 4.1812005043029785 | KNN Loss: 4.149679660797119 | CLS Loss: 0.031520795077085495\n",
      "Epoch 63 / 200 | iteration 60 / 171 | Total Loss: 4.207791328430176 | KNN Loss: 4.190323352813721 | CLS Loss: 0.01746799238026142\n",
      "Epoch 63 / 200 | iteration 70 / 171 | Total Loss: 4.180614948272705 | KNN Loss: 4.160783767700195 | CLS Loss: 0.01983107626438141\n",
      "Epoch 63 / 200 | iteration 80 / 171 | Total Loss: 4.1444993019104 | KNN Loss: 4.135677337646484 | CLS Loss: 0.008821889758110046\n",
      "Epoch 63 / 200 | iteration 90 / 171 | Total Loss: 4.184544563293457 | KNN Loss: 4.161360740661621 | CLS Loss: 0.023183615878224373\n",
      "Epoch 63 / 200 | iteration 100 / 171 | Total Loss: 4.194259166717529 | KNN Loss: 4.171984672546387 | CLS Loss: 0.022274425253272057\n",
      "Epoch 63 / 200 | iteration 110 / 171 | Total Loss: 4.175973415374756 | KNN Loss: 4.166957378387451 | CLS Loss: 0.009016108699142933\n",
      "Epoch 63 / 200 | iteration 120 / 171 | Total Loss: 4.144654750823975 | KNN Loss: 4.135509967803955 | CLS Loss: 0.009144590236246586\n",
      "Epoch 63 / 200 | iteration 130 / 171 | Total Loss: 4.167112350463867 | KNN Loss: 4.142665863037109 | CLS Loss: 0.024446683004498482\n",
      "Epoch 63 / 200 | iteration 140 / 171 | Total Loss: 4.165521621704102 | KNN Loss: 4.137885570526123 | CLS Loss: 0.027636045590043068\n",
      "Epoch 63 / 200 | iteration 150 / 171 | Total Loss: 4.204239368438721 | KNN Loss: 4.175184726715088 | CLS Loss: 0.029054637998342514\n",
      "Epoch 63 / 200 | iteration 160 / 171 | Total Loss: 4.164806365966797 | KNN Loss: 4.148380279541016 | CLS Loss: 0.01642630435526371\n",
      "Epoch 63 / 200 | iteration 170 / 171 | Total Loss: 4.110476970672607 | KNN Loss: 4.1046929359436035 | CLS Loss: 0.005783949978649616\n",
      "Epoch: 063, Loss: 4.1686, Train: 0.9944, Valid: 0.9867, Best: 0.9871\n",
      "Epoch 64 / 200 | iteration 0 / 171 | Total Loss: 4.167189121246338 | KNN Loss: 4.15110969543457 | CLS Loss: 0.016079595312476158\n",
      "Epoch 64 / 200 | iteration 10 / 171 | Total Loss: 4.1750688552856445 | KNN Loss: 4.1519341468811035 | CLS Loss: 0.0231346245855093\n",
      "Epoch 64 / 200 | iteration 20 / 171 | Total Loss: 4.189752101898193 | KNN Loss: 4.178516387939453 | CLS Loss: 0.01123584434390068\n",
      "Epoch 64 / 200 | iteration 30 / 171 | Total Loss: 4.148068904876709 | KNN Loss: 4.144902229309082 | CLS Loss: 0.0031666525173932314\n",
      "Epoch 64 / 200 | iteration 40 / 171 | Total Loss: 4.173707008361816 | KNN Loss: 4.15373420715332 | CLS Loss: 0.01997303031384945\n",
      "Epoch 64 / 200 | iteration 50 / 171 | Total Loss: 4.213079929351807 | KNN Loss: 4.187280178070068 | CLS Loss: 0.02579987794160843\n",
      "Epoch 64 / 200 | iteration 60 / 171 | Total Loss: 4.234850883483887 | KNN Loss: 4.180531978607178 | CLS Loss: 0.054318927228450775\n",
      "Epoch 64 / 200 | iteration 70 / 171 | Total Loss: 4.157501697540283 | KNN Loss: 4.137772560119629 | CLS Loss: 0.01972891204059124\n",
      "Epoch 64 / 200 | iteration 80 / 171 | Total Loss: 4.2068047523498535 | KNN Loss: 4.20176362991333 | CLS Loss: 0.005041286814957857\n",
      "Epoch 64 / 200 | iteration 90 / 171 | Total Loss: 4.1468987464904785 | KNN Loss: 4.139330863952637 | CLS Loss: 0.007567877881228924\n",
      "Epoch 64 / 200 | iteration 100 / 171 | Total Loss: 4.180947303771973 | KNN Loss: 4.175089359283447 | CLS Loss: 0.005857976619154215\n",
      "Epoch 64 / 200 | iteration 110 / 171 | Total Loss: 4.153831481933594 | KNN Loss: 4.136771202087402 | CLS Loss: 0.017060279846191406\n",
      "Epoch 64 / 200 | iteration 120 / 171 | Total Loss: 4.185225009918213 | KNN Loss: 4.161436080932617 | CLS Loss: 0.02378888800740242\n",
      "Epoch 64 / 200 | iteration 130 / 171 | Total Loss: 4.1802802085876465 | KNN Loss: 4.168962478637695 | CLS Loss: 0.01131795160472393\n",
      "Epoch 64 / 200 | iteration 140 / 171 | Total Loss: 4.173274993896484 | KNN Loss: 4.152735710144043 | CLS Loss: 0.020539196208119392\n",
      "Epoch 64 / 200 | iteration 150 / 171 | Total Loss: 4.159019470214844 | KNN Loss: 4.141733169555664 | CLS Loss: 0.017286080867052078\n",
      "Epoch 64 / 200 | iteration 160 / 171 | Total Loss: 4.152770519256592 | KNN Loss: 4.143947124481201 | CLS Loss: 0.008823164738714695\n",
      "Epoch 64 / 200 | iteration 170 / 171 | Total Loss: 4.209372043609619 | KNN Loss: 4.167107105255127 | CLS Loss: 0.042265135794878006\n",
      "Epoch: 064, Loss: 4.1749, Train: 0.9954, Valid: 0.9857, Best: 0.9871\n",
      "Epoch 65 / 200 | iteration 0 / 171 | Total Loss: 4.140468597412109 | KNN Loss: 4.134459495544434 | CLS Loss: 0.006009306758642197\n",
      "Epoch 65 / 200 | iteration 10 / 171 | Total Loss: 4.224895477294922 | KNN Loss: 4.206454753875732 | CLS Loss: 0.018440790474414825\n",
      "Epoch 65 / 200 | iteration 20 / 171 | Total Loss: 4.159619331359863 | KNN Loss: 4.149583339691162 | CLS Loss: 0.010036177933216095\n",
      "Epoch 65 / 200 | iteration 30 / 171 | Total Loss: 4.174957752227783 | KNN Loss: 4.148151874542236 | CLS Loss: 0.026805706322193146\n",
      "Epoch 65 / 200 | iteration 40 / 171 | Total Loss: 4.1442766189575195 | KNN Loss: 4.130958080291748 | CLS Loss: 0.013318700715899467\n",
      "Epoch 65 / 200 | iteration 50 / 171 | Total Loss: 4.186100006103516 | KNN Loss: 4.1650071144104 | CLS Loss: 0.02109266072511673\n",
      "Epoch 65 / 200 | iteration 60 / 171 | Total Loss: 4.177351951599121 | KNN Loss: 4.1599249839782715 | CLS Loss: 0.017426922917366028\n",
      "Epoch 65 / 200 | iteration 70 / 171 | Total Loss: 4.123509883880615 | KNN Loss: 4.114338397979736 | CLS Loss: 0.009171690791845322\n",
      "Epoch 65 / 200 | iteration 80 / 171 | Total Loss: 4.1641459465026855 | KNN Loss: 4.150315761566162 | CLS Loss: 0.013829977251589298\n",
      "Epoch 65 / 200 | iteration 90 / 171 | Total Loss: 4.188598155975342 | KNN Loss: 4.165340423583984 | CLS Loss: 0.02325758896768093\n",
      "Epoch 65 / 200 | iteration 100 / 171 | Total Loss: 4.227217674255371 | KNN Loss: 4.192121982574463 | CLS Loss: 0.035095855593681335\n",
      "Epoch 65 / 200 | iteration 110 / 171 | Total Loss: 4.184606075286865 | KNN Loss: 4.163153648376465 | CLS Loss: 0.021452466025948524\n",
      "Epoch 65 / 200 | iteration 120 / 171 | Total Loss: 4.163449764251709 | KNN Loss: 4.154881477355957 | CLS Loss: 0.008568310178816319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 / 200 | iteration 130 / 171 | Total Loss: 4.148693084716797 | KNN Loss: 4.131814956665039 | CLS Loss: 0.016878211870789528\n",
      "Epoch 65 / 200 | iteration 140 / 171 | Total Loss: 4.119271278381348 | KNN Loss: 4.104356288909912 | CLS Loss: 0.014915036037564278\n",
      "Epoch 65 / 200 | iteration 150 / 171 | Total Loss: 4.254508018493652 | KNN Loss: 4.225152492523193 | CLS Loss: 0.029355740174651146\n",
      "Epoch 65 / 200 | iteration 160 / 171 | Total Loss: 4.1954264640808105 | KNN Loss: 4.18572473526001 | CLS Loss: 0.009701497852802277\n",
      "Epoch 65 / 200 | iteration 170 / 171 | Total Loss: 4.151735782623291 | KNN Loss: 4.1447434425354 | CLS Loss: 0.006992333568632603\n",
      "Epoch: 065, Loss: 4.1721, Train: 0.9947, Valid: 0.9851, Best: 0.9871\n",
      "Epoch 66 / 200 | iteration 0 / 171 | Total Loss: 4.20805025100708 | KNN Loss: 4.175268650054932 | CLS Loss: 0.032781705260276794\n",
      "Epoch 66 / 200 | iteration 10 / 171 | Total Loss: 4.166605472564697 | KNN Loss: 4.160793304443359 | CLS Loss: 0.005812346935272217\n",
      "Epoch 66 / 200 | iteration 20 / 171 | Total Loss: 4.130338668823242 | KNN Loss: 4.117329120635986 | CLS Loss: 0.013009362854063511\n",
      "Epoch 66 / 200 | iteration 30 / 171 | Total Loss: 4.168562412261963 | KNN Loss: 4.164483547210693 | CLS Loss: 0.004079008009284735\n",
      "Epoch 66 / 200 | iteration 40 / 171 | Total Loss: 4.204039573669434 | KNN Loss: 4.191934585571289 | CLS Loss: 0.012104777619242668\n",
      "Epoch 66 / 200 | iteration 50 / 171 | Total Loss: 4.187491416931152 | KNN Loss: 4.1666178703308105 | CLS Loss: 0.020873764529824257\n",
      "Epoch 66 / 200 | iteration 60 / 171 | Total Loss: 4.141767501831055 | KNN Loss: 4.1230573654174805 | CLS Loss: 0.01871035248041153\n",
      "Epoch 66 / 200 | iteration 70 / 171 | Total Loss: 4.187600612640381 | KNN Loss: 4.173110008239746 | CLS Loss: 0.01449060719460249\n",
      "Epoch 66 / 200 | iteration 80 / 171 | Total Loss: 4.171334743499756 | KNN Loss: 4.145421504974365 | CLS Loss: 0.02591312862932682\n",
      "Epoch 66 / 200 | iteration 90 / 171 | Total Loss: 4.214027404785156 | KNN Loss: 4.196259498596191 | CLS Loss: 0.017768006771802902\n",
      "Epoch 66 / 200 | iteration 100 / 171 | Total Loss: 4.176660537719727 | KNN Loss: 4.143901824951172 | CLS Loss: 0.032758843153715134\n",
      "Epoch 66 / 200 | iteration 110 / 171 | Total Loss: 4.168136119842529 | KNN Loss: 4.163084030151367 | CLS Loss: 0.005052005872130394\n",
      "Epoch 66 / 200 | iteration 120 / 171 | Total Loss: 4.172436237335205 | KNN Loss: 4.142378330230713 | CLS Loss: 0.03005789779126644\n",
      "Epoch 66 / 200 | iteration 130 / 171 | Total Loss: 4.160696029663086 | KNN Loss: 4.150273323059082 | CLS Loss: 0.01042291708290577\n",
      "Epoch 66 / 200 | iteration 140 / 171 | Total Loss: 4.1902289390563965 | KNN Loss: 4.163254261016846 | CLS Loss: 0.026974821463227272\n",
      "Epoch 66 / 200 | iteration 150 / 171 | Total Loss: 4.189162254333496 | KNN Loss: 4.145011901855469 | CLS Loss: 0.044150251895189285\n",
      "Epoch 66 / 200 | iteration 160 / 171 | Total Loss: 4.165348052978516 | KNN Loss: 4.143529415130615 | CLS Loss: 0.021818432956933975\n",
      "Epoch 66 / 200 | iteration 170 / 171 | Total Loss: 4.164069175720215 | KNN Loss: 4.141177654266357 | CLS Loss: 0.022891681641340256\n",
      "Epoch: 066, Loss: 4.1728, Train: 0.9958, Valid: 0.9863, Best: 0.9871\n",
      "Epoch 67 / 200 | iteration 0 / 171 | Total Loss: 4.14488410949707 | KNN Loss: 4.138851642608643 | CLS Loss: 0.006032454315572977\n",
      "Epoch 67 / 200 | iteration 10 / 171 | Total Loss: 4.224029541015625 | KNN Loss: 4.202214241027832 | CLS Loss: 0.021815456449985504\n",
      "Epoch 67 / 200 | iteration 20 / 171 | Total Loss: 4.1399641036987305 | KNN Loss: 4.128921985626221 | CLS Loss: 0.011042099446058273\n",
      "Epoch 67 / 200 | iteration 30 / 171 | Total Loss: 4.167049884796143 | KNN Loss: 4.151960372924805 | CLS Loss: 0.015089280903339386\n",
      "Epoch 67 / 200 | iteration 40 / 171 | Total Loss: 4.102209091186523 | KNN Loss: 4.092811584472656 | CLS Loss: 0.009397360496222973\n",
      "Epoch 67 / 200 | iteration 50 / 171 | Total Loss: 4.172130107879639 | KNN Loss: 4.158061981201172 | CLS Loss: 0.014067916199564934\n",
      "Epoch 67 / 200 | iteration 60 / 171 | Total Loss: 4.174209117889404 | KNN Loss: 4.153803825378418 | CLS Loss: 0.020405156537890434\n",
      "Epoch 67 / 200 | iteration 70 / 171 | Total Loss: 4.217357635498047 | KNN Loss: 4.195194721221924 | CLS Loss: 0.022163057699799538\n",
      "Epoch 67 / 200 | iteration 80 / 171 | Total Loss: 4.184106349945068 | KNN Loss: 4.16251277923584 | CLS Loss: 0.021593574434518814\n",
      "Epoch 67 / 200 | iteration 90 / 171 | Total Loss: 4.148869514465332 | KNN Loss: 4.144197463989258 | CLS Loss: 0.0046722679398953915\n",
      "Epoch 67 / 200 | iteration 100 / 171 | Total Loss: 4.2514166831970215 | KNN Loss: 4.222193717956543 | CLS Loss: 0.02922275848686695\n",
      "Epoch 67 / 200 | iteration 110 / 171 | Total Loss: 4.15829610824585 | KNN Loss: 4.150814056396484 | CLS Loss: 0.00748211657628417\n",
      "Epoch 67 / 200 | iteration 120 / 171 | Total Loss: 4.153491973876953 | KNN Loss: 4.138038635253906 | CLS Loss: 0.015453370288014412\n",
      "Epoch 67 / 200 | iteration 130 / 171 | Total Loss: 4.207815647125244 | KNN Loss: 4.185535430908203 | CLS Loss: 0.022280406206846237\n",
      "Epoch 67 / 200 | iteration 140 / 171 | Total Loss: 4.153992176055908 | KNN Loss: 4.131038665771484 | CLS Loss: 0.022953378036618233\n",
      "Epoch 67 / 200 | iteration 150 / 171 | Total Loss: 4.184632301330566 | KNN Loss: 4.1794257164001465 | CLS Loss: 0.005206538364291191\n",
      "Epoch 67 / 200 | iteration 160 / 171 | Total Loss: 4.134505748748779 | KNN Loss: 4.105264186859131 | CLS Loss: 0.02924143522977829\n",
      "Epoch 67 / 200 | iteration 170 / 171 | Total Loss: 4.18647575378418 | KNN Loss: 4.16448450088501 | CLS Loss: 0.021991334855556488\n",
      "Epoch: 067, Loss: 4.1661, Train: 0.9949, Valid: 0.9854, Best: 0.9871\n",
      "Epoch 68 / 200 | iteration 0 / 171 | Total Loss: 4.208415508270264 | KNN Loss: 4.184370994567871 | CLS Loss: 0.02404431439936161\n",
      "Epoch 68 / 200 | iteration 10 / 171 | Total Loss: 4.127252578735352 | KNN Loss: 4.1215009689331055 | CLS Loss: 0.005751545540988445\n",
      "Epoch 68 / 200 | iteration 20 / 171 | Total Loss: 4.14740514755249 | KNN Loss: 4.134199142456055 | CLS Loss: 0.013206188566982746\n",
      "Epoch 68 / 200 | iteration 30 / 171 | Total Loss: 4.169529914855957 | KNN Loss: 4.141394138336182 | CLS Loss: 0.02813572995364666\n",
      "Epoch 68 / 200 | iteration 40 / 171 | Total Loss: 4.195785045623779 | KNN Loss: 4.185500144958496 | CLS Loss: 0.010285104624927044\n",
      "Epoch 68 / 200 | iteration 50 / 171 | Total Loss: 4.151119232177734 | KNN Loss: 4.139520168304443 | CLS Loss: 0.01159910298883915\n",
      "Epoch 68 / 200 | iteration 60 / 171 | Total Loss: 4.161879539489746 | KNN Loss: 4.158313751220703 | CLS Loss: 0.003565825056284666\n",
      "Epoch 68 / 200 | iteration 70 / 171 | Total Loss: 4.160363674163818 | KNN Loss: 4.145111083984375 | CLS Loss: 0.015252647921442986\n",
      "Epoch 68 / 200 | iteration 80 / 171 | Total Loss: 4.170845031738281 | KNN Loss: 4.1529459953308105 | CLS Loss: 0.017898868769407272\n",
      "Epoch 68 / 200 | iteration 90 / 171 | Total Loss: 4.175067901611328 | KNN Loss: 4.158210277557373 | CLS Loss: 0.016857590526342392\n",
      "Epoch 68 / 200 | iteration 100 / 171 | Total Loss: 4.26569128036499 | KNN Loss: 4.248901844024658 | CLS Loss: 0.016789540648460388\n",
      "Epoch 68 / 200 | iteration 110 / 171 | Total Loss: 4.161373138427734 | KNN Loss: 4.14690637588501 | CLS Loss: 0.014466707594692707\n",
      "Epoch 68 / 200 | iteration 120 / 171 | Total Loss: 4.162512302398682 | KNN Loss: 4.140220642089844 | CLS Loss: 0.022291740402579308\n",
      "Epoch 68 / 200 | iteration 130 / 171 | Total Loss: 4.131207466125488 | KNN Loss: 4.108373641967773 | CLS Loss: 0.02283378876745701\n",
      "Epoch 68 / 200 | iteration 140 / 171 | Total Loss: 4.186563968658447 | KNN Loss: 4.1630096435546875 | CLS Loss: 0.023554395884275436\n",
      "Epoch 68 / 200 | iteration 150 / 171 | Total Loss: 4.1658196449279785 | KNN Loss: 4.150879383087158 | CLS Loss: 0.014940321445465088\n",
      "Epoch 68 / 200 | iteration 160 / 171 | Total Loss: 4.139119625091553 | KNN Loss: 4.135045051574707 | CLS Loss: 0.004074649419635534\n",
      "Epoch 68 / 200 | iteration 170 / 171 | Total Loss: 4.148980140686035 | KNN Loss: 4.1371564865112305 | CLS Loss: 0.011823450215160847\n",
      "Epoch: 068, Loss: 4.1676, Train: 0.9952, Valid: 0.9850, Best: 0.9871\n",
      "Epoch 69 / 200 | iteration 0 / 171 | Total Loss: 4.182394504547119 | KNN Loss: 4.174986839294434 | CLS Loss: 0.007407450117170811\n",
      "Epoch 69 / 200 | iteration 10 / 171 | Total Loss: 4.112028121948242 | KNN Loss: 4.10664176940918 | CLS Loss: 0.005386156029999256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 / 200 | iteration 20 / 171 | Total Loss: 4.1215715408325195 | KNN Loss: 4.08599853515625 | CLS Loss: 0.03557300195097923\n",
      "Epoch 69 / 200 | iteration 30 / 171 | Total Loss: 4.150624752044678 | KNN Loss: 4.1423773765563965 | CLS Loss: 0.008247347548604012\n",
      "Epoch 69 / 200 | iteration 40 / 171 | Total Loss: 4.166671276092529 | KNN Loss: 4.147459506988525 | CLS Loss: 0.019211871549487114\n",
      "Epoch 69 / 200 | iteration 50 / 171 | Total Loss: 4.177854061126709 | KNN Loss: 4.1699442863464355 | CLS Loss: 0.007909662090241909\n",
      "Epoch 69 / 200 | iteration 60 / 171 | Total Loss: 4.138155937194824 | KNN Loss: 4.1307759284973145 | CLS Loss: 0.0073798056691884995\n",
      "Epoch 69 / 200 | iteration 70 / 171 | Total Loss: 4.173851490020752 | KNN Loss: 4.156491279602051 | CLS Loss: 0.017360180616378784\n",
      "Epoch 69 / 200 | iteration 80 / 171 | Total Loss: 4.178874969482422 | KNN Loss: 4.16314697265625 | CLS Loss: 0.015727803111076355\n",
      "Epoch 69 / 200 | iteration 90 / 171 | Total Loss: 4.151392459869385 | KNN Loss: 4.143454551696777 | CLS Loss: 0.007937771268188953\n",
      "Epoch 69 / 200 | iteration 100 / 171 | Total Loss: 4.130216121673584 | KNN Loss: 4.106961727142334 | CLS Loss: 0.02325453981757164\n",
      "Epoch 69 / 200 | iteration 110 / 171 | Total Loss: 4.168322563171387 | KNN Loss: 4.1511921882629395 | CLS Loss: 0.017130574211478233\n",
      "Epoch 69 / 200 | iteration 120 / 171 | Total Loss: 4.144301414489746 | KNN Loss: 4.131180286407471 | CLS Loss: 0.013121131807565689\n",
      "Epoch 69 / 200 | iteration 130 / 171 | Total Loss: 4.181023597717285 | KNN Loss: 4.161263465881348 | CLS Loss: 0.019760282710194588\n",
      "Epoch 69 / 200 | iteration 140 / 171 | Total Loss: 4.14998722076416 | KNN Loss: 4.129706859588623 | CLS Loss: 0.020280327647924423\n",
      "Epoch 69 / 200 | iteration 150 / 171 | Total Loss: 4.14469051361084 | KNN Loss: 4.133942127227783 | CLS Loss: 0.010748372413218021\n",
      "Epoch 69 / 200 | iteration 160 / 171 | Total Loss: 4.168665409088135 | KNN Loss: 4.145158767700195 | CLS Loss: 0.02350650355219841\n",
      "Epoch 69 / 200 | iteration 170 / 171 | Total Loss: 4.168950080871582 | KNN Loss: 4.155429840087891 | CLS Loss: 0.013520176522433758\n",
      "Epoch: 069, Loss: 4.1644, Train: 0.9953, Valid: 0.9868, Best: 0.9871\n",
      "Epoch 70 / 200 | iteration 0 / 171 | Total Loss: 4.151479244232178 | KNN Loss: 4.128393650054932 | CLS Loss: 0.023085396736860275\n",
      "Epoch 70 / 200 | iteration 10 / 171 | Total Loss: 4.163852691650391 | KNN Loss: 4.151247501373291 | CLS Loss: 0.012605083175003529\n",
      "Epoch 70 / 200 | iteration 20 / 171 | Total Loss: 4.141063213348389 | KNN Loss: 4.12814474105835 | CLS Loss: 0.012918416410684586\n",
      "Epoch 70 / 200 | iteration 30 / 171 | Total Loss: 4.162270545959473 | KNN Loss: 4.147624969482422 | CLS Loss: 0.014645409770309925\n",
      "Epoch 70 / 200 | iteration 40 / 171 | Total Loss: 4.147405624389648 | KNN Loss: 4.134438991546631 | CLS Loss: 0.012966576963663101\n",
      "Epoch 70 / 200 | iteration 50 / 171 | Total Loss: 4.158893585205078 | KNN Loss: 4.15540075302124 | CLS Loss: 0.003492755349725485\n",
      "Epoch 70 / 200 | iteration 60 / 171 | Total Loss: 4.165871620178223 | KNN Loss: 4.159971714019775 | CLS Loss: 0.005899914540350437\n",
      "Epoch 70 / 200 | iteration 70 / 171 | Total Loss: 4.141997814178467 | KNN Loss: 4.135695457458496 | CLS Loss: 0.006302121561020613\n",
      "Epoch 70 / 200 | iteration 80 / 171 | Total Loss: 4.1465678215026855 | KNN Loss: 4.141747951507568 | CLS Loss: 0.004820103291422129\n",
      "Epoch 70 / 200 | iteration 90 / 171 | Total Loss: 4.1496968269348145 | KNN Loss: 4.145216941833496 | CLS Loss: 0.004480106756091118\n",
      "Epoch 70 / 200 | iteration 100 / 171 | Total Loss: 4.156556606292725 | KNN Loss: 4.13694429397583 | CLS Loss: 0.01961216703057289\n",
      "Epoch 70 / 200 | iteration 110 / 171 | Total Loss: 4.217794418334961 | KNN Loss: 4.206835746765137 | CLS Loss: 0.010958629660308361\n",
      "Epoch 70 / 200 | iteration 120 / 171 | Total Loss: 4.165669918060303 | KNN Loss: 4.141608715057373 | CLS Loss: 0.024060998111963272\n",
      "Epoch 70 / 200 | iteration 130 / 171 | Total Loss: 4.161072254180908 | KNN Loss: 4.142737865447998 | CLS Loss: 0.01833449676632881\n",
      "Epoch 70 / 200 | iteration 140 / 171 | Total Loss: 4.154672622680664 | KNN Loss: 4.143277645111084 | CLS Loss: 0.011394757777452469\n",
      "Epoch 70 / 200 | iteration 150 / 171 | Total Loss: 4.1448073387146 | KNN Loss: 4.1263747215271 | CLS Loss: 0.01843242347240448\n",
      "Epoch 70 / 200 | iteration 160 / 171 | Total Loss: 4.197178840637207 | KNN Loss: 4.166271209716797 | CLS Loss: 0.0309076476842165\n",
      "Epoch 70 / 200 | iteration 170 / 171 | Total Loss: 4.1343793869018555 | KNN Loss: 4.128506660461426 | CLS Loss: 0.00587295088917017\n",
      "Epoch: 070, Loss: 4.1633, Train: 0.9958, Valid: 0.9858, Best: 0.9871\n",
      "Epoch 71 / 200 | iteration 0 / 171 | Total Loss: 4.156848907470703 | KNN Loss: 4.1327924728393555 | CLS Loss: 0.024056369438767433\n",
      "Epoch 71 / 200 | iteration 10 / 171 | Total Loss: 4.215757846832275 | KNN Loss: 4.203376293182373 | CLS Loss: 0.012381556443870068\n",
      "Epoch 71 / 200 | iteration 20 / 171 | Total Loss: 4.13332986831665 | KNN Loss: 4.114340305328369 | CLS Loss: 0.01898977905511856\n",
      "Epoch 71 / 200 | iteration 30 / 171 | Total Loss: 4.143860340118408 | KNN Loss: 4.137138366699219 | CLS Loss: 0.006722184829413891\n",
      "Epoch 71 / 200 | iteration 40 / 171 | Total Loss: 4.139089107513428 | KNN Loss: 4.136509895324707 | CLS Loss: 0.002579143038019538\n",
      "Epoch 71 / 200 | iteration 50 / 171 | Total Loss: 4.195384502410889 | KNN Loss: 4.162605285644531 | CLS Loss: 0.03277900069952011\n",
      "Epoch 71 / 200 | iteration 60 / 171 | Total Loss: 4.188374996185303 | KNN Loss: 4.175501346588135 | CLS Loss: 0.012873816303908825\n",
      "Epoch 71 / 200 | iteration 70 / 171 | Total Loss: 4.213683605194092 | KNN Loss: 4.196140766143799 | CLS Loss: 0.01754293031990528\n",
      "Epoch 71 / 200 | iteration 80 / 171 | Total Loss: 4.178212642669678 | KNN Loss: 4.170840263366699 | CLS Loss: 0.007372588850557804\n",
      "Epoch 71 / 200 | iteration 90 / 171 | Total Loss: 4.207165718078613 | KNN Loss: 4.197077751159668 | CLS Loss: 0.010088114999234676\n",
      "Epoch 71 / 200 | iteration 100 / 171 | Total Loss: 4.167755603790283 | KNN Loss: 4.1652398109436035 | CLS Loss: 0.0025157134514302015\n",
      "Epoch 71 / 200 | iteration 110 / 171 | Total Loss: 4.158899784088135 | KNN Loss: 4.147846698760986 | CLS Loss: 0.011052983812987804\n",
      "Epoch 71 / 200 | iteration 120 / 171 | Total Loss: 4.1590576171875 | KNN Loss: 4.148191452026367 | CLS Loss: 0.010866396129131317\n",
      "Epoch 71 / 200 | iteration 130 / 171 | Total Loss: 4.133374214172363 | KNN Loss: 4.128645896911621 | CLS Loss: 0.004728429950773716\n",
      "Epoch 71 / 200 | iteration 140 / 171 | Total Loss: 4.154864311218262 | KNN Loss: 4.149360656738281 | CLS Loss: 0.005503687541931868\n",
      "Epoch 71 / 200 | iteration 150 / 171 | Total Loss: 4.155918598175049 | KNN Loss: 4.153371810913086 | CLS Loss: 0.0025465746875852346\n",
      "Epoch 71 / 200 | iteration 160 / 171 | Total Loss: 4.185876846313477 | KNN Loss: 4.178789138793945 | CLS Loss: 0.007087775971740484\n",
      "Epoch 71 / 200 | iteration 170 / 171 | Total Loss: 4.138546943664551 | KNN Loss: 4.131715774536133 | CLS Loss: 0.006831392180174589\n",
      "Epoch: 071, Loss: 4.1646, Train: 0.9965, Valid: 0.9865, Best: 0.9871\n",
      "Epoch 72 / 200 | iteration 0 / 171 | Total Loss: 4.182806968688965 | KNN Loss: 4.175469875335693 | CLS Loss: 0.00733704911544919\n",
      "Epoch 72 / 200 | iteration 10 / 171 | Total Loss: 4.1550397872924805 | KNN Loss: 4.129310607910156 | CLS Loss: 0.025729207322001457\n",
      "Epoch 72 / 200 | iteration 20 / 171 | Total Loss: 4.132940769195557 | KNN Loss: 4.127806663513184 | CLS Loss: 0.0051338886842131615\n",
      "Epoch 72 / 200 | iteration 30 / 171 | Total Loss: 4.143426418304443 | KNN Loss: 4.134213924407959 | CLS Loss: 0.009212350472807884\n",
      "Epoch 72 / 200 | iteration 40 / 171 | Total Loss: 4.136659145355225 | KNN Loss: 4.13124942779541 | CLS Loss: 0.005409723147749901\n",
      "Epoch 72 / 200 | iteration 50 / 171 | Total Loss: 4.15821647644043 | KNN Loss: 4.155879497528076 | CLS Loss: 0.0023367598187178373\n",
      "Epoch 72 / 200 | iteration 60 / 171 | Total Loss: 4.178038120269775 | KNN Loss: 4.166808605194092 | CLS Loss: 0.01122952252626419\n",
      "Epoch 72 / 200 | iteration 70 / 171 | Total Loss: 4.154879570007324 | KNN Loss: 4.1400146484375 | CLS Loss: 0.014864901080727577\n",
      "Epoch 72 / 200 | iteration 80 / 171 | Total Loss: 4.157630920410156 | KNN Loss: 4.1481404304504395 | CLS Loss: 0.00949071068316698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 / 200 | iteration 90 / 171 | Total Loss: 4.145737648010254 | KNN Loss: 4.141435623168945 | CLS Loss: 0.004302092362195253\n",
      "Epoch 72 / 200 | iteration 100 / 171 | Total Loss: 4.175812244415283 | KNN Loss: 4.157259464263916 | CLS Loss: 0.018552837893366814\n",
      "Epoch 72 / 200 | iteration 110 / 171 | Total Loss: 4.169170379638672 | KNN Loss: 4.148924350738525 | CLS Loss: 0.020245933905243874\n",
      "Epoch 72 / 200 | iteration 120 / 171 | Total Loss: 4.164619445800781 | KNN Loss: 4.154659748077393 | CLS Loss: 0.009959859773516655\n",
      "Epoch 72 / 200 | iteration 130 / 171 | Total Loss: 4.264278411865234 | KNN Loss: 4.203948497772217 | CLS Loss: 0.06032988801598549\n",
      "Epoch 72 / 200 | iteration 140 / 171 | Total Loss: 4.160919666290283 | KNN Loss: 4.14577579498291 | CLS Loss: 0.015143807046115398\n",
      "Epoch 72 / 200 | iteration 150 / 171 | Total Loss: 4.183973789215088 | KNN Loss: 4.153616428375244 | CLS Loss: 0.030357303097844124\n",
      "Epoch 72 / 200 | iteration 160 / 171 | Total Loss: 4.164931297302246 | KNN Loss: 4.14506721496582 | CLS Loss: 0.019863981753587723\n",
      "Epoch 72 / 200 | iteration 170 / 171 | Total Loss: 4.146109104156494 | KNN Loss: 4.130448341369629 | CLS Loss: 0.01566094160079956\n",
      "Epoch: 072, Loss: 4.1684, Train: 0.9961, Valid: 0.9866, Best: 0.9871\n",
      "Epoch 73 / 200 | iteration 0 / 171 | Total Loss: 4.1680803298950195 | KNN Loss: 4.144154071807861 | CLS Loss: 0.023926248773932457\n",
      "Epoch 73 / 200 | iteration 10 / 171 | Total Loss: 4.145938396453857 | KNN Loss: 4.133301258087158 | CLS Loss: 0.012637320905923843\n",
      "Epoch 73 / 200 | iteration 20 / 171 | Total Loss: 4.199041366577148 | KNN Loss: 4.161726474761963 | CLS Loss: 0.037314798682928085\n",
      "Epoch 73 / 200 | iteration 30 / 171 | Total Loss: 4.170245170593262 | KNN Loss: 4.1511759757995605 | CLS Loss: 0.019069284200668335\n",
      "Epoch 73 / 200 | iteration 40 / 171 | Total Loss: 4.147858619689941 | KNN Loss: 4.136123180389404 | CLS Loss: 0.011735399253666401\n",
      "Epoch 73 / 200 | iteration 50 / 171 | Total Loss: 4.1628828048706055 | KNN Loss: 4.1566009521484375 | CLS Loss: 0.006281688809394836\n",
      "Epoch 73 / 200 | iteration 60 / 171 | Total Loss: 4.147495269775391 | KNN Loss: 4.137707710266113 | CLS Loss: 0.009787756949663162\n",
      "Epoch 73 / 200 | iteration 70 / 171 | Total Loss: 4.135105609893799 | KNN Loss: 4.121216297149658 | CLS Loss: 0.013889270834624767\n",
      "Epoch 73 / 200 | iteration 80 / 171 | Total Loss: 4.165414333343506 | KNN Loss: 4.154463291168213 | CLS Loss: 0.010950851254165173\n",
      "Epoch 73 / 200 | iteration 90 / 171 | Total Loss: 4.166416168212891 | KNN Loss: 4.135647296905518 | CLS Loss: 0.030768759548664093\n",
      "Epoch 73 / 200 | iteration 100 / 171 | Total Loss: 4.186281681060791 | KNN Loss: 4.17876672744751 | CLS Loss: 0.0075151813216507435\n",
      "Epoch 73 / 200 | iteration 110 / 171 | Total Loss: 4.131246089935303 | KNN Loss: 4.119856834411621 | CLS Loss: 0.011389237828552723\n",
      "Epoch 73 / 200 | iteration 120 / 171 | Total Loss: 4.191174507141113 | KNN Loss: 4.150544166564941 | CLS Loss: 0.040630463510751724\n",
      "Epoch 73 / 200 | iteration 130 / 171 | Total Loss: 4.1375813484191895 | KNN Loss: 4.132177352905273 | CLS Loss: 0.005404073279350996\n",
      "Epoch 73 / 200 | iteration 140 / 171 | Total Loss: 4.149356365203857 | KNN Loss: 4.1374897956848145 | CLS Loss: 0.011866601184010506\n",
      "Epoch 73 / 200 | iteration 150 / 171 | Total Loss: 4.182949066162109 | KNN Loss: 4.15681791305542 | CLS Loss: 0.026131046935915947\n",
      "Epoch 73 / 200 | iteration 160 / 171 | Total Loss: 4.145371913909912 | KNN Loss: 4.133598327636719 | CLS Loss: 0.011773386970162392\n",
      "Epoch 73 / 200 | iteration 170 / 171 | Total Loss: 4.163200378417969 | KNN Loss: 4.156655311584473 | CLS Loss: 0.006545038428157568\n",
      "Epoch: 073, Loss: 4.1638, Train: 0.9959, Valid: 0.9866, Best: 0.9871\n",
      "Epoch 74 / 200 | iteration 0 / 171 | Total Loss: 4.132678031921387 | KNN Loss: 4.1116766929626465 | CLS Loss: 0.021001432090997696\n",
      "Epoch 74 / 200 | iteration 10 / 171 | Total Loss: 4.113563537597656 | KNN Loss: 4.100165843963623 | CLS Loss: 0.01339787058532238\n",
      "Epoch 74 / 200 | iteration 20 / 171 | Total Loss: 4.213095664978027 | KNN Loss: 4.195791721343994 | CLS Loss: 0.017303962260484695\n",
      "Epoch 74 / 200 | iteration 30 / 171 | Total Loss: 4.1537766456604 | KNN Loss: 4.11232852935791 | CLS Loss: 0.04144832864403725\n",
      "Epoch 74 / 200 | iteration 40 / 171 | Total Loss: 4.174717903137207 | KNN Loss: 4.165361404418945 | CLS Loss: 0.009356721304357052\n",
      "Epoch 74 / 200 | iteration 50 / 171 | Total Loss: 4.107941150665283 | KNN Loss: 4.106299877166748 | CLS Loss: 0.0016413374105468392\n",
      "Epoch 74 / 200 | iteration 60 / 171 | Total Loss: 4.20595645904541 | KNN Loss: 4.193673610687256 | CLS Loss: 0.012283043004572392\n",
      "Epoch 74 / 200 | iteration 70 / 171 | Total Loss: 4.167614459991455 | KNN Loss: 4.15482759475708 | CLS Loss: 0.012786796316504478\n",
      "Epoch 74 / 200 | iteration 80 / 171 | Total Loss: 4.158015727996826 | KNN Loss: 4.128942489624023 | CLS Loss: 0.029073119163513184\n",
      "Epoch 74 / 200 | iteration 90 / 171 | Total Loss: 4.141158580780029 | KNN Loss: 4.136087894439697 | CLS Loss: 0.005070850718766451\n",
      "Epoch 74 / 200 | iteration 100 / 171 | Total Loss: 4.135199546813965 | KNN Loss: 4.123865127563477 | CLS Loss: 0.011334436945617199\n",
      "Epoch 74 / 200 | iteration 110 / 171 | Total Loss: 4.183694362640381 | KNN Loss: 4.164748191833496 | CLS Loss: 0.01894640177488327\n",
      "Epoch 74 / 200 | iteration 120 / 171 | Total Loss: 4.148594856262207 | KNN Loss: 4.1438374519348145 | CLS Loss: 0.0047576227225363255\n",
      "Epoch 74 / 200 | iteration 130 / 171 | Total Loss: 4.138525485992432 | KNN Loss: 4.117956638336182 | CLS Loss: 0.02056884765625\n",
      "Epoch 74 / 200 | iteration 140 / 171 | Total Loss: 4.179823398590088 | KNN Loss: 4.162438869476318 | CLS Loss: 0.017384368926286697\n",
      "Epoch 74 / 200 | iteration 150 / 171 | Total Loss: 4.192402362823486 | KNN Loss: 4.1688313484191895 | CLS Loss: 0.02357085794210434\n",
      "Epoch 74 / 200 | iteration 160 / 171 | Total Loss: 4.222757339477539 | KNN Loss: 4.2065253257751465 | CLS Loss: 0.016231970861554146\n",
      "Epoch 74 / 200 | iteration 170 / 171 | Total Loss: 4.28870153427124 | KNN Loss: 4.2712554931640625 | CLS Loss: 0.01744614541530609\n",
      "Epoch: 074, Loss: 4.1689, Train: 0.9891, Valid: 0.9826, Best: 0.9871\n",
      "Epoch 75 / 200 | iteration 0 / 171 | Total Loss: 4.274074554443359 | KNN Loss: 4.224678993225098 | CLS Loss: 0.04939567297697067\n",
      "Epoch 75 / 200 | iteration 10 / 171 | Total Loss: 4.23612117767334 | KNN Loss: 4.19349479675293 | CLS Loss: 0.04262656718492508\n",
      "Epoch 75 / 200 | iteration 20 / 171 | Total Loss: 4.232237339019775 | KNN Loss: 4.206973552703857 | CLS Loss: 0.02526380680501461\n",
      "Epoch 75 / 200 | iteration 30 / 171 | Total Loss: 4.2292799949646 | KNN Loss: 4.190861701965332 | CLS Loss: 0.038418110460042953\n",
      "Epoch 75 / 200 | iteration 40 / 171 | Total Loss: 4.153731822967529 | KNN Loss: 4.145930767059326 | CLS Loss: 0.007800840772688389\n",
      "Epoch 75 / 200 | iteration 50 / 171 | Total Loss: 4.163209438323975 | KNN Loss: 4.139666557312012 | CLS Loss: 0.023543061688542366\n",
      "Epoch 75 / 200 | iteration 60 / 171 | Total Loss: 4.149819374084473 | KNN Loss: 4.1450958251953125 | CLS Loss: 0.004723767284303904\n",
      "Epoch 75 / 200 | iteration 70 / 171 | Total Loss: 4.136646747589111 | KNN Loss: 4.124703407287598 | CLS Loss: 0.011943522840738297\n",
      "Epoch 75 / 200 | iteration 80 / 171 | Total Loss: 4.212308406829834 | KNN Loss: 4.174160003662109 | CLS Loss: 0.038148410618305206\n",
      "Epoch 75 / 200 | iteration 90 / 171 | Total Loss: 4.211333751678467 | KNN Loss: 4.198403835296631 | CLS Loss: 0.012929881922900677\n",
      "Epoch 75 / 200 | iteration 100 / 171 | Total Loss: 4.157207012176514 | KNN Loss: 4.142923831939697 | CLS Loss: 0.014283251948654652\n",
      "Epoch 75 / 200 | iteration 110 / 171 | Total Loss: 4.17340612411499 | KNN Loss: 4.15413761138916 | CLS Loss: 0.01926855370402336\n",
      "Epoch 75 / 200 | iteration 120 / 171 | Total Loss: 4.178062915802002 | KNN Loss: 4.156439304351807 | CLS Loss: 0.021623644977808\n",
      "Epoch 75 / 200 | iteration 130 / 171 | Total Loss: 4.152721405029297 | KNN Loss: 4.13957405090332 | CLS Loss: 0.013147257268428802\n",
      "Epoch 75 / 200 | iteration 140 / 171 | Total Loss: 4.174935817718506 | KNN Loss: 4.166289806365967 | CLS Loss: 0.008646134287118912\n",
      "Epoch 75 / 200 | iteration 150 / 171 | Total Loss: 4.1813273429870605 | KNN Loss: 4.164896488189697 | CLS Loss: 0.01643100194633007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 200 | iteration 160 / 171 | Total Loss: 4.222021102905273 | KNN Loss: 4.214169979095459 | CLS Loss: 0.00785116944462061\n",
      "Epoch 75 / 200 | iteration 170 / 171 | Total Loss: 4.131816864013672 | KNN Loss: 4.116368770599365 | CLS Loss: 0.015447967685759068\n",
      "Epoch: 075, Loss: 4.1815, Train: 0.9948, Valid: 0.9859, Best: 0.9871\n",
      "Epoch 76 / 200 | iteration 0 / 171 | Total Loss: 4.248978614807129 | KNN Loss: 4.218338966369629 | CLS Loss: 0.030639667063951492\n",
      "Epoch 76 / 200 | iteration 10 / 171 | Total Loss: 4.192474365234375 | KNN Loss: 4.165014743804932 | CLS Loss: 0.027459723874926567\n",
      "Epoch 76 / 200 | iteration 20 / 171 | Total Loss: 4.174904823303223 | KNN Loss: 4.144264221191406 | CLS Loss: 0.030640719458460808\n",
      "Epoch 76 / 200 | iteration 30 / 171 | Total Loss: 4.182766437530518 | KNN Loss: 4.168643951416016 | CLS Loss: 0.014122456312179565\n",
      "Epoch 76 / 200 | iteration 40 / 171 | Total Loss: 4.168797016143799 | KNN Loss: 4.156519412994385 | CLS Loss: 0.012277767062187195\n",
      "Epoch 76 / 200 | iteration 50 / 171 | Total Loss: 4.174563884735107 | KNN Loss: 4.130599021911621 | CLS Loss: 0.04396463558077812\n",
      "Epoch 76 / 200 | iteration 60 / 171 | Total Loss: 4.158645153045654 | KNN Loss: 4.140626430511475 | CLS Loss: 0.01801876910030842\n",
      "Epoch 76 / 200 | iteration 70 / 171 | Total Loss: 4.148243427276611 | KNN Loss: 4.1352858543396 | CLS Loss: 0.012957559898495674\n",
      "Epoch 76 / 200 | iteration 80 / 171 | Total Loss: 4.214798450469971 | KNN Loss: 4.176541805267334 | CLS Loss: 0.03825657069683075\n",
      "Epoch 76 / 200 | iteration 90 / 171 | Total Loss: 4.145437717437744 | KNN Loss: 4.133951663970947 | CLS Loss: 0.01148606650531292\n",
      "Epoch 76 / 200 | iteration 100 / 171 | Total Loss: 4.200323581695557 | KNN Loss: 4.143090724945068 | CLS Loss: 0.057233039289712906\n",
      "Epoch 76 / 200 | iteration 110 / 171 | Total Loss: 4.221404075622559 | KNN Loss: 4.196061611175537 | CLS Loss: 0.02534257248044014\n",
      "Epoch 76 / 200 | iteration 120 / 171 | Total Loss: 4.175421714782715 | KNN Loss: 4.157592296600342 | CLS Loss: 0.01782962493598461\n",
      "Epoch 76 / 200 | iteration 130 / 171 | Total Loss: 4.174795627593994 | KNN Loss: 4.161165714263916 | CLS Loss: 0.013630150817334652\n",
      "Epoch 76 / 200 | iteration 140 / 171 | Total Loss: 4.1194353103637695 | KNN Loss: 4.1140851974487305 | CLS Loss: 0.005350091028958559\n",
      "Epoch 76 / 200 | iteration 150 / 171 | Total Loss: 4.195271015167236 | KNN Loss: 4.186947345733643 | CLS Loss: 0.008323466405272484\n",
      "Epoch 76 / 200 | iteration 160 / 171 | Total Loss: 4.212221622467041 | KNN Loss: 4.185266971588135 | CLS Loss: 0.02695469558238983\n",
      "Epoch 76 / 200 | iteration 170 / 171 | Total Loss: 4.174843788146973 | KNN Loss: 4.168134689331055 | CLS Loss: 0.006709271110594273\n",
      "Epoch: 076, Loss: 4.1711, Train: 0.9948, Valid: 0.9857, Best: 0.9871\n",
      "Epoch 77 / 200 | iteration 0 / 171 | Total Loss: 4.178108215332031 | KNN Loss: 4.163608074188232 | CLS Loss: 0.014499989338219166\n",
      "Epoch 77 / 200 | iteration 10 / 171 | Total Loss: 4.15411901473999 | KNN Loss: 4.136314868927002 | CLS Loss: 0.017804034054279327\n",
      "Epoch 77 / 200 | iteration 20 / 171 | Total Loss: 4.176199436187744 | KNN Loss: 4.155043125152588 | CLS Loss: 0.021156104281544685\n",
      "Epoch 77 / 200 | iteration 30 / 171 | Total Loss: 4.127064228057861 | KNN Loss: 4.107096195220947 | CLS Loss: 0.019968178123235703\n",
      "Epoch 77 / 200 | iteration 40 / 171 | Total Loss: 4.173135757446289 | KNN Loss: 4.147736072540283 | CLS Loss: 0.02539965696632862\n",
      "Epoch 77 / 200 | iteration 50 / 171 | Total Loss: 4.157686233520508 | KNN Loss: 4.146255016326904 | CLS Loss: 0.011430991813540459\n",
      "Epoch 77 / 200 | iteration 60 / 171 | Total Loss: 4.182170867919922 | KNN Loss: 4.161693096160889 | CLS Loss: 0.02047770470380783\n",
      "Epoch 77 / 200 | iteration 70 / 171 | Total Loss: 4.121481895446777 | KNN Loss: 4.119616985321045 | CLS Loss: 0.0018648066325113177\n",
      "Epoch 77 / 200 | iteration 80 / 171 | Total Loss: 4.161851406097412 | KNN Loss: 4.140853404998779 | CLS Loss: 0.020997805520892143\n",
      "Epoch 77 / 200 | iteration 90 / 171 | Total Loss: 4.163154125213623 | KNN Loss: 4.145328044891357 | CLS Loss: 0.01782597042620182\n",
      "Epoch 77 / 200 | iteration 100 / 171 | Total Loss: 4.199635982513428 | KNN Loss: 4.175968170166016 | CLS Loss: 0.02366788685321808\n",
      "Epoch 77 / 200 | iteration 110 / 171 | Total Loss: 4.118635177612305 | KNN Loss: 4.107344627380371 | CLS Loss: 0.011290725320577621\n",
      "Epoch 77 / 200 | iteration 120 / 171 | Total Loss: 4.1342267990112305 | KNN Loss: 4.116486549377441 | CLS Loss: 0.01774010993540287\n",
      "Epoch 77 / 200 | iteration 130 / 171 | Total Loss: 4.178027629852295 | KNN Loss: 4.1619486808776855 | CLS Loss: 0.016079014167189598\n",
      "Epoch 77 / 200 | iteration 140 / 171 | Total Loss: 4.1792426109313965 | KNN Loss: 4.159765720367432 | CLS Loss: 0.019476721063256264\n",
      "Epoch 77 / 200 | iteration 150 / 171 | Total Loss: 4.175997734069824 | KNN Loss: 4.156858444213867 | CLS Loss: 0.019139304757118225\n",
      "Epoch 77 / 200 | iteration 160 / 171 | Total Loss: 4.138591766357422 | KNN Loss: 4.130804061889648 | CLS Loss: 0.0077876378782093525\n",
      "Epoch 77 / 200 | iteration 170 / 171 | Total Loss: 4.162648677825928 | KNN Loss: 4.145768642425537 | CLS Loss: 0.016880182549357414\n",
      "Epoch: 077, Loss: 4.1606, Train: 0.9962, Valid: 0.9868, Best: 0.9871\n",
      "Epoch 78 / 200 | iteration 0 / 171 | Total Loss: 4.191802024841309 | KNN Loss: 4.175698280334473 | CLS Loss: 0.01610354706645012\n",
      "Epoch 78 / 200 | iteration 10 / 171 | Total Loss: 4.147976398468018 | KNN Loss: 4.13057279586792 | CLS Loss: 0.01740357093513012\n",
      "Epoch 78 / 200 | iteration 20 / 171 | Total Loss: 4.135653018951416 | KNN Loss: 4.118295669555664 | CLS Loss: 0.01735757291316986\n",
      "Epoch 78 / 200 | iteration 30 / 171 | Total Loss: 4.181236743927002 | KNN Loss: 4.162531852722168 | CLS Loss: 0.018705086782574654\n",
      "Epoch 78 / 200 | iteration 40 / 171 | Total Loss: 4.143525123596191 | KNN Loss: 4.140017509460449 | CLS Loss: 0.0035076839849352837\n",
      "Epoch 78 / 200 | iteration 50 / 171 | Total Loss: 4.139279365539551 | KNN Loss: 4.133737564086914 | CLS Loss: 0.005541879218071699\n",
      "Epoch 78 / 200 | iteration 60 / 171 | Total Loss: 4.145678997039795 | KNN Loss: 4.14117956161499 | CLS Loss: 0.004499657545238733\n",
      "Epoch 78 / 200 | iteration 70 / 171 | Total Loss: 4.130518913269043 | KNN Loss: 4.12898588180542 | CLS Loss: 0.00153286661952734\n",
      "Epoch 78 / 200 | iteration 80 / 171 | Total Loss: 4.135832786560059 | KNN Loss: 4.130974292755127 | CLS Loss: 0.004858675412833691\n",
      "Epoch 78 / 200 | iteration 90 / 171 | Total Loss: 4.179294586181641 | KNN Loss: 4.159453392028809 | CLS Loss: 0.019841236993670464\n",
      "Epoch 78 / 200 | iteration 100 / 171 | Total Loss: 4.142223358154297 | KNN Loss: 4.140284538269043 | CLS Loss: 0.001938692876137793\n",
      "Epoch 78 / 200 | iteration 110 / 171 | Total Loss: 4.165946006774902 | KNN Loss: 4.1493730545043945 | CLS Loss: 0.01657288894057274\n",
      "Epoch 78 / 200 | iteration 120 / 171 | Total Loss: 4.169283390045166 | KNN Loss: 4.150748252868652 | CLS Loss: 0.01853528805077076\n",
      "Epoch 78 / 200 | iteration 130 / 171 | Total Loss: 4.207231044769287 | KNN Loss: 4.200046539306641 | CLS Loss: 0.007184495683759451\n",
      "Epoch 78 / 200 | iteration 140 / 171 | Total Loss: 4.169455051422119 | KNN Loss: 4.150458335876465 | CLS Loss: 0.018996765837073326\n",
      "Epoch 78 / 200 | iteration 150 / 171 | Total Loss: 4.165594577789307 | KNN Loss: 4.1567888259887695 | CLS Loss: 0.008805928751826286\n",
      "Epoch 78 / 200 | iteration 160 / 171 | Total Loss: 4.175759315490723 | KNN Loss: 4.154971599578857 | CLS Loss: 0.020787682384252548\n",
      "Epoch 78 / 200 | iteration 170 / 171 | Total Loss: 4.177663803100586 | KNN Loss: 4.14915657043457 | CLS Loss: 0.02850707620382309\n",
      "Epoch: 078, Loss: 4.1611, Train: 0.9966, Valid: 0.9863, Best: 0.9871\n",
      "Epoch 79 / 200 | iteration 0 / 171 | Total Loss: 4.111815929412842 | KNN Loss: 4.107869625091553 | CLS Loss: 0.003946447279304266\n",
      "Epoch 79 / 200 | iteration 10 / 171 | Total Loss: 4.161752223968506 | KNN Loss: 4.153287410736084 | CLS Loss: 0.008465036749839783\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc = 0\n",
    "losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train(model, train_data_iter, optimizer, device)\n",
    "#     print(f\"Loss: {loss} =============================\")\n",
    "    losses.append(loss)\n",
    "    train_acc = test(model, train_data_iter, device)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_acc = test(model, test_data_iter, device)\n",
    "    val_accs.append(valid_acc)\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Train: {train_acc:.4f}, Valid: {valid_acc:.4f}, '\n",
    "              f'Best: {best_valid_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9884, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_data_iter, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a403ea36b552452dadbc983fe28bc5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses, label='train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a43fbcce1e491f88d77b697981e618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_accs, label='train accuracy')\n",
    "plt.plot(val_accs, label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9254a15533a04d888e06574d77bc1baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = torch.tensor([])\n",
    "projections = torch.tensor([])\n",
    "labels = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(test_data_iter):\n",
    "        test_samples = torch.cat([test_samples, x])\n",
    "        labels = torch.cat([labels, y])\n",
    "        x = x.to(device)\n",
    "        _, interm = model(x, True)\n",
    "        projections = torch.cat([projections, interm.detach().cpu().flatten(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1ee36a847341f0af6f7cae99d9a887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6255f8931a37434789baea063e476f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=2, min_samples=10).fit_predict(projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inliers: 0.8989539079987209\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of inliers: {sum(clusters != -1) / len(clusters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a731abfeb54cb7a55f183db3e60603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(test_samples.flatten(1)[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 400\n",
    "log_interval = 100\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=test_samples.shape[2], output_dim=len(set(clusters)) - 1, depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "Epoch: 00 | Batch: 000 / 039 | Total loss: 2.083 | Reg loss: 0.009 | Tree loss: 2.083 | Accuracy: 0.005859 | 0.245 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 01 | Batch: 000 / 039 | Total loss: 1.779 | Reg loss: 0.005 | Tree loss: 1.779 | Accuracy: 0.894531 | 0.219 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 02 | Batch: 000 / 039 | Total loss: 1.589 | Reg loss: 0.007 | Tree loss: 1.589 | Accuracy: 0.896484 | 0.22 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 03 | Batch: 000 / 039 | Total loss: 1.408 | Reg loss: 0.009 | Tree loss: 1.408 | Accuracy: 0.900391 | 0.221 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 04 | Batch: 000 / 039 | Total loss: 1.246 | Reg loss: 0.010 | Tree loss: 1.246 | Accuracy: 0.917969 | 0.221 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 05 | Batch: 000 / 039 | Total loss: 1.114 | Reg loss: 0.011 | Tree loss: 1.114 | Accuracy: 0.892578 | 0.22 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 06 | Batch: 000 / 039 | Total loss: 0.984 | Reg loss: 0.012 | Tree loss: 0.984 | Accuracy: 0.900391 | 0.22 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 07 | Batch: 000 / 039 | Total loss: 0.886 | Reg loss: 0.013 | Tree loss: 0.886 | Accuracy: 0.900391 | 0.22 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 08 | Batch: 000 / 039 | Total loss: 0.837 | Reg loss: 0.013 | Tree loss: 0.837 | Accuracy: 0.888672 | 0.219 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 09 | Batch: 000 / 039 | Total loss: 0.770 | Reg loss: 0.014 | Tree loss: 0.770 | Accuracy: 0.878906 | 0.219 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 10 | Batch: 000 / 039 | Total loss: 0.638 | Reg loss: 0.014 | Tree loss: 0.638 | Accuracy: 0.917969 | 0.219 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 11 | Batch: 000 / 039 | Total loss: 0.649 | Reg loss: 0.015 | Tree loss: 0.649 | Accuracy: 0.898438 | 0.218 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 12 | Batch: 000 / 039 | Total loss: 0.645 | Reg loss: 0.015 | Tree loss: 0.645 | Accuracy: 0.882812 | 0.218 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 13 | Batch: 000 / 039 | Total loss: 0.577 | Reg loss: 0.015 | Tree loss: 0.577 | Accuracy: 0.896484 | 0.218 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 14 | Batch: 000 / 039 | Total loss: 0.636 | Reg loss: 0.015 | Tree loss: 0.636 | Accuracy: 0.865234 | 0.218 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 15 | Batch: 000 / 039 | Total loss: 0.495 | Reg loss: 0.015 | Tree loss: 0.495 | Accuracy: 0.917969 | 0.218 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 16 | Batch: 000 / 039 | Total loss: 0.602 | Reg loss: 0.015 | Tree loss: 0.602 | Accuracy: 0.867188 | 0.218 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 17 | Batch: 000 / 039 | Total loss: 0.519 | Reg loss: 0.015 | Tree loss: 0.519 | Accuracy: 0.898438 | 0.218 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 18 | Batch: 000 / 039 | Total loss: 0.484 | Reg loss: 0.015 | Tree loss: 0.484 | Accuracy: 0.906250 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 19 | Batch: 000 / 039 | Total loss: 0.539 | Reg loss: 0.016 | Tree loss: 0.539 | Accuracy: 0.878906 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 20 | Batch: 000 / 039 | Total loss: 0.532 | Reg loss: 0.016 | Tree loss: 0.532 | Accuracy: 0.878906 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 21 | Batch: 000 / 039 | Total loss: 0.445 | Reg loss: 0.016 | Tree loss: 0.445 | Accuracy: 0.906250 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 22 | Batch: 000 / 039 | Total loss: 0.489 | Reg loss: 0.016 | Tree loss: 0.489 | Accuracy: 0.890625 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Batch: 000 / 039 | Total loss: 0.460 | Reg loss: 0.016 | Tree loss: 0.460 | Accuracy: 0.900391 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 24 | Batch: 000 / 039 | Total loss: 0.407 | Reg loss: 0.016 | Tree loss: 0.407 | Accuracy: 0.917969 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 25 | Batch: 000 / 039 | Total loss: 0.416 | Reg loss: 0.016 | Tree loss: 0.416 | Accuracy: 0.912109 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 26 | Batch: 000 / 039 | Total loss: 0.471 | Reg loss: 0.016 | Tree loss: 0.471 | Accuracy: 0.878906 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 27 | Batch: 000 / 039 | Total loss: 0.423 | Reg loss: 0.016 | Tree loss: 0.423 | Accuracy: 0.904297 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 28 | Batch: 000 / 039 | Total loss: 0.473 | Reg loss: 0.016 | Tree loss: 0.473 | Accuracy: 0.871094 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 29 | Batch: 000 / 039 | Total loss: 0.412 | Reg loss: 0.016 | Tree loss: 0.412 | Accuracy: 0.904297 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 30 | Batch: 000 / 039 | Total loss: 0.395 | Reg loss: 0.016 | Tree loss: 0.395 | Accuracy: 0.900391 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 31 | Batch: 000 / 039 | Total loss: 0.440 | Reg loss: 0.016 | Tree loss: 0.440 | Accuracy: 0.892578 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 32 | Batch: 000 / 039 | Total loss: 0.435 | Reg loss: 0.016 | Tree loss: 0.435 | Accuracy: 0.876953 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 33 | Batch: 000 / 039 | Total loss: 0.518 | Reg loss: 0.016 | Tree loss: 0.518 | Accuracy: 0.859375 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 34 | Batch: 000 / 039 | Total loss: 0.380 | Reg loss: 0.016 | Tree loss: 0.380 | Accuracy: 0.908203 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 35 | Batch: 000 / 039 | Total loss: 0.386 | Reg loss: 0.016 | Tree loss: 0.386 | Accuracy: 0.908203 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 36 | Batch: 000 / 039 | Total loss: 0.390 | Reg loss: 0.016 | Tree loss: 0.390 | Accuracy: 0.902344 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 37 | Batch: 000 / 039 | Total loss: 0.419 | Reg loss: 0.016 | Tree loss: 0.419 | Accuracy: 0.892578 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 38 | Batch: 000 / 039 | Total loss: 0.381 | Reg loss: 0.016 | Tree loss: 0.381 | Accuracy: 0.902344 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 39 | Batch: 000 / 039 | Total loss: 0.352 | Reg loss: 0.016 | Tree loss: 0.352 | Accuracy: 0.914062 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 40 | Batch: 000 / 039 | Total loss: 0.345 | Reg loss: 0.016 | Tree loss: 0.345 | Accuracy: 0.923828 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 41 | Batch: 000 / 039 | Total loss: 0.363 | Reg loss: 0.016 | Tree loss: 0.363 | Accuracy: 0.912109 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 42 | Batch: 000 / 039 | Total loss: 0.352 | Reg loss: 0.015 | Tree loss: 0.352 | Accuracy: 0.910156 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 43 | Batch: 000 / 039 | Total loss: 0.420 | Reg loss: 0.015 | Tree loss: 0.420 | Accuracy: 0.892578 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 44 | Batch: 000 / 039 | Total loss: 0.368 | Reg loss: 0.015 | Tree loss: 0.368 | Accuracy: 0.906250 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 45 | Batch: 000 / 039 | Total loss: 0.460 | Reg loss: 0.015 | Tree loss: 0.460 | Accuracy: 0.873047 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Batch: 000 / 039 | Total loss: 0.368 | Reg loss: 0.015 | Tree loss: 0.368 | Accuracy: 0.914062 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 47 | Batch: 000 / 039 | Total loss: 0.345 | Reg loss: 0.015 | Tree loss: 0.345 | Accuracy: 0.921875 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 48 | Batch: 000 / 039 | Total loss: 0.424 | Reg loss: 0.015 | Tree loss: 0.424 | Accuracy: 0.882812 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 49 | Batch: 000 / 039 | Total loss: 0.350 | Reg loss: 0.015 | Tree loss: 0.350 | Accuracy: 0.927734 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 50 | Batch: 000 / 039 | Total loss: 0.384 | Reg loss: 0.015 | Tree loss: 0.384 | Accuracy: 0.906250 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 51 | Batch: 000 / 039 | Total loss: 0.386 | Reg loss: 0.015 | Tree loss: 0.386 | Accuracy: 0.919922 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 52 | Batch: 000 / 039 | Total loss: 0.380 | Reg loss: 0.015 | Tree loss: 0.380 | Accuracy: 0.912109 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 53 | Batch: 000 / 039 | Total loss: 0.342 | Reg loss: 0.015 | Tree loss: 0.342 | Accuracy: 0.933594 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 54 | Batch: 000 / 039 | Total loss: 0.337 | Reg loss: 0.015 | Tree loss: 0.337 | Accuracy: 0.933594 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 55 | Batch: 000 / 039 | Total loss: 0.379 | Reg loss: 0.015 | Tree loss: 0.379 | Accuracy: 0.912109 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 56 | Batch: 000 / 039 | Total loss: 0.358 | Reg loss: 0.015 | Tree loss: 0.358 | Accuracy: 0.923828 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 57 | Batch: 000 / 039 | Total loss: 0.346 | Reg loss: 0.015 | Tree loss: 0.346 | Accuracy: 0.925781 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 58 | Batch: 000 / 039 | Total loss: 0.377 | Reg loss: 0.015 | Tree loss: 0.377 | Accuracy: 0.927734 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 59 | Batch: 000 / 039 | Total loss: 0.371 | Reg loss: 0.015 | Tree loss: 0.371 | Accuracy: 0.929688 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 60 | Batch: 000 / 039 | Total loss: 0.370 | Reg loss: 0.015 | Tree loss: 0.370 | Accuracy: 0.923828 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 61 | Batch: 000 / 039 | Total loss: 0.350 | Reg loss: 0.015 | Tree loss: 0.350 | Accuracy: 0.937500 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 62 | Batch: 000 / 039 | Total loss: 0.323 | Reg loss: 0.015 | Tree loss: 0.323 | Accuracy: 0.945312 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 63 | Batch: 000 / 039 | Total loss: 0.383 | Reg loss: 0.015 | Tree loss: 0.383 | Accuracy: 0.919922 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 64 | Batch: 000 / 039 | Total loss: 0.388 | Reg loss: 0.015 | Tree loss: 0.388 | Accuracy: 0.917969 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 65 | Batch: 000 / 039 | Total loss: 0.400 | Reg loss: 0.015 | Tree loss: 0.400 | Accuracy: 0.916016 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 66 | Batch: 000 / 039 | Total loss: 0.363 | Reg loss: 0.015 | Tree loss: 0.363 | Accuracy: 0.931641 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 67 | Batch: 000 / 039 | Total loss: 0.373 | Reg loss: 0.015 | Tree loss: 0.373 | Accuracy: 0.935547 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 68 | Batch: 000 / 039 | Total loss: 0.385 | Reg loss: 0.015 | Tree loss: 0.385 | Accuracy: 0.923828 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 | Batch: 000 / 039 | Total loss: 0.344 | Reg loss: 0.015 | Tree loss: 0.344 | Accuracy: 0.947266 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 70 | Batch: 000 / 039 | Total loss: 0.402 | Reg loss: 0.015 | Tree loss: 0.402 | Accuracy: 0.925781 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 71 | Batch: 000 / 039 | Total loss: 0.358 | Reg loss: 0.015 | Tree loss: 0.358 | Accuracy: 0.929688 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 72 | Batch: 000 / 039 | Total loss: 0.337 | Reg loss: 0.015 | Tree loss: 0.337 | Accuracy: 0.943359 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 73 | Batch: 000 / 039 | Total loss: 0.376 | Reg loss: 0.015 | Tree loss: 0.376 | Accuracy: 0.931641 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 74 | Batch: 000 / 039 | Total loss: 0.368 | Reg loss: 0.015 | Tree loss: 0.368 | Accuracy: 0.919922 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 75 | Batch: 000 / 039 | Total loss: 0.385 | Reg loss: 0.015 | Tree loss: 0.385 | Accuracy: 0.916016 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 76 | Batch: 000 / 039 | Total loss: 0.379 | Reg loss: 0.015 | Tree loss: 0.379 | Accuracy: 0.925781 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 77 | Batch: 000 / 039 | Total loss: 0.366 | Reg loss: 0.015 | Tree loss: 0.366 | Accuracy: 0.921875 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 78 | Batch: 000 / 039 | Total loss: 0.386 | Reg loss: 0.015 | Tree loss: 0.386 | Accuracy: 0.917969 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 79 | Batch: 000 / 039 | Total loss: 0.324 | Reg loss: 0.015 | Tree loss: 0.324 | Accuracy: 0.941406 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 80 | Batch: 000 / 039 | Total loss: 0.367 | Reg loss: 0.015 | Tree loss: 0.367 | Accuracy: 0.933594 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 81 | Batch: 000 / 039 | Total loss: 0.343 | Reg loss: 0.015 | Tree loss: 0.343 | Accuracy: 0.939453 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 82 | Batch: 000 / 039 | Total loss: 0.394 | Reg loss: 0.015 | Tree loss: 0.394 | Accuracy: 0.923828 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 83 | Batch: 000 / 039 | Total loss: 0.394 | Reg loss: 0.015 | Tree loss: 0.394 | Accuracy: 0.923828 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 84 | Batch: 000 / 039 | Total loss: 0.364 | Reg loss: 0.015 | Tree loss: 0.364 | Accuracy: 0.939453 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 85 | Batch: 000 / 039 | Total loss: 0.407 | Reg loss: 0.015 | Tree loss: 0.407 | Accuracy: 0.914062 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 86 | Batch: 000 / 039 | Total loss: 0.375 | Reg loss: 0.015 | Tree loss: 0.375 | Accuracy: 0.931641 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 87 | Batch: 000 / 039 | Total loss: 0.384 | Reg loss: 0.015 | Tree loss: 0.384 | Accuracy: 0.921875 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 88 | Batch: 000 / 039 | Total loss: 0.367 | Reg loss: 0.015 | Tree loss: 0.367 | Accuracy: 0.941406 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 89 | Batch: 000 / 039 | Total loss: 0.368 | Reg loss: 0.015 | Tree loss: 0.368 | Accuracy: 0.929688 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 90 | Batch: 000 / 039 | Total loss: 0.364 | Reg loss: 0.015 | Tree loss: 0.364 | Accuracy: 0.939453 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 91 | Batch: 000 / 039 | Total loss: 0.357 | Reg loss: 0.015 | Tree loss: 0.357 | Accuracy: 0.937500 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92 | Batch: 000 / 039 | Total loss: 0.340 | Reg loss: 0.015 | Tree loss: 0.340 | Accuracy: 0.937500 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 93 | Batch: 000 / 039 | Total loss: 0.342 | Reg loss: 0.015 | Tree loss: 0.342 | Accuracy: 0.945312 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 94 | Batch: 000 / 039 | Total loss: 0.345 | Reg loss: 0.015 | Tree loss: 0.345 | Accuracy: 0.935547 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 95 | Batch: 000 / 039 | Total loss: 0.381 | Reg loss: 0.015 | Tree loss: 0.381 | Accuracy: 0.927734 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 96 | Batch: 000 / 039 | Total loss: 0.372 | Reg loss: 0.015 | Tree loss: 0.372 | Accuracy: 0.933594 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 97 | Batch: 000 / 039 | Total loss: 0.404 | Reg loss: 0.015 | Tree loss: 0.404 | Accuracy: 0.914062 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 98 | Batch: 000 / 039 | Total loss: 0.395 | Reg loss: 0.015 | Tree loss: 0.395 | Accuracy: 0.923828 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 99 | Batch: 000 / 039 | Total loss: 0.326 | Reg loss: 0.015 | Tree loss: 0.326 | Accuracy: 0.949219 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 100 | Batch: 000 / 039 | Total loss: 0.369 | Reg loss: 0.015 | Tree loss: 0.369 | Accuracy: 0.931641 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 101 | Batch: 000 / 039 | Total loss: 0.410 | Reg loss: 0.015 | Tree loss: 0.410 | Accuracy: 0.917969 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 102 | Batch: 000 / 039 | Total loss: 0.345 | Reg loss: 0.015 | Tree loss: 0.345 | Accuracy: 0.939453 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 103 | Batch: 000 / 039 | Total loss: 0.328 | Reg loss: 0.015 | Tree loss: 0.328 | Accuracy: 0.953125 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 104 | Batch: 000 / 039 | Total loss: 0.373 | Reg loss: 0.015 | Tree loss: 0.373 | Accuracy: 0.927734 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 105 | Batch: 000 / 039 | Total loss: 0.348 | Reg loss: 0.015 | Tree loss: 0.348 | Accuracy: 0.933594 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 106 | Batch: 000 / 039 | Total loss: 0.382 | Reg loss: 0.015 | Tree loss: 0.382 | Accuracy: 0.927734 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 107 | Batch: 000 / 039 | Total loss: 0.324 | Reg loss: 0.015 | Tree loss: 0.324 | Accuracy: 0.941406 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 108 | Batch: 000 / 039 | Total loss: 0.380 | Reg loss: 0.015 | Tree loss: 0.380 | Accuracy: 0.933594 | 0.217 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 109 | Batch: 000 / 039 | Total loss: 0.348 | Reg loss: 0.015 | Tree loss: 0.348 | Accuracy: 0.943359 | 0.216 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 110 | Batch: 000 / 039 | Total loss: 0.303 | Reg loss: 0.015 | Tree loss: 0.303 | Accuracy: 0.957031 | 0.216 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 111 | Batch: 000 / 039 | Total loss: 0.312 | Reg loss: 0.015 | Tree loss: 0.312 | Accuracy: 0.955078 | 0.216 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 112 | Batch: 000 / 039 | Total loss: 0.336 | Reg loss: 0.015 | Tree loss: 0.336 | Accuracy: 0.947266 | 0.216 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 113 | Batch: 000 / 039 | Total loss: 0.344 | Reg loss: 0.015 | Tree loss: 0.344 | Accuracy: 0.949219 | 0.216 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 114 | Batch: 000 / 039 | Total loss: 0.345 | Reg loss: 0.015 | Tree loss: 0.345 | Accuracy: 0.939453 | 0.216 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 115 | Batch: 000 / 039 | Total loss: 0.341 | Reg loss: 0.015 | Tree loss: 0.341 | Accuracy: 0.939453 | 0.215 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 116 | Batch: 000 / 039 | Total loss: 0.347 | Reg loss: 0.015 | Tree loss: 0.347 | Accuracy: 0.931641 | 0.215 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 117 | Batch: 000 / 039 | Total loss: 0.430 | Reg loss: 0.015 | Tree loss: 0.430 | Accuracy: 0.919922 | 0.215 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 118 | Batch: 000 / 039 | Total loss: 0.311 | Reg loss: 0.015 | Tree loss: 0.311 | Accuracy: 0.947266 | 0.215 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 119 | Batch: 000 / 039 | Total loss: 0.364 | Reg loss: 0.015 | Tree loss: 0.364 | Accuracy: 0.937500 | 0.215 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 120 | Batch: 000 / 039 | Total loss: 0.352 | Reg loss: 0.015 | Tree loss: 0.352 | Accuracy: 0.927734 | 0.214 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 121 | Batch: 000 / 039 | Total loss: 0.372 | Reg loss: 0.015 | Tree loss: 0.372 | Accuracy: 0.937500 | 0.214 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 122 | Batch: 000 / 039 | Total loss: 0.373 | Reg loss: 0.015 | Tree loss: 0.373 | Accuracy: 0.921875 | 0.214 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 123 | Batch: 000 / 039 | Total loss: 0.324 | Reg loss: 0.015 | Tree loss: 0.324 | Accuracy: 0.943359 | 0.214 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 124 | Batch: 000 / 039 | Total loss: 0.351 | Reg loss: 0.015 | Tree loss: 0.351 | Accuracy: 0.941406 | 0.214 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 125 | Batch: 000 / 039 | Total loss: 0.389 | Reg loss: 0.015 | Tree loss: 0.389 | Accuracy: 0.919922 | 0.214 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 126 | Batch: 000 / 039 | Total loss: 0.403 | Reg loss: 0.015 | Tree loss: 0.403 | Accuracy: 0.910156 | 0.213 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 127 | Batch: 000 / 039 | Total loss: 0.367 | Reg loss: 0.015 | Tree loss: 0.367 | Accuracy: 0.931641 | 0.213 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 128 | Batch: 000 / 039 | Total loss: 0.415 | Reg loss: 0.015 | Tree loss: 0.415 | Accuracy: 0.914062 | 0.213 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 129 | Batch: 000 / 039 | Total loss: 0.376 | Reg loss: 0.015 | Tree loss: 0.376 | Accuracy: 0.937500 | 0.213 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 130 | Batch: 000 / 039 | Total loss: 0.337 | Reg loss: 0.015 | Tree loss: 0.337 | Accuracy: 0.951172 | 0.213 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 131 | Batch: 000 / 039 | Total loss: 0.371 | Reg loss: 0.015 | Tree loss: 0.371 | Accuracy: 0.929688 | 0.213 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 132 | Batch: 000 / 039 | Total loss: 0.340 | Reg loss: 0.015 | Tree loss: 0.340 | Accuracy: 0.951172 | 0.213 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 133 | Batch: 000 / 039 | Total loss: 0.381 | Reg loss: 0.015 | Tree loss: 0.381 | Accuracy: 0.933594 | 0.212 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 134 | Batch: 000 / 039 | Total loss: 0.356 | Reg loss: 0.015 | Tree loss: 0.356 | Accuracy: 0.933594 | 0.212 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 135 | Batch: 000 / 039 | Total loss: 0.362 | Reg loss: 0.015 | Tree loss: 0.362 | Accuracy: 0.935547 | 0.212 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 136 | Batch: 000 / 039 | Total loss: 0.397 | Reg loss: 0.015 | Tree loss: 0.397 | Accuracy: 0.914062 | 0.212 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 137 | Batch: 000 / 039 | Total loss: 0.374 | Reg loss: 0.015 | Tree loss: 0.374 | Accuracy: 0.927734 | 0.212 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 138 | Batch: 000 / 039 | Total loss: 0.382 | Reg loss: 0.015 | Tree loss: 0.382 | Accuracy: 0.921875 | 0.212 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 139 | Batch: 000 / 039 | Total loss: 0.372 | Reg loss: 0.015 | Tree loss: 0.372 | Accuracy: 0.923828 | 0.212 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 140 | Batch: 000 / 039 | Total loss: 0.361 | Reg loss: 0.015 | Tree loss: 0.361 | Accuracy: 0.941406 | 0.212 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 141 | Batch: 000 / 039 | Total loss: 0.373 | Reg loss: 0.015 | Tree loss: 0.373 | Accuracy: 0.935547 | 0.212 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 142 | Batch: 000 / 039 | Total loss: 0.405 | Reg loss: 0.015 | Tree loss: 0.405 | Accuracy: 0.917969 | 0.211 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 143 | Batch: 000 / 039 | Total loss: 0.405 | Reg loss: 0.015 | Tree loss: 0.405 | Accuracy: 0.927734 | 0.211 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 144 | Batch: 000 / 039 | Total loss: 0.378 | Reg loss: 0.015 | Tree loss: 0.378 | Accuracy: 0.927734 | 0.211 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 145 | Batch: 000 / 039 | Total loss: 0.337 | Reg loss: 0.015 | Tree loss: 0.337 | Accuracy: 0.951172 | 0.211 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 146 | Batch: 000 / 039 | Total loss: 0.351 | Reg loss: 0.015 | Tree loss: 0.351 | Accuracy: 0.933594 | 0.211 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 147 | Batch: 000 / 039 | Total loss: 0.352 | Reg loss: 0.015 | Tree loss: 0.352 | Accuracy: 0.943359 | 0.211 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 148 | Batch: 000 / 039 | Total loss: 0.365 | Reg loss: 0.015 | Tree loss: 0.365 | Accuracy: 0.939453 | 0.211 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 149 | Batch: 000 / 039 | Total loss: 0.356 | Reg loss: 0.015 | Tree loss: 0.356 | Accuracy: 0.929688 | 0.211 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 150 | Batch: 000 / 039 | Total loss: 0.346 | Reg loss: 0.015 | Tree loss: 0.346 | Accuracy: 0.945312 | 0.211 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 151 | Batch: 000 / 039 | Total loss: 0.373 | Reg loss: 0.015 | Tree loss: 0.373 | Accuracy: 0.933594 | 0.21 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 152 | Batch: 000 / 039 | Total loss: 0.350 | Reg loss: 0.015 | Tree loss: 0.350 | Accuracy: 0.931641 | 0.21 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 153 | Batch: 000 / 039 | Total loss: 0.370 | Reg loss: 0.015 | Tree loss: 0.370 | Accuracy: 0.929688 | 0.21 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 154 | Batch: 000 / 039 | Total loss: 0.401 | Reg loss: 0.015 | Tree loss: 0.401 | Accuracy: 0.917969 | 0.21 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 155 | Batch: 000 / 039 | Total loss: 0.357 | Reg loss: 0.015 | Tree loss: 0.357 | Accuracy: 0.931641 | 0.21 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 156 | Batch: 000 / 039 | Total loss: 0.360 | Reg loss: 0.015 | Tree loss: 0.360 | Accuracy: 0.937500 | 0.21 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 157 | Batch: 000 / 039 | Total loss: 0.402 | Reg loss: 0.015 | Tree loss: 0.402 | Accuracy: 0.921875 | 0.21 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 158 | Batch: 000 / 039 | Total loss: 0.368 | Reg loss: 0.015 | Tree loss: 0.368 | Accuracy: 0.923828 | 0.21 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 159 | Batch: 000 / 039 | Total loss: 0.364 | Reg loss: 0.015 | Tree loss: 0.364 | Accuracy: 0.933594 | 0.21 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 160 | Batch: 000 / 039 | Total loss: 0.334 | Reg loss: 0.015 | Tree loss: 0.334 | Accuracy: 0.947266 | 0.209 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 161 | Batch: 000 / 039 | Total loss: 0.348 | Reg loss: 0.015 | Tree loss: 0.348 | Accuracy: 0.945312 | 0.209 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 162 | Batch: 000 / 039 | Total loss: 0.344 | Reg loss: 0.015 | Tree loss: 0.344 | Accuracy: 0.933594 | 0.209 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 163 | Batch: 000 / 039 | Total loss: 0.338 | Reg loss: 0.015 | Tree loss: 0.338 | Accuracy: 0.945312 | 0.209 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 164 | Batch: 000 / 039 | Total loss: 0.410 | Reg loss: 0.015 | Tree loss: 0.410 | Accuracy: 0.919922 | 0.209 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 165 | Batch: 000 / 039 | Total loss: 0.344 | Reg loss: 0.015 | Tree loss: 0.344 | Accuracy: 0.947266 | 0.209 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 166 | Batch: 000 / 039 | Total loss: 0.405 | Reg loss: 0.015 | Tree loss: 0.405 | Accuracy: 0.921875 | 0.209 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 167 | Batch: 000 / 039 | Total loss: 0.347 | Reg loss: 0.015 | Tree loss: 0.347 | Accuracy: 0.939453 | 0.209 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 168 | Batch: 000 / 039 | Total loss: 0.319 | Reg loss: 0.015 | Tree loss: 0.319 | Accuracy: 0.945312 | 0.209 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 169 | Batch: 000 / 039 | Total loss: 0.381 | Reg loss: 0.015 | Tree loss: 0.381 | Accuracy: 0.927734 | 0.209 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 170 | Batch: 000 / 039 | Total loss: 0.329 | Reg loss: 0.015 | Tree loss: 0.329 | Accuracy: 0.937500 | 0.209 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 171 | Batch: 000 / 039 | Total loss: 0.356 | Reg loss: 0.015 | Tree loss: 0.356 | Accuracy: 0.937500 | 0.208 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 172 | Batch: 000 / 039 | Total loss: 0.350 | Reg loss: 0.015 | Tree loss: 0.350 | Accuracy: 0.933594 | 0.208 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 173 | Batch: 000 / 039 | Total loss: 0.340 | Reg loss: 0.015 | Tree loss: 0.340 | Accuracy: 0.935547 | 0.208 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 174 | Batch: 000 / 039 | Total loss: 0.379 | Reg loss: 0.015 | Tree loss: 0.379 | Accuracy: 0.927734 | 0.208 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 175 | Batch: 000 / 039 | Total loss: 0.343 | Reg loss: 0.015 | Tree loss: 0.343 | Accuracy: 0.939453 | 0.208 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 176 | Batch: 000 / 039 | Total loss: 0.354 | Reg loss: 0.015 | Tree loss: 0.354 | Accuracy: 0.925781 | 0.208 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 177 | Batch: 000 / 039 | Total loss: 0.326 | Reg loss: 0.015 | Tree loss: 0.326 | Accuracy: 0.951172 | 0.208 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 178 | Batch: 000 / 039 | Total loss: 0.323 | Reg loss: 0.015 | Tree loss: 0.323 | Accuracy: 0.937500 | 0.208 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 179 | Batch: 000 / 039 | Total loss: 0.393 | Reg loss: 0.015 | Tree loss: 0.393 | Accuracy: 0.919922 | 0.208 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 180 | Batch: 000 / 039 | Total loss: 0.366 | Reg loss: 0.015 | Tree loss: 0.366 | Accuracy: 0.921875 | 0.208 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 181 | Batch: 000 / 039 | Total loss: 0.328 | Reg loss: 0.015 | Tree loss: 0.328 | Accuracy: 0.945312 | 0.208 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 182 | Batch: 000 / 039 | Total loss: 0.292 | Reg loss: 0.015 | Tree loss: 0.292 | Accuracy: 0.957031 | 0.208 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 183 | Batch: 000 / 039 | Total loss: 0.329 | Reg loss: 0.015 | Tree loss: 0.329 | Accuracy: 0.945312 | 0.207 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 184 | Batch: 000 / 039 | Total loss: 0.355 | Reg loss: 0.015 | Tree loss: 0.355 | Accuracy: 0.939453 | 0.207 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 185 | Batch: 000 / 039 | Total loss: 0.371 | Reg loss: 0.015 | Tree loss: 0.371 | Accuracy: 0.914062 | 0.207 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 186 | Batch: 000 / 039 | Total loss: 0.345 | Reg loss: 0.015 | Tree loss: 0.345 | Accuracy: 0.935547 | 0.207 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 187 | Batch: 000 / 039 | Total loss: 0.348 | Reg loss: 0.015 | Tree loss: 0.348 | Accuracy: 0.933594 | 0.207 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 188 | Batch: 000 / 039 | Total loss: 0.310 | Reg loss: 0.015 | Tree loss: 0.310 | Accuracy: 0.949219 | 0.207 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 189 | Batch: 000 / 039 | Total loss: 0.339 | Reg loss: 0.015 | Tree loss: 0.339 | Accuracy: 0.933594 | 0.207 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 190 | Batch: 000 / 039 | Total loss: 0.349 | Reg loss: 0.015 | Tree loss: 0.349 | Accuracy: 0.925781 | 0.207 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 191 | Batch: 000 / 039 | Total loss: 0.334 | Reg loss: 0.015 | Tree loss: 0.334 | Accuracy: 0.935547 | 0.207 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 192 | Batch: 000 / 039 | Total loss: 0.346 | Reg loss: 0.015 | Tree loss: 0.346 | Accuracy: 0.941406 | 0.207 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 193 | Batch: 000 / 039 | Total loss: 0.344 | Reg loss: 0.015 | Tree loss: 0.344 | Accuracy: 0.927734 | 0.207 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 194 | Batch: 000 / 039 | Total loss: 0.390 | Reg loss: 0.015 | Tree loss: 0.390 | Accuracy: 0.910156 | 0.207 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 195 | Batch: 000 / 039 | Total loss: 0.425 | Reg loss: 0.015 | Tree loss: 0.425 | Accuracy: 0.910156 | 0.207 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 196 | Batch: 000 / 039 | Total loss: 0.354 | Reg loss: 0.015 | Tree loss: 0.354 | Accuracy: 0.919922 | 0.207 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 197 | Batch: 000 / 039 | Total loss: 0.369 | Reg loss: 0.015 | Tree loss: 0.369 | Accuracy: 0.912109 | 0.206 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 198 | Batch: 000 / 039 | Total loss: 0.382 | Reg loss: 0.015 | Tree loss: 0.382 | Accuracy: 0.916016 | 0.206 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 199 | Batch: 000 / 039 | Total loss: 0.315 | Reg loss: 0.015 | Tree loss: 0.315 | Accuracy: 0.943359 | 0.206 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 200 | Batch: 000 / 039 | Total loss: 0.352 | Reg loss: 0.014 | Tree loss: 0.352 | Accuracy: 0.929688 | 0.206 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 201 | Batch: 000 / 039 | Total loss: 0.350 | Reg loss: 0.015 | Tree loss: 0.350 | Accuracy: 0.927734 | 0.206 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 202 | Batch: 000 / 039 | Total loss: 0.366 | Reg loss: 0.015 | Tree loss: 0.366 | Accuracy: 0.921875 | 0.206 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 203 | Batch: 000 / 039 | Total loss: 0.373 | Reg loss: 0.014 | Tree loss: 0.373 | Accuracy: 0.927734 | 0.206 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 204 | Batch: 000 / 039 | Total loss: 0.324 | Reg loss: 0.014 | Tree loss: 0.324 | Accuracy: 0.935547 | 0.206 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 205 | Batch: 000 / 039 | Total loss: 0.316 | Reg loss: 0.015 | Tree loss: 0.316 | Accuracy: 0.949219 | 0.206 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 206 | Batch: 000 / 039 | Total loss: 0.379 | Reg loss: 0.015 | Tree loss: 0.379 | Accuracy: 0.917969 | 0.206 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 207 | Batch: 000 / 039 | Total loss: 0.326 | Reg loss: 0.015 | Tree loss: 0.326 | Accuracy: 0.943359 | 0.206 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 208 | Batch: 000 / 039 | Total loss: 0.376 | Reg loss: 0.015 | Tree loss: 0.376 | Accuracy: 0.921875 | 0.206 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 209 | Batch: 000 / 039 | Total loss: 0.386 | Reg loss: 0.014 | Tree loss: 0.386 | Accuracy: 0.917969 | 0.206 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 210 | Batch: 000 / 039 | Total loss: 0.371 | Reg loss: 0.014 | Tree loss: 0.371 | Accuracy: 0.914062 | 0.206 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 211 | Batch: 000 / 039 | Total loss: 0.374 | Reg loss: 0.015 | Tree loss: 0.374 | Accuracy: 0.923828 | 0.206 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 212 | Batch: 000 / 039 | Total loss: 0.389 | Reg loss: 0.014 | Tree loss: 0.389 | Accuracy: 0.923828 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 213 | Batch: 000 / 039 | Total loss: 0.321 | Reg loss: 0.014 | Tree loss: 0.321 | Accuracy: 0.931641 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 214 | Batch: 000 / 039 | Total loss: 0.344 | Reg loss: 0.014 | Tree loss: 0.344 | Accuracy: 0.937500 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 215 | Batch: 000 / 039 | Total loss: 0.392 | Reg loss: 0.014 | Tree loss: 0.392 | Accuracy: 0.914062 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 216 | Batch: 000 / 039 | Total loss: 0.324 | Reg loss: 0.014 | Tree loss: 0.324 | Accuracy: 0.941406 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 217 | Batch: 000 / 039 | Total loss: 0.357 | Reg loss: 0.014 | Tree loss: 0.357 | Accuracy: 0.929688 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 218 | Batch: 000 / 039 | Total loss: 0.353 | Reg loss: 0.014 | Tree loss: 0.353 | Accuracy: 0.937500 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 219 | Batch: 000 / 039 | Total loss: 0.320 | Reg loss: 0.014 | Tree loss: 0.320 | Accuracy: 0.941406 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 220 | Batch: 000 / 039 | Total loss: 0.333 | Reg loss: 0.014 | Tree loss: 0.333 | Accuracy: 0.939453 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 221 | Batch: 000 / 039 | Total loss: 0.318 | Reg loss: 0.014 | Tree loss: 0.318 | Accuracy: 0.931641 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 222 | Batch: 000 / 039 | Total loss: 0.322 | Reg loss: 0.014 | Tree loss: 0.322 | Accuracy: 0.939453 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 223 | Batch: 000 / 039 | Total loss: 0.349 | Reg loss: 0.014 | Tree loss: 0.349 | Accuracy: 0.923828 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 224 | Batch: 000 / 039 | Total loss: 0.349 | Reg loss: 0.014 | Tree loss: 0.349 | Accuracy: 0.916016 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 225 | Batch: 000 / 039 | Total loss: 0.401 | Reg loss: 0.014 | Tree loss: 0.401 | Accuracy: 0.898438 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 226 | Batch: 000 / 039 | Total loss: 0.332 | Reg loss: 0.014 | Tree loss: 0.332 | Accuracy: 0.941406 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 227 | Batch: 000 / 039 | Total loss: 0.360 | Reg loss: 0.014 | Tree loss: 0.360 | Accuracy: 0.931641 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 228 | Batch: 000 / 039 | Total loss: 0.367 | Reg loss: 0.014 | Tree loss: 0.367 | Accuracy: 0.923828 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 229 | Batch: 000 / 039 | Total loss: 0.357 | Reg loss: 0.014 | Tree loss: 0.357 | Accuracy: 0.916016 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 230 | Batch: 000 / 039 | Total loss: 0.333 | Reg loss: 0.014 | Tree loss: 0.333 | Accuracy: 0.925781 | 0.205 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 231 | Batch: 000 / 039 | Total loss: 0.340 | Reg loss: 0.014 | Tree loss: 0.340 | Accuracy: 0.929688 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 232 | Batch: 000 / 039 | Total loss: 0.391 | Reg loss: 0.014 | Tree loss: 0.391 | Accuracy: 0.914062 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 233 | Batch: 000 / 039 | Total loss: 0.333 | Reg loss: 0.014 | Tree loss: 0.333 | Accuracy: 0.933594 | 0.205 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 234 | Batch: 000 / 039 | Total loss: 0.360 | Reg loss: 0.014 | Tree loss: 0.360 | Accuracy: 0.929688 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 235 | Batch: 000 / 039 | Total loss: 0.351 | Reg loss: 0.014 | Tree loss: 0.351 | Accuracy: 0.929688 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 236 | Batch: 000 / 039 | Total loss: 0.304 | Reg loss: 0.014 | Tree loss: 0.304 | Accuracy: 0.941406 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 237 | Batch: 000 / 039 | Total loss: 0.332 | Reg loss: 0.014 | Tree loss: 0.332 | Accuracy: 0.935547 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 238 | Batch: 000 / 039 | Total loss: 0.343 | Reg loss: 0.014 | Tree loss: 0.343 | Accuracy: 0.929688 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 239 | Batch: 000 / 039 | Total loss: 0.343 | Reg loss: 0.014 | Tree loss: 0.343 | Accuracy: 0.925781 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 240 | Batch: 000 / 039 | Total loss: 0.340 | Reg loss: 0.014 | Tree loss: 0.340 | Accuracy: 0.929688 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 241 | Batch: 000 / 039 | Total loss: 0.295 | Reg loss: 0.014 | Tree loss: 0.295 | Accuracy: 0.937500 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 242 | Batch: 000 / 039 | Total loss: 0.373 | Reg loss: 0.014 | Tree loss: 0.373 | Accuracy: 0.919922 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 243 | Batch: 000 / 039 | Total loss: 0.348 | Reg loss: 0.014 | Tree loss: 0.348 | Accuracy: 0.927734 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 244 | Batch: 000 / 039 | Total loss: 0.326 | Reg loss: 0.014 | Tree loss: 0.326 | Accuracy: 0.927734 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 245 | Batch: 000 / 039 | Total loss: 0.429 | Reg loss: 0.014 | Tree loss: 0.429 | Accuracy: 0.888672 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 246 | Batch: 000 / 039 | Total loss: 0.370 | Reg loss: 0.014 | Tree loss: 0.370 | Accuracy: 0.919922 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 247 | Batch: 000 / 039 | Total loss: 0.371 | Reg loss: 0.014 | Tree loss: 0.371 | Accuracy: 0.923828 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 248 | Batch: 000 / 039 | Total loss: 0.308 | Reg loss: 0.014 | Tree loss: 0.308 | Accuracy: 0.943359 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 249 | Batch: 000 / 039 | Total loss: 0.350 | Reg loss: 0.014 | Tree loss: 0.350 | Accuracy: 0.927734 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 250 | Batch: 000 / 039 | Total loss: 0.315 | Reg loss: 0.014 | Tree loss: 0.315 | Accuracy: 0.939453 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 251 | Batch: 000 / 039 | Total loss: 0.352 | Reg loss: 0.014 | Tree loss: 0.352 | Accuracy: 0.925781 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 252 | Batch: 000 / 039 | Total loss: 0.410 | Reg loss: 0.014 | Tree loss: 0.410 | Accuracy: 0.898438 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 253 | Batch: 000 / 039 | Total loss: 0.340 | Reg loss: 0.014 | Tree loss: 0.340 | Accuracy: 0.931641 | 0.204 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 254 | Batch: 000 / 039 | Total loss: 0.419 | Reg loss: 0.014 | Tree loss: 0.419 | Accuracy: 0.894531 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 255 | Batch: 000 / 039 | Total loss: 0.339 | Reg loss: 0.014 | Tree loss: 0.339 | Accuracy: 0.935547 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 256 | Batch: 000 / 039 | Total loss: 0.290 | Reg loss: 0.014 | Tree loss: 0.290 | Accuracy: 0.951172 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 257 | Batch: 000 / 039 | Total loss: 0.399 | Reg loss: 0.014 | Tree loss: 0.399 | Accuracy: 0.916016 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 258 | Batch: 000 / 039 | Total loss: 0.317 | Reg loss: 0.014 | Tree loss: 0.317 | Accuracy: 0.939453 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 259 | Batch: 000 / 039 | Total loss: 0.361 | Reg loss: 0.014 | Tree loss: 0.361 | Accuracy: 0.931641 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 260 | Batch: 000 / 039 | Total loss: 0.367 | Reg loss: 0.014 | Tree loss: 0.367 | Accuracy: 0.919922 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 261 | Batch: 000 / 039 | Total loss: 0.359 | Reg loss: 0.014 | Tree loss: 0.359 | Accuracy: 0.925781 | 0.204 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 262 | Batch: 000 / 039 | Total loss: 0.330 | Reg loss: 0.014 | Tree loss: 0.330 | Accuracy: 0.935547 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 263 | Batch: 000 / 039 | Total loss: 0.331 | Reg loss: 0.014 | Tree loss: 0.331 | Accuracy: 0.939453 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 264 | Batch: 000 / 039 | Total loss: 0.386 | Reg loss: 0.014 | Tree loss: 0.386 | Accuracy: 0.914062 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 265 | Batch: 000 / 039 | Total loss: 0.326 | Reg loss: 0.014 | Tree loss: 0.326 | Accuracy: 0.939453 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 266 | Batch: 000 / 039 | Total loss: 0.332 | Reg loss: 0.014 | Tree loss: 0.332 | Accuracy: 0.939453 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 267 | Batch: 000 / 039 | Total loss: 0.374 | Reg loss: 0.014 | Tree loss: 0.374 | Accuracy: 0.925781 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 268 | Batch: 000 / 039 | Total loss: 0.331 | Reg loss: 0.014 | Tree loss: 0.331 | Accuracy: 0.945312 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 269 | Batch: 000 / 039 | Total loss: 0.278 | Reg loss: 0.014 | Tree loss: 0.278 | Accuracy: 0.951172 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 270 | Batch: 000 / 039 | Total loss: 0.368 | Reg loss: 0.014 | Tree loss: 0.368 | Accuracy: 0.923828 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 271 | Batch: 000 / 039 | Total loss: 0.374 | Reg loss: 0.014 | Tree loss: 0.374 | Accuracy: 0.917969 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 272 | Batch: 000 / 039 | Total loss: 0.331 | Reg loss: 0.014 | Tree loss: 0.331 | Accuracy: 0.933594 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 273 | Batch: 000 / 039 | Total loss: 0.358 | Reg loss: 0.014 | Tree loss: 0.358 | Accuracy: 0.923828 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 274 | Batch: 000 / 039 | Total loss: 0.294 | Reg loss: 0.014 | Tree loss: 0.294 | Accuracy: 0.943359 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 275 | Batch: 000 / 039 | Total loss: 0.373 | Reg loss: 0.014 | Tree loss: 0.373 | Accuracy: 0.916016 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 276 | Batch: 000 / 039 | Total loss: 0.325 | Reg loss: 0.014 | Tree loss: 0.325 | Accuracy: 0.937500 | 0.203 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 277 | Batch: 000 / 039 | Total loss: 0.328 | Reg loss: 0.014 | Tree loss: 0.328 | Accuracy: 0.939453 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 278 | Batch: 000 / 039 | Total loss: 0.331 | Reg loss: 0.014 | Tree loss: 0.331 | Accuracy: 0.945312 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 279 | Batch: 000 / 039 | Total loss: 0.379 | Reg loss: 0.014 | Tree loss: 0.379 | Accuracy: 0.919922 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 280 | Batch: 000 / 039 | Total loss: 0.354 | Reg loss: 0.014 | Tree loss: 0.354 | Accuracy: 0.925781 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 281 | Batch: 000 / 039 | Total loss: 0.381 | Reg loss: 0.014 | Tree loss: 0.381 | Accuracy: 0.919922 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 282 | Batch: 000 / 039 | Total loss: 0.382 | Reg loss: 0.014 | Tree loss: 0.382 | Accuracy: 0.910156 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 283 | Batch: 000 / 039 | Total loss: 0.329 | Reg loss: 0.014 | Tree loss: 0.329 | Accuracy: 0.933594 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 284 | Batch: 000 / 039 | Total loss: 0.333 | Reg loss: 0.014 | Tree loss: 0.333 | Accuracy: 0.941406 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 285 | Batch: 000 / 039 | Total loss: 0.387 | Reg loss: 0.014 | Tree loss: 0.387 | Accuracy: 0.908203 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 286 | Batch: 000 / 039 | Total loss: 0.329 | Reg loss: 0.014 | Tree loss: 0.329 | Accuracy: 0.937500 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 287 | Batch: 000 / 039 | Total loss: 0.355 | Reg loss: 0.014 | Tree loss: 0.355 | Accuracy: 0.919922 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 288 | Batch: 000 / 039 | Total loss: 0.344 | Reg loss: 0.014 | Tree loss: 0.344 | Accuracy: 0.931641 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 289 | Batch: 000 / 039 | Total loss: 0.344 | Reg loss: 0.014 | Tree loss: 0.344 | Accuracy: 0.929688 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 290 | Batch: 000 / 039 | Total loss: 0.348 | Reg loss: 0.014 | Tree loss: 0.348 | Accuracy: 0.919922 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 291 | Batch: 000 / 039 | Total loss: 0.383 | Reg loss: 0.014 | Tree loss: 0.383 | Accuracy: 0.914062 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 292 | Batch: 000 / 039 | Total loss: 0.313 | Reg loss: 0.014 | Tree loss: 0.313 | Accuracy: 0.947266 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 293 | Batch: 000 / 039 | Total loss: 0.328 | Reg loss: 0.014 | Tree loss: 0.328 | Accuracy: 0.935547 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 294 | Batch: 000 / 039 | Total loss: 0.338 | Reg loss: 0.014 | Tree loss: 0.338 | Accuracy: 0.925781 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 295 | Batch: 000 / 039 | Total loss: 0.341 | Reg loss: 0.014 | Tree loss: 0.341 | Accuracy: 0.927734 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 296 | Batch: 000 / 039 | Total loss: 0.370 | Reg loss: 0.014 | Tree loss: 0.370 | Accuracy: 0.925781 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 297 | Batch: 000 / 039 | Total loss: 0.300 | Reg loss: 0.014 | Tree loss: 0.300 | Accuracy: 0.947266 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 298 | Batch: 000 / 039 | Total loss: 0.377 | Reg loss: 0.014 | Tree loss: 0.377 | Accuracy: 0.919922 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 299 | Batch: 000 / 039 | Total loss: 0.372 | Reg loss: 0.014 | Tree loss: 0.372 | Accuracy: 0.912109 | 0.203 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 300 | Batch: 000 / 039 | Total loss: 0.358 | Reg loss: 0.014 | Tree loss: 0.358 | Accuracy: 0.925781 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 301 | Batch: 000 / 039 | Total loss: 0.378 | Reg loss: 0.014 | Tree loss: 0.378 | Accuracy: 0.914062 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 302 | Batch: 000 / 039 | Total loss: 0.328 | Reg loss: 0.014 | Tree loss: 0.328 | Accuracy: 0.941406 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 303 | Batch: 000 / 039 | Total loss: 0.324 | Reg loss: 0.014 | Tree loss: 0.324 | Accuracy: 0.925781 | 0.203 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 304 | Batch: 000 / 039 | Total loss: 0.347 | Reg loss: 0.014 | Tree loss: 0.347 | Accuracy: 0.921875 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 305 | Batch: 000 / 039 | Total loss: 0.347 | Reg loss: 0.014 | Tree loss: 0.347 | Accuracy: 0.916016 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 306 | Batch: 000 / 039 | Total loss: 0.313 | Reg loss: 0.014 | Tree loss: 0.313 | Accuracy: 0.945312 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 307 | Batch: 000 / 039 | Total loss: 0.403 | Reg loss: 0.014 | Tree loss: 0.403 | Accuracy: 0.916016 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 308 | Batch: 000 / 039 | Total loss: 0.363 | Reg loss: 0.014 | Tree loss: 0.363 | Accuracy: 0.923828 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 309 | Batch: 000 / 039 | Total loss: 0.314 | Reg loss: 0.014 | Tree loss: 0.314 | Accuracy: 0.937500 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 310 | Batch: 000 / 039 | Total loss: 0.300 | Reg loss: 0.014 | Tree loss: 0.300 | Accuracy: 0.949219 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 311 | Batch: 000 / 039 | Total loss: 0.386 | Reg loss: 0.014 | Tree loss: 0.386 | Accuracy: 0.902344 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 312 | Batch: 000 / 039 | Total loss: 0.354 | Reg loss: 0.014 | Tree loss: 0.354 | Accuracy: 0.927734 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 313 | Batch: 000 / 039 | Total loss: 0.342 | Reg loss: 0.014 | Tree loss: 0.342 | Accuracy: 0.925781 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 314 | Batch: 000 / 039 | Total loss: 0.384 | Reg loss: 0.014 | Tree loss: 0.384 | Accuracy: 0.914062 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 315 | Batch: 000 / 039 | Total loss: 0.395 | Reg loss: 0.014 | Tree loss: 0.395 | Accuracy: 0.906250 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 316 | Batch: 000 / 039 | Total loss: 0.358 | Reg loss: 0.014 | Tree loss: 0.358 | Accuracy: 0.912109 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 317 | Batch: 000 / 039 | Total loss: 0.348 | Reg loss: 0.014 | Tree loss: 0.348 | Accuracy: 0.923828 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 318 | Batch: 000 / 039 | Total loss: 0.353 | Reg loss: 0.014 | Tree loss: 0.353 | Accuracy: 0.919922 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 319 | Batch: 000 / 039 | Total loss: 0.314 | Reg loss: 0.014 | Tree loss: 0.314 | Accuracy: 0.943359 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 320 | Batch: 000 / 039 | Total loss: 0.332 | Reg loss: 0.014 | Tree loss: 0.332 | Accuracy: 0.931641 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 321 | Batch: 000 / 039 | Total loss: 0.353 | Reg loss: 0.014 | Tree loss: 0.353 | Accuracy: 0.929688 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 322 | Batch: 000 / 039 | Total loss: 0.373 | Reg loss: 0.014 | Tree loss: 0.373 | Accuracy: 0.917969 | 0.202 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 323 | Batch: 000 / 039 | Total loss: 0.286 | Reg loss: 0.014 | Tree loss: 0.286 | Accuracy: 0.947266 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 324 | Batch: 000 / 039 | Total loss: 0.345 | Reg loss: 0.014 | Tree loss: 0.345 | Accuracy: 0.933594 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 325 | Batch: 000 / 039 | Total loss: 0.314 | Reg loss: 0.014 | Tree loss: 0.314 | Accuracy: 0.937500 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 326 | Batch: 000 / 039 | Total loss: 0.309 | Reg loss: 0.014 | Tree loss: 0.309 | Accuracy: 0.933594 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 327 | Batch: 000 / 039 | Total loss: 0.345 | Reg loss: 0.014 | Tree loss: 0.345 | Accuracy: 0.921875 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 328 | Batch: 000 / 039 | Total loss: 0.332 | Reg loss: 0.014 | Tree loss: 0.332 | Accuracy: 0.923828 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 329 | Batch: 000 / 039 | Total loss: 0.322 | Reg loss: 0.014 | Tree loss: 0.322 | Accuracy: 0.939453 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 330 | Batch: 000 / 039 | Total loss: 0.374 | Reg loss: 0.014 | Tree loss: 0.374 | Accuracy: 0.908203 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 331 | Batch: 000 / 039 | Total loss: 0.313 | Reg loss: 0.014 | Tree loss: 0.313 | Accuracy: 0.945312 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 332 | Batch: 000 / 039 | Total loss: 0.402 | Reg loss: 0.014 | Tree loss: 0.402 | Accuracy: 0.908203 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 333 | Batch: 000 / 039 | Total loss: 0.372 | Reg loss: 0.014 | Tree loss: 0.372 | Accuracy: 0.919922 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 334 | Batch: 000 / 039 | Total loss: 0.388 | Reg loss: 0.014 | Tree loss: 0.388 | Accuracy: 0.916016 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 335 | Batch: 000 / 039 | Total loss: 0.372 | Reg loss: 0.014 | Tree loss: 0.372 | Accuracy: 0.927734 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 336 | Batch: 000 / 039 | Total loss: 0.393 | Reg loss: 0.014 | Tree loss: 0.393 | Accuracy: 0.914062 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 337 | Batch: 000 / 039 | Total loss: 0.375 | Reg loss: 0.014 | Tree loss: 0.375 | Accuracy: 0.910156 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 338 | Batch: 000 / 039 | Total loss: 0.364 | Reg loss: 0.014 | Tree loss: 0.364 | Accuracy: 0.933594 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 339 | Batch: 000 / 039 | Total loss: 0.367 | Reg loss: 0.014 | Tree loss: 0.367 | Accuracy: 0.914062 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 340 | Batch: 000 / 039 | Total loss: 0.313 | Reg loss: 0.014 | Tree loss: 0.313 | Accuracy: 0.937500 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 341 | Batch: 000 / 039 | Total loss: 0.379 | Reg loss: 0.014 | Tree loss: 0.379 | Accuracy: 0.917969 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 342 | Batch: 000 / 039 | Total loss: 0.375 | Reg loss: 0.014 | Tree loss: 0.375 | Accuracy: 0.906250 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 343 | Batch: 000 / 039 | Total loss: 0.341 | Reg loss: 0.014 | Tree loss: 0.341 | Accuracy: 0.929688 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 344 | Batch: 000 / 039 | Total loss: 0.372 | Reg loss: 0.014 | Tree loss: 0.372 | Accuracy: 0.914062 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 345 | Batch: 000 / 039 | Total loss: 0.339 | Reg loss: 0.014 | Tree loss: 0.339 | Accuracy: 0.937500 | 0.202 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 346 | Batch: 000 / 039 | Total loss: 0.383 | Reg loss: 0.014 | Tree loss: 0.383 | Accuracy: 0.910156 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 347 | Batch: 000 / 039 | Total loss: 0.398 | Reg loss: 0.014 | Tree loss: 0.398 | Accuracy: 0.906250 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 348 | Batch: 000 / 039 | Total loss: 0.332 | Reg loss: 0.014 | Tree loss: 0.332 | Accuracy: 0.933594 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 349 | Batch: 000 / 039 | Total loss: 0.342 | Reg loss: 0.014 | Tree loss: 0.342 | Accuracy: 0.937500 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 350 | Batch: 000 / 039 | Total loss: 0.345 | Reg loss: 0.014 | Tree loss: 0.345 | Accuracy: 0.937500 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 351 | Batch: 000 / 039 | Total loss: 0.323 | Reg loss: 0.014 | Tree loss: 0.323 | Accuracy: 0.927734 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 352 | Batch: 000 / 039 | Total loss: 0.307 | Reg loss: 0.014 | Tree loss: 0.307 | Accuracy: 0.951172 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 353 | Batch: 000 / 039 | Total loss: 0.360 | Reg loss: 0.014 | Tree loss: 0.360 | Accuracy: 0.925781 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 354 | Batch: 000 / 039 | Total loss: 0.328 | Reg loss: 0.014 | Tree loss: 0.328 | Accuracy: 0.939453 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 355 | Batch: 000 / 039 | Total loss: 0.369 | Reg loss: 0.014 | Tree loss: 0.369 | Accuracy: 0.908203 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 356 | Batch: 000 / 039 | Total loss: 0.362 | Reg loss: 0.014 | Tree loss: 0.362 | Accuracy: 0.917969 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 357 | Batch: 000 / 039 | Total loss: 0.329 | Reg loss: 0.014 | Tree loss: 0.329 | Accuracy: 0.933594 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 358 | Batch: 000 / 039 | Total loss: 0.326 | Reg loss: 0.014 | Tree loss: 0.326 | Accuracy: 0.939453 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 359 | Batch: 000 / 039 | Total loss: 0.343 | Reg loss: 0.014 | Tree loss: 0.343 | Accuracy: 0.923828 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 360 | Batch: 000 / 039 | Total loss: 0.351 | Reg loss: 0.014 | Tree loss: 0.351 | Accuracy: 0.935547 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 361 | Batch: 000 / 039 | Total loss: 0.337 | Reg loss: 0.014 | Tree loss: 0.337 | Accuracy: 0.917969 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 362 | Batch: 000 / 039 | Total loss: 0.327 | Reg loss: 0.014 | Tree loss: 0.327 | Accuracy: 0.941406 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 363 | Batch: 000 / 039 | Total loss: 0.395 | Reg loss: 0.014 | Tree loss: 0.395 | Accuracy: 0.908203 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 364 | Batch: 000 / 039 | Total loss: 0.357 | Reg loss: 0.014 | Tree loss: 0.357 | Accuracy: 0.917969 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 365 | Batch: 000 / 039 | Total loss: 0.359 | Reg loss: 0.014 | Tree loss: 0.359 | Accuracy: 0.921875 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 366 | Batch: 000 / 039 | Total loss: 0.330 | Reg loss: 0.014 | Tree loss: 0.330 | Accuracy: 0.939453 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 367 | Batch: 000 / 039 | Total loss: 0.331 | Reg loss: 0.014 | Tree loss: 0.331 | Accuracy: 0.929688 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 368 | Batch: 000 / 039 | Total loss: 0.294 | Reg loss: 0.014 | Tree loss: 0.294 | Accuracy: 0.949219 | 0.202 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 369 | Batch: 000 / 039 | Total loss: 0.361 | Reg loss: 0.014 | Tree loss: 0.361 | Accuracy: 0.925781 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 370 | Batch: 000 / 039 | Total loss: 0.361 | Reg loss: 0.014 | Tree loss: 0.361 | Accuracy: 0.925781 | 0.202 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 371 | Batch: 000 / 039 | Total loss: 0.334 | Reg loss: 0.014 | Tree loss: 0.334 | Accuracy: 0.917969 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 372 | Batch: 000 / 039 | Total loss: 0.356 | Reg loss: 0.014 | Tree loss: 0.356 | Accuracy: 0.919922 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 373 | Batch: 000 / 039 | Total loss: 0.336 | Reg loss: 0.014 | Tree loss: 0.336 | Accuracy: 0.929688 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 374 | Batch: 000 / 039 | Total loss: 0.350 | Reg loss: 0.014 | Tree loss: 0.350 | Accuracy: 0.916016 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 375 | Batch: 000 / 039 | Total loss: 0.377 | Reg loss: 0.014 | Tree loss: 0.377 | Accuracy: 0.916016 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 376 | Batch: 000 / 039 | Total loss: 0.341 | Reg loss: 0.014 | Tree loss: 0.341 | Accuracy: 0.929688 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 377 | Batch: 000 / 039 | Total loss: 0.382 | Reg loss: 0.014 | Tree loss: 0.382 | Accuracy: 0.917969 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 378 | Batch: 000 / 039 | Total loss: 0.359 | Reg loss: 0.014 | Tree loss: 0.359 | Accuracy: 0.923828 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 379 | Batch: 000 / 039 | Total loss: 0.322 | Reg loss: 0.014 | Tree loss: 0.322 | Accuracy: 0.925781 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 380 | Batch: 000 / 039 | Total loss: 0.340 | Reg loss: 0.014 | Tree loss: 0.340 | Accuracy: 0.931641 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 381 | Batch: 000 / 039 | Total loss: 0.353 | Reg loss: 0.014 | Tree loss: 0.353 | Accuracy: 0.921875 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 382 | Batch: 000 / 039 | Total loss: 0.349 | Reg loss: 0.014 | Tree loss: 0.349 | Accuracy: 0.931641 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 383 | Batch: 000 / 039 | Total loss: 0.384 | Reg loss: 0.014 | Tree loss: 0.384 | Accuracy: 0.919922 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 384 | Batch: 000 / 039 | Total loss: 0.362 | Reg loss: 0.014 | Tree loss: 0.362 | Accuracy: 0.914062 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 385 | Batch: 000 / 039 | Total loss: 0.315 | Reg loss: 0.014 | Tree loss: 0.315 | Accuracy: 0.933594 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 386 | Batch: 000 / 039 | Total loss: 0.368 | Reg loss: 0.014 | Tree loss: 0.368 | Accuracy: 0.927734 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 387 | Batch: 000 / 039 | Total loss: 0.412 | Reg loss: 0.014 | Tree loss: 0.412 | Accuracy: 0.908203 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 388 | Batch: 000 / 039 | Total loss: 0.315 | Reg loss: 0.014 | Tree loss: 0.315 | Accuracy: 0.943359 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 389 | Batch: 000 / 039 | Total loss: 0.321 | Reg loss: 0.014 | Tree loss: 0.321 | Accuracy: 0.947266 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 390 | Batch: 000 / 039 | Total loss: 0.367 | Reg loss: 0.014 | Tree loss: 0.367 | Accuracy: 0.925781 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 391 | Batch: 000 / 039 | Total loss: 0.351 | Reg loss: 0.014 | Tree loss: 0.351 | Accuracy: 0.923828 | 0.201 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 392 | Batch: 000 / 039 | Total loss: 0.350 | Reg loss: 0.014 | Tree loss: 0.350 | Accuracy: 0.927734 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 393 | Batch: 000 / 039 | Total loss: 0.390 | Reg loss: 0.014 | Tree loss: 0.390 | Accuracy: 0.908203 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 394 | Batch: 000 / 039 | Total loss: 0.376 | Reg loss: 0.014 | Tree loss: 0.376 | Accuracy: 0.921875 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 395 | Batch: 000 / 039 | Total loss: 0.301 | Reg loss: 0.014 | Tree loss: 0.301 | Accuracy: 0.945312 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 396 | Batch: 000 / 039 | Total loss: 0.345 | Reg loss: 0.014 | Tree loss: 0.345 | Accuracy: 0.925781 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 397 | Batch: 000 / 039 | Total loss: 0.355 | Reg loss: 0.014 | Tree loss: 0.355 | Accuracy: 0.916016 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 398 | Batch: 000 / 039 | Total loss: 0.417 | Reg loss: 0.014 | Tree loss: 0.417 | Accuracy: 0.906250 | 0.201 sec/iter\n",
      "Average sparseness: 0.9840425531914894\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "layer 5: 0.9840425531914894\n",
      "layer 6: 0.9840425531914895\n",
      "Epoch: 399 | Batch: 000 / 039 | Total loss: 0.367 | Reg loss: 0.014 | Tree loss: 0.367 | Accuracy: 0.925781 | 0.201 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df484b54c3e43bb9a8b5e740b72ac29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4584cb023b7f4dc68d56894559f73711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423562b0df5f4469b0681eb52eb4991b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "# plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9065399910b8451fad3f28f50a4c9646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 4.888888888888889\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "372\n",
      "============== Pattern 2 ==============\n",
      "11490\n",
      "============== Pattern 3 ==============\n",
      "7817\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "Average comprehensibility: 26.666666666666668\n",
      "std comprehensibility: 13.063945294843617\n",
      "var comprehensibility: 170.66666666666666\n",
      "minimum comprehensibility: 6\n",
      "maximum comprehensibility: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    }
   ],
   "source": [
    "attr_names = [f\"T_{i}\" for i in range(test_samples.shape[2])]\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
